{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3952497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (4.62.3)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 85.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (21.2)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 50.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.1.2 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow==2.3.0\n",
      "  Downloading tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4 MB)\n",
      "\u001b[K     |███████████████████████████▉    | 279.0 MB 55.6 MB/s eta 0:00:011  |████████▋                       | 86.4 MB 4.5 MB/s eta 0:00:53     |██████████████▍                 | 144.0 MB 103.8 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 320.4 MB 43 kB/s \n",
      "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (3.19.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 120.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2.3.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (2.6.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (2.3.0)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 118.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.41.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.26.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.35.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (58.5.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.8.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.10.0.2)\n",
      "Installing collected packages: numpy, scipy, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.3\n",
      "    Uninstalling scipy-1.5.3:\n",
      "      Successfully uninstalled scipy-1.5.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.3.4\n",
      "    Uninstalling tensorflow-2.3.4:\n",
      "      Successfully uninstalled tensorflow-2.3.4\n",
      "Successfully installed numpy-1.18.5 scipy-1.4.1 tensorflow-2.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install tensorflow==2.3.0\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaConfig, AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification\n",
    "# import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbefef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tensorflow version\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3edd38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import s3fs\n",
    "import json\n",
    "import re\n",
    "import gc\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "MAX_LEN = 128\n",
    "PEACE_COUNTRY = set(['Australia', 'New Zealand', \n",
    "                 'Belgium', 'Sweden', 'Denmark', \n",
    "                 'Norway', 'Finland', 'Czech Republic', \n",
    "                 'Netherlands', 'Austria'])\n",
    "MAJOR_COUNTRY = set(['Australia', 'India'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960d4d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70a5754b96a466a9253ac61fed5fd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8983da6a71ab4678ae7882a477201e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bbcc03e3914ff18b1ccdcac521a6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f71a5dd1114a40ae7cacd9e1f4fcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=1, #Binary Classification\n",
    "    dropout=0.1,\n",
    "    attention_dropout=0.1,\n",
    "    output_hidden_states=False,\n",
    "    output_attentions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c05526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoding function parser\n",
    "def regular_encode(texts, tokenizer, maxlen=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Function to encode the word\n",
    "    \"\"\"\n",
    "    # encode the word to vector of integer\n",
    "    enc_di = tokenizer.encode_plus(\n",
    "        texts, \n",
    "        return_attention_mask=True, \n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=maxlen)\n",
    "    \n",
    "    return np.array(enc_di['input_ids']), np.array(enc_di['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39656876",
   "metadata": {},
   "source": [
    "## Load Unshuffled, Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3f2bca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val [532, 468]\n",
      "Val [1065, 935]\n",
      "Val [1607, 1393]\n",
      "Val [2144, 1856]\n",
      "Val [2690, 2310]\n",
      "Train [240, 760]\n",
      "Val [3255, 2745]\n",
      "Val [3805, 3195]\n",
      "Val [4348, 3652]\n",
      "Val [4906, 4094]\n",
      "Val [5457, 4543]\n",
      "Train [481, 1519]\n",
      "Val [5984, 5016]\n",
      "Val [6519, 5481]\n",
      "Val [7059, 5941]\n",
      "Val [7638, 6362]\n",
      "Val [8191, 6809]\n",
      "Train [749, 2251]\n",
      "Val [8747, 7253]\n",
      "Val [9315, 7685]\n",
      "Val [9837, 8163]\n",
      "Val [10000, 9000]\n",
      "Train [1028, 2972]\n",
      "Val [10000, 10000]\n",
      "Train [1268, 3732]\n",
      "Train [1522, 4478]\n",
      "Train [1770, 5230]\n",
      "Train [2027, 5973]\n",
      "Train [2301, 6699]\n",
      "Train [2562, 7438]\n",
      "Train [2796, 8204]\n",
      "Train [3071, 8929]\n",
      "Train [3327, 9673]\n",
      "Train [3601, 10399]\n",
      "Train [3863, 11137]\n",
      "Train [4128, 11872]\n",
      "Train [4384, 12616]\n",
      "Train [4619, 13381]\n",
      "Train [4861, 14139]\n",
      "Train [5101, 14899]\n",
      "Train [5352, 15648]\n",
      "Train [5588, 16412]\n",
      "Train [5843, 17157]\n",
      "Train [6113, 17887]\n",
      "Train [6353, 18647]\n",
      "Train [6634, 19366]\n",
      "Train [6926, 20074]\n",
      "Train [7193, 20807]\n",
      "Train [7450, 21550]\n",
      "Train [7714, 22286]\n",
      "Train [7969, 23031]\n",
      "Train [8237, 23763]\n",
      "Train [8478, 24522]\n",
      "Train [8739, 25261]\n",
      "Train [8998, 26002]\n",
      "Train [9246, 26754]\n",
      "Train [9510, 27490]\n",
      "Train [9770, 28230]\n",
      "Train [10036, 28964]\n",
      "Train [10282, 29718]\n",
      "Train [10535, 30465]\n",
      "Train [10780, 31220]\n",
      "Train [11045, 31955]\n",
      "Train [11295, 32705]\n",
      "Train [11532, 33468]\n",
      "Train [11790, 34210]\n",
      "Train [12040, 34960]\n",
      "Train [12289, 35711]\n",
      "Train [12528, 36472]\n",
      "Train [12795, 37205]\n",
      "Train [13051, 37949]\n",
      "Train [13300, 38700]\n",
      "Train [13554, 39446]\n",
      "Train [14000, 40000]\n",
      "Train [15000, 40000]\n",
      "Train [16000, 40000]\n",
      "Train [17000, 40000]\n",
      "Train [18000, 40000]\n",
      "Train [19000, 40000]\n",
      "Train [20000, 40000]\n",
      "Train [21000, 40000]\n",
      "Train [22000, 40000]\n",
      "Train [23000, 40000]\n",
      "Train [24000, 40000]\n",
      "Train [25000, 40000]\n",
      "Train [26000, 40000]\n",
      "Train [27000, 40000]\n",
      "Train [28000, 40000]\n",
      "Train [29000, 40000]\n",
      "Train [30000, 40000]\n",
      "Train [31000, 40000]\n",
      "Train [32000, 40000]\n",
      "Train [33000, 40000]\n",
      "Train [34000, 40000]\n",
      "Train [35000, 40000]\n",
      "Train [36000, 40000]\n",
      "Train [37000, 40000]\n",
      "Train [38000, 40000]\n",
      "Train [39000, 40000]\n",
      "Train [40000, 40000]\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "train_count = 0  \n",
    "val_count = 0  \n",
    "train_label_counter = [0, 0]\n",
    "val_label_counter = [0, 0]\n",
    "\n",
    "train_label_count_max = 4e4 \n",
    "val_label_count_max = 1e4 \n",
    "total_train = 2 * train_label_count_max\n",
    "total_val = 2 * val_label_count_max\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for line in fs.open('s3://compressed-data-sample/processed_train.json'):\n",
    "    if train_count >= total_train and val_count >= total_val:\n",
    "        break\n",
    "    json_file = json.loads(line)\n",
    "    country = json_file['country']\n",
    "    label =  int(json_file['country'] in PEACE_COUNTRY)\n",
    "    \n",
    "    if not country in MAJOR_COUNTRY:\n",
    "        if train_label_counter[label] < train_label_count_max :\n",
    "            sent = json_file['content_cleaned']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_train.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_train.append(label)\n",
    "            train_count += 1\n",
    "            train_label_counter[label] += 1\n",
    "            if sum(train_label_counter) % 1e3 == 0:\n",
    "                print('Train', train_label_counter)\n",
    "    else:\n",
    "        if val_label_counter[label] < val_label_count_max :\n",
    "            sent = json_file['content_cleaned']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_val.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_val.append(label)\n",
    "            val_count += 1\n",
    "            val_label_counter[label] += 1\n",
    "            if sum(val_label_counter) % 1e3 == 0:\n",
    "                print('Val', val_label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34022106",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_all = np.hstack([X_train, X_val])\n",
    "y_all = np.hstack([y_train, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af4b9d",
   "metadata": {},
   "source": [
    "## Prepare for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "412daed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def get_model(lr = 3e-5):\n",
    "    bert_model = TFAutoModelForSequenceClassification.from_pretrained('roberta-base', trainable=True, config=config)\n",
    "    input_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='input_ids', dtype='int32')\n",
    "    input_masks_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='attention_mask', dtype='int32')\n",
    "    output_layer = bert_model(input_ids_in, input_masks_ids_in)[0]\n",
    "    output_layer = tf.keras.layers.Activation(activation='sigmoid')(output_layer)\n",
    "    model = tf.keras.Model(inputs=[input_ids_in, input_masks_ids_in], outputs = output_layer)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.5),\n",
    "               tf.keras.metrics.Precision(name='precision', thresholds=0.5),\n",
    "               tf.keras.metrics.Recall(name='recall', thresholds=0.5),\n",
    "               tf.keras.metrics.TruePositives(name='TP', thresholds=0.5),\n",
    "               tf.keras.metrics.TrueNegatives(name='TN', thresholds=0.5),\n",
    "               tf.keras.metrics.FalsePositives(name='FP', thresholds=0.5),\n",
    "               tf.keras.metrics.FalseNegatives(name='FN', thresholds=0.5)]\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define Splits\n",
    "skf_train = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "skf_val = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6a3db",
   "metadata": {},
   "source": [
    "## Define test runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a9f2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct test function\n",
    "def run_test(X_train, y_train, \n",
    "             X_val, y_val,\n",
    "             train_index, val_index, \n",
    "             class_weight, epochs, batch_size=32, lr=3e-5):\n",
    "    eval_results = []\n",
    "    metrics_names = None\n",
    "\n",
    "    for train_index, val_index in list(zip(train_index, val_index)):\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_val[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_val[val_index]\n",
    "\n",
    "        model = get_model(lr)\n",
    "        \n",
    "        train_input1 = np.vstack([x['input_ids'] for x in X_train_fold])\n",
    "        train_input2 = np.vstack([x['attention_mask'] for x in X_train_fold])\n",
    "        model.fit(x=[train_input1, train_input2], \n",
    "                  y=np.asarray(y_train_fold),\n",
    "                  epochs = epochs, \n",
    "                  batch_size = batch_size,\n",
    "                  class_weight=class_weight)\n",
    "\n",
    "        eval_input1 = np.vstack([x['input_ids'] for x in X_val_fold])\n",
    "        eval_input2 = np.vstack([x['attention_mask'] for x in X_val_fold])\n",
    "        er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                            y=np.asarray(y_val_fold), return_dict=True)\n",
    "        f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "        \n",
    "        precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "        recall_neg = er['TN'] / (er['TN']+er['FP'])\n",
    "        f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "        \n",
    "        er = list(er.values())\n",
    "        er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "        eval_results.append(er)\n",
    "        metrics_names = model.metrics_names\n",
    "        print(er)\n",
    "        \n",
    "        ## clear memory\n",
    "        del model\n",
    "        del X_train_fold, X_val_fold, y_train_fold, y_val_fold\n",
    "        del train_input1, train_input2, eval_input1, eval_input2\n",
    "        gc.collect() \n",
    "\n",
    "    \n",
    "    eval_results = np.array(eval_results)\n",
    "    metrics_names += ['f1', 'precision_neg', 'recall_neg', 'f1_neg']\n",
    "    eval_results = pd.DataFrame(eval_results, columns=metrics_names)\n",
    "    ci = eval_results.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "    ci.index = ['ci_lower', 'ci_upper']\n",
    "    eval_results = pd.concat([eval_results.describe(), ci])\n",
    "    \n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f82222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Test 0: Whether model overfits with 10 epochs\n",
    "# ### Train-Test Random Split Method\n",
    "# Train: 9000, Val: 1000\n",
    "# ## To fasten the training, take a random subset of X_all and fit the model with K-fold\n",
    "# skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "# all_sample_idx = [i[1] for i in skf.split(X_all, y_all)][0]\n",
    "# X_all_sample = X_all[all_sample_idx]\n",
    "# y_all_sample = y_all[all_sample_idx]\n",
    "\n",
    "# train_index, val_index = list(zip(*[(train_split, val_split) for train_split, val_split in skf.split(X_all_sample,y_all_sample)])) \n",
    "\n",
    "# test_result = run_test(X_all_sample, y_all_sample, X_all_sample, y_all_sample, train_index, val_index, class_weight = {0: 1., 1: 1.}, epochs=1)\n",
    "# test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92b9fe",
   "metadata": {},
   "source": [
    "## Randomly Split, train on original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "28945934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 189s 757ms/step - loss: 0.2552 - accuracy: 0.8930 - precision: 0.8944 - recall: 0.8913 - TP: 3565.0000 - TN: 3579.0000 - FP: 421.0000 - FN: 435.0000\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 0.1518 - accuracy: 0.9465 - precision: 0.9443 - recall: 0.9490 - TP: 949.0000 - TN: 944.0000 - FP: 56.0000 - FN: 51.0000\n",
      "[0.15177413821220398, 0.9465000033378601, 0.9442785978317261, 0.9490000009536743, 949.0, 944.0, 56.0, 51.0, 0.9466334123437856, 0.9487437185929648, 0.944, 0.9463659147869674]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 762ms/step - loss: 0.2506 - accuracy: 0.8934 - precision: 0.8703 - recall: 0.9245 - TP: 3698.0000 - TN: 3449.0000 - FP: 551.0000 - FN: 302.0000\n",
      "63/63 [==============================] - 16s 248ms/step - loss: 0.1685 - accuracy: 0.9270 - precision: 0.8826 - recall: 0.9850 - TP: 985.0000 - TN: 869.0000 - FP: 131.0000 - FN: 15.0000\n",
      "[0.1685006320476532, 0.9269999861717224, 0.8826164603233337, 0.9850000143051147, 985.0, 869.0, 131.0, 15.0, 0.9310018816549271, 0.9830316742081447, 0.869, 0.9225053078556263]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 188s 752ms/step - loss: 0.2689 - accuracy: 0.8838 - precision: 0.8585 - recall: 0.9190 - TP: 3676.0000 - TN: 3394.0000 - FP: 606.0000 - FN: 324.0000\n",
      "63/63 [==============================] - 15s 244ms/step - loss: 0.1504 - accuracy: 0.9475 - precision: 0.9299 - recall: 0.9680 - TP: 968.0000 - TN: 927.0000 - FP: 73.0000 - FN: 32.0000\n",
      "[0.15044142305850983, 0.9474999904632568, 0.9298751354217529, 0.9679999947547913, 968.0, 927.0, 73.0, 32.0, 0.9485546355487958, 0.9666319082377477, 0.927, 0.9464012251148545]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 761ms/step - loss: 0.2529 - accuracy: 0.8953 - precision: 0.8968 - recall: 0.8932 - TP: 3573.0000 - TN: 3589.0000 - FP: 411.0000 - FN: 427.0000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1573 - accuracy: 0.9455 - precision: 0.9223 - recall: 0.9730 - TP: 973.0000 - TN: 918.0000 - FP: 82.0000 - FN: 27.0000\n",
      "[0.1573292762041092, 0.9455000162124634, 0.9222748875617981, 0.9729999899864197, 973.0, 918.0, 82.0, 27.0, 0.9469586359137774, 0.9714285714285714, 0.918, 0.9439588688946015]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 192s 767ms/step - loss: 0.2451 - accuracy: 0.8929 - precision: 0.8922 - recall: 0.8938 - TP: 3575.0000 - TN: 3568.0000 - FP: 432.0000 - FN: 425.0000\n",
      "63/63 [==============================] - 15s 239ms/step - loss: 0.1302 - accuracy: 0.9545 - precision: 0.9478 - recall: 0.9620 - TP: 962.0000 - TN: 947.0000 - FP: 53.0000 - FN: 38.0000\n",
      "[0.1302296668291092, 0.9545000195503235, 0.9477832317352295, 0.9620000123977661, 962.0, 947.0, 53.0, 38.0, 0.9548387058905319, 0.9614213197969543, 0.947, 0.9541561712846347]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 760ms/step - loss: 0.2476 - accuracy: 0.9018 - precision: 0.8933 - recall: 0.9125 - TP: 3650.0000 - TN: 3564.0000 - FP: 436.0000 - FN: 350.0000\n",
      "63/63 [==============================] - 16s 252ms/step - loss: 0.1372 - accuracy: 0.9535 - precision: 0.9348 - recall: 0.9750 - TP: 975.0000 - TN: 932.0000 - FP: 68.0000 - FN: 25.0000\n",
      "[0.1371743232011795, 0.953499972820282, 0.9348034262657166, 0.9750000238418579, 975.0, 932.0, 68.0, 25.0, 0.9544787060104906, 0.9738766980146291, 0.932, 0.9524782830863566]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 188s 753ms/step - loss: 0.2416 - accuracy: 0.8981 - precision: 0.8718 - recall: 0.9335 - TP: 3734.0000 - TN: 3451.0000 - FP: 549.0000 - FN: 266.0000\n",
      "63/63 [==============================] - 15s 244ms/step - loss: 0.1348 - accuracy: 0.9510 - precision: 0.9583 - recall: 0.9430 - TP: 943.0000 - TN: 959.0000 - FP: 41.0000 - FN: 57.0000\n",
      "[0.1347600817680359, 0.9509999752044678, 0.9583333134651184, 0.9430000185966492, 943.0, 959.0, 41.0, 57.0, 0.9506048383840533, 0.9438976377952756, 0.959, 0.951388888888889]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 191s 762ms/step - loss: 0.2356 - accuracy: 0.9006 - precision: 0.8764 - recall: 0.9327 - TP: 3731.0000 - TN: 3474.0000 - FP: 526.0000 - FN: 269.0000\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 0.1877 - accuracy: 0.9460 - precision: 0.9272 - recall: 0.9680 - TP: 968.0000 - TN: 924.0000 - FP: 76.0000 - FN: 32.0000\n",
      "[0.1877158135175705, 0.9459999799728394, 0.9272030591964722, 0.9679999947547913, 968.0, 924.0, 76.0, 32.0, 0.947162421005567, 0.9665271966527197, 0.924, 0.9447852760736196]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 186s 744ms/step - loss: 0.2362 - accuracy: 0.9010 - precision: 0.8928 - recall: 0.9115 - TP: 3646.0000 - TN: 3562.0000 - FP: 438.0000 - FN: 354.0000\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 0.1269 - accuracy: 0.9510 - precision: 0.9492 - recall: 0.9530 - TP: 953.0000 - TN: 949.0000 - FP: 51.0000 - FN: 47.0000\n",
      "[0.1269349604845047, 0.9509999752044678, 0.9492031931877136, 0.953000009059906, 953.0, 949.0, 51.0, 47.0, 0.9510978118833253, 0.9528112449799196, 0.949, 0.9509018036072143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 189s 755ms/step - loss: 0.2401 - accuracy: 0.8997 - precision: 0.8759 - recall: 0.9315 - TP: 3726.0000 - TN: 3472.0000 - FP: 528.0000 - FN: 274.0000\n",
      "63/63 [==============================] - 16s 249ms/step - loss: 0.1522 - accuracy: 0.9455 - precision: 0.9231 - recall: 0.9720 - TP: 972.0000 - TN: 919.0000 - FP: 81.0000 - FN: 28.0000\n",
      "[0.15219509601593018, 0.9455000162124634, 0.9230769276618958, 0.972000002861023, 972.0, 919.0, 81.0, 28.0, 0.946906969186448, 0.9704329461457233, 0.919, 0.9440164355418592]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision_neg</th>\n",
       "      <th>recall_neg</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.149706</td>\n",
       "      <td>0.946800</td>\n",
       "      <td>0.931945</td>\n",
       "      <td>0.964800</td>\n",
       "      <td>964.800000</td>\n",
       "      <td>928.800000</td>\n",
       "      <td>71.200000</td>\n",
       "      <td>35.200000</td>\n",
       "      <td>0.947824</td>\n",
       "      <td>0.963880</td>\n",
       "      <td>0.928800</td>\n",
       "      <td>0.945696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.018695</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>0.021181</td>\n",
       "      <td>0.013011</td>\n",
       "      <td>13.011106</td>\n",
       "      <td>25.182887</td>\n",
       "      <td>25.182887</td>\n",
       "      <td>13.011106</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>0.025183</td>\n",
       "      <td>0.008960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.126935</td>\n",
       "      <td>0.927000</td>\n",
       "      <td>0.882616</td>\n",
       "      <td>0.943000</td>\n",
       "      <td>943.000000</td>\n",
       "      <td>869.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.931002</td>\n",
       "      <td>0.943898</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.922505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.135364</td>\n",
       "      <td>0.945625</td>\n",
       "      <td>0.924108</td>\n",
       "      <td>0.955250</td>\n",
       "      <td>955.250000</td>\n",
       "      <td>920.250000</td>\n",
       "      <td>53.750000</td>\n",
       "      <td>27.250000</td>\n",
       "      <td>0.946920</td>\n",
       "      <td>0.954964</td>\n",
       "      <td>0.920250</td>\n",
       "      <td>0.944209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.151108</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>0.932339</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>929.500000</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.947859</td>\n",
       "      <td>0.966580</td>\n",
       "      <td>0.929500</td>\n",
       "      <td>0.946384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.156046</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.946907</td>\n",
       "      <td>0.972750</td>\n",
       "      <td>972.750000</td>\n",
       "      <td>946.250000</td>\n",
       "      <td>79.750000</td>\n",
       "      <td>44.750000</td>\n",
       "      <td>0.950975</td>\n",
       "      <td>0.971180</td>\n",
       "      <td>0.946250</td>\n",
       "      <td>0.951267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.187716</td>\n",
       "      <td>0.954500</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.954839</td>\n",
       "      <td>0.983032</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>0.954156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_lower</th>\n",
       "      <td>0.136533</td>\n",
       "      <td>0.941362</td>\n",
       "      <td>0.917021</td>\n",
       "      <td>0.955632</td>\n",
       "      <td>955.632383</td>\n",
       "      <td>911.056154</td>\n",
       "      <td>53.456154</td>\n",
       "      <td>26.032383</td>\n",
       "      <td>0.943133</td>\n",
       "      <td>0.955289</td>\n",
       "      <td>0.911056</td>\n",
       "      <td>0.939383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_upper</th>\n",
       "      <td>0.162878</td>\n",
       "      <td>0.952238</td>\n",
       "      <td>0.946869</td>\n",
       "      <td>0.973968</td>\n",
       "      <td>973.967617</td>\n",
       "      <td>946.543846</td>\n",
       "      <td>88.943846</td>\n",
       "      <td>44.367617</td>\n",
       "      <td>0.952515</td>\n",
       "      <td>0.972472</td>\n",
       "      <td>0.946544</td>\n",
       "      <td>0.952009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss   accuracy  precision     recall          TP          TN  \\\n",
       "count     10.000000  10.000000  10.000000  10.000000   10.000000   10.000000   \n",
       "mean       0.149706   0.946800   0.931945   0.964800  964.800000  928.800000   \n",
       "std        0.018695   0.007718   0.021181   0.013011   13.011106   25.182887   \n",
       "min        0.126935   0.927000   0.882616   0.943000  943.000000  869.000000   \n",
       "25%        0.135364   0.945625   0.924108   0.955250  955.250000  920.250000   \n",
       "50%        0.151108   0.947000   0.932339   0.968000  968.000000  929.500000   \n",
       "75%        0.156046   0.951000   0.946907   0.972750  972.750000  946.250000   \n",
       "max        0.187716   0.954500   0.958333   0.985000  985.000000  959.000000   \n",
       "ci_lower   0.136533   0.941362   0.917021   0.955632  955.632383  911.056154   \n",
       "ci_upper   0.162878   0.952238   0.946869   0.973968  973.967617  946.543846   \n",
       "\n",
       "                  FP         FN         f1  precision_neg  recall_neg  \\\n",
       "count      10.000000  10.000000  10.000000      10.000000   10.000000   \n",
       "mean       71.200000  35.200000   0.947824       0.963880    0.928800   \n",
       "std        25.182887  13.011106   0.006658       0.012194    0.025183   \n",
       "min        41.000000  15.000000   0.931002       0.943898    0.869000   \n",
       "25%        53.750000  27.250000   0.946920       0.954964    0.920250   \n",
       "50%        70.500000  32.000000   0.947859       0.966580    0.929500   \n",
       "75%        79.750000  44.750000   0.950975       0.971180    0.946250   \n",
       "max       131.000000  57.000000   0.954839       0.983032    0.959000   \n",
       "ci_lower   53.456154  26.032383   0.943133       0.955289    0.911056   \n",
       "ci_upper   88.943846  44.367617   0.952515       0.972472    0.946544   \n",
       "\n",
       "             f1_neg  \n",
       "count     10.000000  \n",
       "mean       0.945696  \n",
       "std        0.008960  \n",
       "min        0.922505  \n",
       "25%        0.944209  \n",
       "50%        0.946384  \n",
       "75%        0.951267  \n",
       "max        0.954156  \n",
       "ci_lower   0.939383  \n",
       "ci_upper   0.952009  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Throw out some data to make sample # same as split by country\n",
    "_, train_index = train_test_split(np.array(range(len(X_all))),test_size=0.1, stratify=y_all)\n",
    "\n",
    "X_all_sample, y_all_sample = X_all[train_index], y_all[train_index]\n",
    "\n",
    "train_index, val_index = list(zip(*[train_test_split(np.array(range(len(X_all_sample))), \n",
    "                            test_size=0.2, stratify=y_all_sample) for i in range(10)]))\n",
    "\n",
    "test_result = run_test(X_all_sample, y_all_sample, X_all_sample, y_all_sample, \n",
    "                       train_index, val_index, class_weight = {0: 1., 1: 1.}, epochs=1)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e221191",
   "metadata": {},
   "source": [
    "## Split by country, train on original\n",
    "Train: 8000, Val: 2000 per trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d8a769b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 189s 756ms/step - loss: 0.1998 - accuracy: 0.9149 - precision: 0.8988 - recall: 0.9350 - TP: 3740.0000 - TN: 3579.0000 - FP: 421.0000 - FN: 260.0000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.8088 - accuracy: 0.7185 - precision: 0.6752 - recall: 0.8420 - TP: 842.0000 - TN: 595.0000 - FP: 405.0000 - FN: 158.0000\n",
      "[0.8087649941444397, 0.718500018119812, 0.6752205491065979, 0.8420000076293945, 842.0, 595.0, 405.0, 158.0, 0.7494437179553833, 0.7901726427622842, 0.595, 0.6788362806617227]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 191s 764ms/step - loss: 0.2128 - accuracy: 0.9084 - precision: 0.8816 - recall: 0.9435 - TP: 3774.0000 - TN: 3493.0000 - FP: 507.0000 - FN: 226.0000\n",
      "63/63 [==============================] - 15s 239ms/step - loss: 0.9327 - accuracy: 0.7110 - precision: 0.6822 - recall: 0.7900 - TP: 790.0000 - TN: 632.0000 - FP: 368.0000 - FN: 210.0000\n",
      "[0.9326938390731812, 0.7110000252723694, 0.6822106838226318, 0.7900000214576721, 790.0, 632.0, 368.0, 210.0, 0.7321594020822159, 0.7505938242280285, 0.632, 0.6862106406080347]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 192s 766ms/step - loss: 0.1947 - accuracy: 0.9179 - precision: 0.8925 - recall: 0.9503 - TP: 3801.0000 - TN: 3542.0000 - FP: 458.0000 - FN: 199.0000\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.8797 - accuracy: 0.7190 - precision: 0.6664 - recall: 0.8770 - TP: 877.0000 - TN: 561.0000 - FP: 439.0000 - FN: 123.0000\n",
      "[0.8797447681427002, 0.718999981880188, 0.6664133667945862, 0.8769999742507935, 877.0, 561.0, 439.0, 123.0, 0.757340227632583, 0.8201754385964912, 0.561, 0.666270783847981]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 189s 754ms/step - loss: 0.2171 - accuracy: 0.9089 - precision: 0.8826 - recall: 0.9433 - TP: 3773.0000 - TN: 3498.0000 - FP: 502.0000 - FN: 227.0000\n",
      "63/63 [==============================] - 15s 246ms/step - loss: 0.8206 - accuracy: 0.7495 - precision: 0.7030 - recall: 0.8640 - TP: 864.0000 - TN: 635.0000 - FP: 365.0000 - FN: 136.0000\n",
      "[0.8205527663230896, 0.7494999766349792, 0.7030105590820312, 0.8640000224113464, 864.0, 635.0, 365.0, 136.0, 0.7752355293267114, 0.8236057068741893, 0.635, 0.7171089779785432]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 189s 758ms/step - loss: 0.1835 - accuracy: 0.9220 - precision: 0.9168 - recall: 0.9283 - TP: 3713.0000 - TN: 3663.0000 - FP: 337.0000 - FN: 287.0000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.9861 - accuracy: 0.7040 - precision: 0.6409 - recall: 0.9280 - TP: 928.0000 - TN: 480.0000 - FP: 520.0000 - FN: 72.0000\n",
      "[0.9860922694206238, 0.7039999961853027, 0.6408839821815491, 0.9279999732971191, 928.0, 480.0, 520.0, 72.0, 0.7581699287243625, 0.8695652173913043, 0.48, 0.6185567010309279]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 191s 765ms/step - loss: 0.1928 - accuracy: 0.9141 - precision: 0.9238 - recall: 0.9028 - TP: 3611.0000 - TN: 3702.0000 - FP: 298.0000 - FN: 389.0000\n",
      "63/63 [==============================] - 15s 239ms/step - loss: 1.2182 - accuracy: 0.6645 - precision: 0.6814 - recall: 0.6180 - TP: 618.0000 - TN: 711.0000 - FP: 289.0000 - FN: 382.0000\n",
      "[1.2181950807571411, 0.6644999980926514, 0.6813671588897705, 0.6179999709129333, 618.0, 711.0, 289.0, 382.0, 0.6481384278804156, 0.6505032021957914, 0.711, 0.6794075489727663]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 192s 766ms/step - loss: 0.2059 - accuracy: 0.9115 - precision: 0.9048 - recall: 0.9197 - TP: 3679.0000 - TN: 3613.0000 - FP: 387.0000 - FN: 321.0000\n",
      "63/63 [==============================] - 15s 245ms/step - loss: 0.9630 - accuracy: 0.7305 - precision: 0.6814 - recall: 0.8660 - TP: 866.0000 - TN: 595.0000 - FP: 405.0000 - FN: 134.0000\n",
      "[0.9629929661750793, 0.7304999828338623, 0.6813532710075378, 0.8659999966621399, 866.0, 595.0, 405.0, 134.0, 0.7626596236900539, 0.8161865569272977, 0.595, 0.6882591093117408]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 188s 753ms/step - loss: 0.1938 - accuracy: 0.9231 - precision: 0.9075 - recall: 0.9423 - TP: 3769.0000 - TN: 3616.0000 - FP: 384.0000 - FN: 231.0000\n",
      "63/63 [==============================] - 15s 244ms/step - loss: 0.9011 - accuracy: 0.7360 - precision: 0.6850 - recall: 0.8740 - TP: 874.0000 - TN: 598.0000 - FP: 402.0000 - FN: 126.0000\n",
      "[0.9011290073394775, 0.7360000014305115, 0.684952974319458, 0.8740000128746033, 874.0, 598.0, 402.0, 126.0, 0.768014062375549, 0.8259668508287292, 0.598, 0.693735498839907]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 760ms/step - loss: 0.1993 - accuracy: 0.9170 - precision: 0.9191 - recall: 0.9145 - TP: 3658.0000 - TN: 3678.0000 - FP: 322.0000 - FN: 342.0000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.8475 - accuracy: 0.7435 - precision: 0.6832 - recall: 0.9080 - TP: 908.0000 - TN: 579.0000 - FP: 421.0000 - FN: 92.0000\n",
      "[0.8474687337875366, 0.7434999942779541, 0.6832204461097717, 0.9079999923706055, 908.0, 579.0, 421.0, 92.0, 0.779733775224211, 0.8628912071535022, 0.579, 0.6929982046678635]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 192s 766ms/step - loss: 0.2374 - accuracy: 0.9047 - precision: 0.9000 - recall: 0.9107 - TP: 3643.0000 - TN: 3595.0000 - FP: 405.0000 - FN: 357.0000\n",
      "63/63 [==============================] - 15s 239ms/step - loss: 1.0762 - accuracy: 0.7115 - precision: 0.6656 - recall: 0.8500 - TP: 850.0000 - TN: 573.0000 - FP: 427.0000 - FN: 150.0000\n",
      "[1.076216220855713, 0.7114999890327454, 0.6656225323677063, 0.8500000238418579, 850.0, 573.0, 427.0, 150.0, 0.7465963950776651, 0.7925311203319502, 0.573, 0.665118978525827]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision_neg</th>\n",
       "      <th>recall_neg</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.943385</td>\n",
       "      <td>0.718800</td>\n",
       "      <td>0.676426</td>\n",
       "      <td>0.841700</td>\n",
       "      <td>841.700000</td>\n",
       "      <td>595.900000</td>\n",
       "      <td>404.100000</td>\n",
       "      <td>158.300000</td>\n",
       "      <td>0.747749</td>\n",
       "      <td>0.800219</td>\n",
       "      <td>0.595900</td>\n",
       "      <td>0.678650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.126374</td>\n",
       "      <td>0.024192</td>\n",
       "      <td>0.016294</td>\n",
       "      <td>0.086925</td>\n",
       "      <td>86.925319</td>\n",
       "      <td>59.204823</td>\n",
       "      <td>59.204823</td>\n",
       "      <td>86.925319</td>\n",
       "      <td>0.037685</td>\n",
       "      <td>0.062926</td>\n",
       "      <td>0.059205</td>\n",
       "      <td>0.025842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.808765</td>\n",
       "      <td>0.664500</td>\n",
       "      <td>0.640884</td>\n",
       "      <td>0.618000</td>\n",
       "      <td>618.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.648138</td>\n",
       "      <td>0.650503</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.618557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.855538</td>\n",
       "      <td>0.711125</td>\n",
       "      <td>0.668615</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>844.000000</td>\n",
       "      <td>574.500000</td>\n",
       "      <td>376.500000</td>\n",
       "      <td>123.750000</td>\n",
       "      <td>0.747308</td>\n",
       "      <td>0.790762</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>0.669412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.916911</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.681360</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>595.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.757755</td>\n",
       "      <td>0.818181</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.682809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.980317</td>\n",
       "      <td>0.734625</td>\n",
       "      <td>0.682968</td>\n",
       "      <td>0.876250</td>\n",
       "      <td>876.250000</td>\n",
       "      <td>623.500000</td>\n",
       "      <td>425.500000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.766675</td>\n",
       "      <td>0.825377</td>\n",
       "      <td>0.623500</td>\n",
       "      <td>0.691813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.218195</td>\n",
       "      <td>0.749500</td>\n",
       "      <td>0.703011</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>928.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>0.779734</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.717109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_lower</th>\n",
       "      <td>0.854342</td>\n",
       "      <td>0.701755</td>\n",
       "      <td>0.664945</td>\n",
       "      <td>0.780452</td>\n",
       "      <td>780.452475</td>\n",
       "      <td>554.184319</td>\n",
       "      <td>362.384319</td>\n",
       "      <td>97.052475</td>\n",
       "      <td>0.721196</td>\n",
       "      <td>0.755881</td>\n",
       "      <td>0.554184</td>\n",
       "      <td>0.660442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_upper</th>\n",
       "      <td>1.032428</td>\n",
       "      <td>0.735845</td>\n",
       "      <td>0.687906</td>\n",
       "      <td>0.902948</td>\n",
       "      <td>902.947525</td>\n",
       "      <td>637.615681</td>\n",
       "      <td>445.815681</td>\n",
       "      <td>219.547525</td>\n",
       "      <td>0.774302</td>\n",
       "      <td>0.844557</td>\n",
       "      <td>0.637616</td>\n",
       "      <td>0.696858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss   accuracy  precision     recall          TP          TN  \\\n",
       "count     10.000000  10.000000  10.000000  10.000000   10.000000   10.000000   \n",
       "mean       0.943385   0.718800   0.676426   0.841700  841.700000  595.900000   \n",
       "std        0.126374   0.024192   0.016294   0.086925   86.925319   59.204823   \n",
       "min        0.808765   0.664500   0.640884   0.618000  618.000000  480.000000   \n",
       "25%        0.855538   0.711125   0.668615   0.844000  844.000000  574.500000   \n",
       "50%        0.916911   0.718750   0.681360   0.865000  865.000000  595.000000   \n",
       "75%        0.980317   0.734625   0.682968   0.876250  876.250000  623.500000   \n",
       "max        1.218195   0.749500   0.703011   0.928000  928.000000  711.000000   \n",
       "ci_lower   0.854342   0.701755   0.664945   0.780452  780.452475  554.184319   \n",
       "ci_upper   1.032428   0.735845   0.687906   0.902948  902.947525  637.615681   \n",
       "\n",
       "                  FP          FN         f1  precision_neg  recall_neg  \\\n",
       "count      10.000000   10.000000  10.000000      10.000000   10.000000   \n",
       "mean      404.100000  158.300000   0.747749       0.800219    0.595900   \n",
       "std        59.204823   86.925319   0.037685       0.062926    0.059205   \n",
       "min       289.000000   72.000000   0.648138       0.650503    0.480000   \n",
       "25%       376.500000  123.750000   0.747308       0.790762    0.574500   \n",
       "50%       405.000000  135.000000   0.757755       0.818181    0.595000   \n",
       "75%       425.500000  156.000000   0.766675       0.825377    0.623500   \n",
       "max       520.000000  382.000000   0.779734       0.869565    0.711000   \n",
       "ci_lower  362.384319   97.052475   0.721196       0.755881    0.554184   \n",
       "ci_upper  445.815681  219.547525   0.774302       0.844557    0.637616   \n",
       "\n",
       "             f1_neg  \n",
       "count     10.000000  \n",
       "mean       0.678650  \n",
       "std        0.025842  \n",
       "min        0.618557  \n",
       "25%        0.669412  \n",
       "50%        0.682809  \n",
       "75%        0.691813  \n",
       "max        0.717109  \n",
       "ci_lower   0.660442  \n",
       "ci_upper   0.696858  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index = [val_split for _, val_split in skf_train.split(X_train,y_train)] \n",
    "val_index = [val_split for _, val_split in skf_val.split(X_val,y_val)] \n",
    "\n",
    "test_result = run_test(X_train, y_train, X_val, y_val,\n",
    "                       train_index, val_index, class_weight = {0: 1., 1: 1.}, epochs=1)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45abead1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dc47ccff3f43c99990a5863c145f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/627M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "250/250 [==============================] - 213s 854ms/step - loss: 0.2103 - accuracy: 0.9094 - precision: 0.9241 - recall: 0.8920 - TP: 3568.0000 - TN: 3707.0000 - FP: 293.0000 - FN: 432.0000\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 235s 939ms/step - loss: 0.0849 - accuracy: 0.9670 - precision: 0.9656 - recall: 0.9685 - TP: 3874.0000 - TN: 3862.0000 - FP: 138.0000 - FN: 126.0000\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 0.9824 - accuracy: 0.7190 - precision: 0.6819 - recall: 0.8210 - TP: 821.0000 - TN: 617.0000 - FP: 383.0000 - FN: 179.0000\n",
      "[0.982352614402771, 0.718999981880188, 0.6818937063217163, 0.8209999799728394, 821.0, 617.0, 383.0, 179.0, 0.7450090772741605, 0.7751256281407035, 0.617, 0.6870824053452116]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "250/250 [==============================] - 235s 940ms/step - loss: 0.2100 - accuracy: 0.9095 - precision: 0.9025 - recall: 0.9183 - TP: 3673.0000 - TN: 3603.0000 - FP: 397.0000 - FN: 327.0000\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 235s 942ms/step - loss: 0.0859 - accuracy: 0.9685 - precision: 0.9662 - recall: 0.9710 - TP: 3884.0000 - TN: 3864.0000 - FP: 136.0000 - FN: 116.0000\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 1.2715 - accuracy: 0.7190 - precision: 0.6692 - recall: 0.8660 - TP: 866.0000 - TN: 572.0000 - FP: 428.0000 - FN: 134.0000\n",
      "[1.2714521884918213, 0.718999981880188, 0.6692426800727844, 0.8659999966621399, 866.0, 572.0, 428.0, 134.0, 0.7550130901021854, 0.8101983002832861, 0.572, 0.6705744431418522]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "250/250 [==============================] - 235s 940ms/step - loss: 0.1999 - accuracy: 0.9175 - precision: 0.9228 - recall: 0.9112 - TP: 3645.0000 - TN: 3695.0000 - FP: 305.0000 - FN: 355.0000\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 235s 939ms/step - loss: 0.0855 - accuracy: 0.9675 - precision: 0.9643 - recall: 0.9710 - TP: 3884.0000 - TN: 3856.0000 - FP: 144.0000 - FN: 116.0000\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 1.1532 - accuracy: 0.7020 - precision: 0.6395 - recall: 0.9260 - TP: 926.0000 - TN: 478.0000 - FP: 522.0000 - FN: 74.0000\n",
      "[1.1532037258148193, 0.7020000219345093, 0.639502763748169, 0.9259999990463257, 926.0, 478.0, 522.0, 74.0, 0.7565359483158742, 0.8659420289855072, 0.478, 0.615979381443299]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "250/250 [==============================] - 232s 929ms/step - loss: 0.2181 - accuracy: 0.9079 - precision: 0.8851 - recall: 0.9375 - TP: 3750.0000 - TN: 3513.0000 - FP: 487.0000 - FN: 250.0000\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 232s 927ms/step - loss: 0.0795 - accuracy: 0.9707 - precision: 0.9717 - recall: 0.9697 - TP: 3879.0000 - TN: 3887.0000 - FP: 113.0000 - FN: 121.0000\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 1.0379 - accuracy: 0.7465 - precision: 0.7066 - recall: 0.8430 - TP: 843.0000 - TN: 650.0000 - FP: 350.0000 - FN: 157.0000\n",
      "[1.0379314422607422, 0.7465000152587891, 0.7066219449043274, 0.8429999947547913, 843.0, 650.0, 350.0, 157.0, 0.7688098375517384, 0.80545229244114, 0.65, 0.7194244604316548]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "250/250 [==============================] - 235s 939ms/step - loss: 0.1973 - accuracy: 0.9149 - precision: 0.9137 - recall: 0.9162 - TP: 3665.0000 - TN: 3654.0000 - FP: 346.0000 - FN: 335.0000\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 235s 938ms/step - loss: 0.0714 - accuracy: 0.9741 - precision: 0.9747 - recall: 0.9735 - TP: 3894.0000 - TN: 3899.0000 - FP: 101.0000 - FN: 106.0000\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 1.1865 - accuracy: 0.6990 - precision: 0.6736 - recall: 0.7720 - TP: 772.0000 - TN: 626.0000 - FP: 374.0000 - FN: 228.0000\n",
      "[1.186518907546997, 0.6990000009536743, 0.6736474633216858, 0.7720000147819519, 772.0, 626.0, 374.0, 228.0, 0.7194781017075634, 0.7330210772833724, 0.626, 0.6752966558791802]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "250/250 [==============================] - 232s 929ms/step - loss: 0.1963 - accuracy: 0.9139 - precision: 0.9214 - recall: 0.9050 - TP: 3620.0000 - TN: 3691.0000 - FP: 309.0000 - FN: 380.0000\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 231s 926ms/step - loss: 0.0803 - accuracy: 0.9697 - precision: 0.9700 - recall: 0.9695 - TP: 3878.0000 - TN: 3880.0000 - FP: 120.0000 - FN: 122.0000\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 1.1400 - accuracy: 0.6975 - precision: 0.6731 - recall: 0.7680 - TP: 768.0000 - TN: 627.0000 - FP: 373.0000 - FN: 232.0000\n",
      "[1.1399801969528198, 0.6974999904632568, 0.6730937957763672, 0.7680000066757202, 768.0, 627.0, 373.0, 232.0, 0.7174217788877385, 0.729918509895227, 0.627, 0.6745562130177515]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "250/250 [==============================] - 234s 937ms/step - loss: 0.2053 - accuracy: 0.9112 - precision: 0.9181 - recall: 0.9030 - TP: 3612.0000 - TN: 3678.0000 - FP: 322.0000 - FN: 388.0000\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 235s 938ms/step - loss: 0.0865 - accuracy: 0.9644 - precision: 0.9675 - recall: 0.9610 - TP: 3844.0000 - TN: 3871.0000 - FP: 129.0000 - FN: 156.0000\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 1.0808 - accuracy: 0.7100 - precision: 0.7258 - recall: 0.6750 - TP: 675.0000 - TN: 745.0000 - FP: 255.0000 - FN: 325.0000\n",
      "[1.0808156728744507, 0.7099999785423279, 0.725806474685669, 0.675000011920929, 675.0, 745.0, 255.0, 325.0, 0.6994818824003672, 0.6962616822429907, 0.745, 0.7198067632850241]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "250/250 [==============================] - 234s 937ms/step - loss: 0.1966 - accuracy: 0.9170 - precision: 0.9240 - recall: 0.9087 - TP: 3635.0000 - TN: 3701.0000 - FP: 299.0000 - FN: 365.0000\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 235s 938ms/step - loss: 0.0816 - accuracy: 0.9688 - precision: 0.9713 - recall: 0.9660 - TP: 3864.0000 - TN: 3886.0000 - FP: 114.0000 - FN: 136.0000\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 1.3856 - accuracy: 0.7345 - precision: 0.6636 - recall: 0.9510 - TP: 951.0000 - TN: 518.0000 - FP: 482.0000 - FN: 49.0000\n",
      "[1.3856040239334106, 0.734499990940094, 0.6636427044868469, 0.9509999752044678, 951.0, 518.0, 482.0, 49.0, 0.7817509142422456, 0.9135802469135802, 0.518, 0.6611359285258457]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "250/250 [==============================] - 233s 932ms/step - loss: 0.2042 - accuracy: 0.9074 - precision: 0.9057 - recall: 0.9095 - TP: 3638.0000 - TN: 3621.0000 - FP: 379.0000 - FN: 362.0000\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 232s 929ms/step - loss: 0.0802 - accuracy: 0.9721 - precision: 0.9708 - recall: 0.9735 - TP: 3894.0000 - TN: 3883.0000 - FP: 117.0000 - FN: 106.0000\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 1.1779 - accuracy: 0.7340 - precision: 0.6736 - recall: 0.9080 - TP: 908.0000 - TN: 560.0000 - FP: 440.0000 - FN: 92.0000\n",
      "[1.177861213684082, 0.734000027179718, 0.6735904812812805, 0.9079999923706055, 908.0, 560.0, 440.0, 92.0, 0.7734241727595725, 0.8588957055214724, 0.56, 0.6779661016949153]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "250/250 [==============================] - 233s 930ms/step - loss: 0.2129 - accuracy: 0.9099 - precision: 0.9286 - recall: 0.8880 - TP: 3552.0000 - TN: 3727.0000 - FP: 273.0000 - FN: 448.0000\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 232s 927ms/step - loss: 0.0860 - accuracy: 0.9686 - precision: 0.9713 - recall: 0.9657 - TP: 3863.0000 - TN: 3886.0000 - FP: 114.0000 - FN: 137.0000\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 1.4736 - accuracy: 0.6375 - precision: 0.6449 - recall: 0.6120 - TP: 612.0000 - TN: 663.0000 - FP: 337.0000 - FN: 388.0000\n",
      "[1.4736427068710327, 0.637499988079071, 0.6448893547058105, 0.6119999885559082, 612.0, 663.0, 337.0, 388.0, 0.6280143591250125, 0.6308277830637488, 0.663, 0.6465138956606533]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision_neg</th>\n",
       "      <th>recall_neg</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.188936</td>\n",
       "      <td>0.709900</td>\n",
       "      <td>0.675193</td>\n",
       "      <td>0.814200</td>\n",
       "      <td>814.200000</td>\n",
       "      <td>605.600000</td>\n",
       "      <td>394.400000</td>\n",
       "      <td>185.800000</td>\n",
       "      <td>0.734494</td>\n",
       "      <td>0.781922</td>\n",
       "      <td>0.605600</td>\n",
       "      <td>0.674834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.151846</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.025744</td>\n",
       "      <td>0.109416</td>\n",
       "      <td>109.416432</td>\n",
       "      <td>76.642315</td>\n",
       "      <td>76.642315</td>\n",
       "      <td>109.416432</td>\n",
       "      <td>0.045976</td>\n",
       "      <td>0.086269</td>\n",
       "      <td>0.076642</td>\n",
       "      <td>0.030989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.982353</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.639503</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.628014</td>\n",
       "      <td>0.630828</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.615979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.095607</td>\n",
       "      <td>0.699750</td>\n",
       "      <td>0.665043</td>\n",
       "      <td>0.769000</td>\n",
       "      <td>769.000000</td>\n",
       "      <td>563.000000</td>\n",
       "      <td>355.750000</td>\n",
       "      <td>102.500000</td>\n",
       "      <td>0.717936</td>\n",
       "      <td>0.730694</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.663496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.165532</td>\n",
       "      <td>0.714500</td>\n",
       "      <td>0.673342</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>621.500000</td>\n",
       "      <td>378.500000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.750011</td>\n",
       "      <td>0.790289</td>\n",
       "      <td>0.621500</td>\n",
       "      <td>0.674926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.250219</td>\n",
       "      <td>0.730250</td>\n",
       "      <td>0.679832</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>897.500000</td>\n",
       "      <td>644.250000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>0.765741</td>\n",
       "      <td>0.846721</td>\n",
       "      <td>0.644250</td>\n",
       "      <td>0.684803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.473643</td>\n",
       "      <td>0.746500</td>\n",
       "      <td>0.725806</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>951.000000</td>\n",
       "      <td>745.000000</td>\n",
       "      <td>522.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>0.781751</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.719807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_lower</th>\n",
       "      <td>1.081945</td>\n",
       "      <td>0.688544</td>\n",
       "      <td>0.657054</td>\n",
       "      <td>0.737105</td>\n",
       "      <td>737.105251</td>\n",
       "      <td>551.597873</td>\n",
       "      <td>340.397873</td>\n",
       "      <td>108.705251</td>\n",
       "      <td>0.702099</td>\n",
       "      <td>0.721138</td>\n",
       "      <td>0.551598</td>\n",
       "      <td>0.652999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_upper</th>\n",
       "      <td>1.295927</td>\n",
       "      <td>0.731256</td>\n",
       "      <td>0.693332</td>\n",
       "      <td>0.891295</td>\n",
       "      <td>891.294749</td>\n",
       "      <td>659.602127</td>\n",
       "      <td>448.402127</td>\n",
       "      <td>262.894749</td>\n",
       "      <td>0.766889</td>\n",
       "      <td>0.842707</td>\n",
       "      <td>0.659602</td>\n",
       "      <td>0.696668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss   accuracy  precision     recall          TP          TN  \\\n",
       "count     10.000000  10.000000  10.000000  10.000000   10.000000   10.000000   \n",
       "mean       1.188936   0.709900   0.675193   0.814200  814.200000  605.600000   \n",
       "std        0.151846   0.030309   0.025744   0.109416  109.416432   76.642315   \n",
       "min        0.982353   0.637500   0.639503   0.612000  612.000000  478.000000   \n",
       "25%        1.095607   0.699750   0.665043   0.769000  769.000000  563.000000   \n",
       "50%        1.165532   0.714500   0.673342   0.832000  832.000000  621.500000   \n",
       "75%        1.250219   0.730250   0.679832   0.897500  897.500000  644.250000   \n",
       "max        1.473643   0.746500   0.725806   0.951000  951.000000  745.000000   \n",
       "ci_lower   1.081945   0.688544   0.657054   0.737105  737.105251  551.597873   \n",
       "ci_upper   1.295927   0.731256   0.693332   0.891295  891.294749  659.602127   \n",
       "\n",
       "                  FP          FN         f1  precision_neg  recall_neg  \\\n",
       "count      10.000000   10.000000  10.000000      10.000000   10.000000   \n",
       "mean      394.400000  185.800000   0.734494       0.781922    0.605600   \n",
       "std        76.642315  109.416432   0.045976       0.086269    0.076642   \n",
       "min       255.000000   49.000000   0.628014       0.630828    0.478000   \n",
       "25%       355.750000  102.500000   0.717936       0.730694    0.563000   \n",
       "50%       378.500000  168.000000   0.750011       0.790289    0.621500   \n",
       "75%       437.000000  231.000000   0.765741       0.846721    0.644250   \n",
       "max       522.000000  388.000000   0.781751       0.913580    0.745000   \n",
       "ci_lower  340.397873  108.705251   0.702099       0.721138    0.551598   \n",
       "ci_upper  448.402127  262.894749   0.766889       0.842707    0.659602   \n",
       "\n",
       "             f1_neg  \n",
       "count     10.000000  \n",
       "mean       0.674834  \n",
       "std        0.030989  \n",
       "min        0.615979  \n",
       "25%        0.663496  \n",
       "50%        0.674926  \n",
       "75%        0.684803  \n",
       "max        0.719807  \n",
       "ci_lower   0.652999  \n",
       "ci_upper   0.696668  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index = [val_split for _, val_split in skf_train.split(X_train,y_train)] \n",
    "val_index = [val_split for _, val_split in skf_val.split(X_val,y_val)] \n",
    "\n",
    "test_result = run_test(X_train, y_train, X_val, y_val,\n",
    "                       train_index, val_index, class_weight = {0: 1., 1: 1.}, epochs=2)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc33975",
   "metadata": {},
   "source": [
    "## Split by country, half data (50K) in one fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "54badf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf_train_half = StratifiedKFold(n_splits=2, random_state=0, shuffle=True)\n",
    "skf_val_half = StratifiedKFold(n_splits=2, random_state=0, shuffle=True)\n",
    "\n",
    "train_index = [val_split for _, val_split in skf_train_half.split(X_train,y_train)][0]\n",
    "val_index = [val_split for _, val_split in skf_val_half.split(X_val,y_val)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d89c19b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 956s 765ms/step - loss: 0.1126 - accuracy: 0.9564 - precision: 0.9525 - recall: 0.9609 - TP: 19217.0000 - TN: 19041.0000 - FP: 959.0000 - FN: 783.0000\n",
      "313/313 [==============================] - 76s 242ms/step - loss: 0.7675 - accuracy: 0.7134 - precision: 0.6946 - recall: 0.7616 - TP: 3808.0000 - TN: 3326.0000 - FP: 1674.0000 - FN: 1192.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>0.767456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.694637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.761600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>3808.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>3326.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>1674.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>1192.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.726579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_neg</th>\n",
       "      <td>0.736166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_neg</th>\n",
       "      <td>0.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_neg</th>\n",
       "      <td>0.698886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "loss              0.767456\n",
       "accuracy          0.713400\n",
       "precision         0.694637\n",
       "recall            0.761600\n",
       "TP             3808.000000\n",
       "TN             3326.000000\n",
       "FP             1674.000000\n",
       "FN             1192.000000\n",
       "f1                0.726579\n",
       "precision_neg     0.736166\n",
       "recall_neg        0.665200\n",
       "f1_neg            0.698886"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "\n",
    "train_input1 = np.vstack([x['input_ids'] for x in X_train[train_index]])\n",
    "train_input2 = np.vstack([x['attention_mask'] for x in X_train[train_index]])\n",
    "model.fit(x=[train_input1, train_input2], \n",
    "          y=np.asarray(y_train[train_index]),\n",
    "          epochs = 1,\n",
    "          class_weight={0: 1., 1: 1.})\n",
    "\n",
    "eval_input1 = np.vstack([x['input_ids'] for x in X_val[val_index]])\n",
    "eval_input2 = np.vstack([x['attention_mask'] for x in X_val[val_index]])\n",
    "er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                    y=np.asarray(y_val[val_index]), return_dict=True)\n",
    "f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "\n",
    "precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "recall_neg = er['TN'] / (er['TN']+er['FP'])\n",
    "f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "\n",
    "er = list(er.values())\n",
    "er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "pd.DataFrame(data = er, \n",
    "             index = model.metrics_names + ['f1', 'precision_neg', 'recall_neg', 'f1_neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09301f",
   "metadata": {},
   "source": [
    "## Reduce Batch Size to 1 and train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0d10101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 2351s 59ms/step - loss: 0.1196 - accuracy: 0.9530 - precision: 0.9510 - recall: 0.9552 - TP: 19103.0000 - TN: 19015.0000 - FP: 985.0000 - FN: 897.0000\n",
      "313/313 [==============================] - 79s 253ms/step - loss: 0.8788 - accuracy: 0.7501 - precision: 0.6872 - recall: 0.9180 - TP: 4590.0000 - TN: 2911.0000 - FP: 2089.0000 - FN: 410.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>0.878792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.687229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>4590.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>2911.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>2089.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>410.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.786026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_neg</th>\n",
       "      <td>0.876543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_neg</th>\n",
       "      <td>0.582200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_neg</th>\n",
       "      <td>0.699676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "loss              0.878792\n",
       "accuracy          0.750100\n",
       "precision         0.687229\n",
       "recall            0.918000\n",
       "TP             4590.000000\n",
       "TN             2911.000000\n",
       "FP             2089.000000\n",
       "FN              410.000000\n",
       "f1                0.786026\n",
       "precision_neg     0.876543\n",
       "recall_neg        0.582200\n",
       "f1_neg            0.699676"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = get_model(lr = 3e-5/32)\n",
    "\n",
    "train_input1 = np.vstack([x['input_ids'] for x in X_train[train_index]])\n",
    "train_input2 = np.vstack([x['attention_mask'] for x in X_train[train_index]])\n",
    "model.fit(x=[train_input1, train_input2], \n",
    "          y=np.asarray(y_train[train_index]),\n",
    "          epochs = 1,\n",
    "          batch_size = 1,\n",
    "          class_weight={0: 1., 1: 1.})\n",
    "\n",
    "eval_input1 = np.vstack([x['input_ids'] for x in X_val[val_index]])\n",
    "eval_input2 = np.vstack([x['attention_mask'] for x in X_val[val_index]])\n",
    "er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                    y=np.asarray(y_val[val_index]), return_dict=True)\n",
    "f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "\n",
    "precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "recall_neg = er['TN'] / (er['TN']+er['FP'])\n",
    "f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "\n",
    "er = list(er.values())\n",
    "er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "pd.DataFrame(data = er, \n",
    "             index = model.metrics_names + ['f1', 'precision_neg', 'recall_neg', 'f1_neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2ef6b",
   "metadata": {},
   "source": [
    "## Split by country, all data in one fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "397fbd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 1945s 778ms/step - loss: 0.0855 - accuracy: 0.9671 - precision: 0.9660 - recall: 0.9682 - TP: 38729.0000 - TN: 38636.0000 - FP: 1364.0000 - FN: 1271.0000\n",
      "625/625 [==============================] - 152s 243ms/step - loss: 1.0858 - accuracy: 0.7229 - precision: 0.7179 - recall: 0.7341 - TP: 7341.0000 - TN: 7116.0000 - FP: 2884.0000 - FN: 2659.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.085753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.722850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.717946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>7341.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>7116.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>2884.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>2659.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.725933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_neg</th>\n",
       "      <td>0.727980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_neg</th>\n",
       "      <td>0.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_neg</th>\n",
       "      <td>0.719697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "loss              1.085753\n",
       "accuracy          0.722850\n",
       "precision         0.717946\n",
       "recall            0.734100\n",
       "TP             7341.000000\n",
       "TN             7116.000000\n",
       "FP             2884.000000\n",
       "FN             2659.000000\n",
       "f1                0.725933\n",
       "precision_neg     0.727980\n",
       "recall_neg        0.711600\n",
       "f1_neg            0.719697"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "        \n",
    "train_input1 = np.vstack([x['input_ids'] for x in X_train])\n",
    "train_input2 = np.vstack([x['attention_mask'] for x in X_train])\n",
    "model.fit(x=[train_input1, train_input2], \n",
    "          y=np.asarray(y_train),\n",
    "          epochs = 1,\n",
    "          class_weight={0: 1., 1: 1.})\n",
    "\n",
    "eval_input1 = np.vstack([x['input_ids'] for x in X_val])\n",
    "eval_input2 = np.vstack([x['attention_mask'] for x in X_val])\n",
    "er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                    y=np.asarray(y_val), return_dict=True)\n",
    "f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "\n",
    "precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "recall_neg = er['TN'] / (er['TN']+er['FP'])\n",
    "f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "\n",
    "er = list(er.values())\n",
    "er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "pd.DataFrame(data = er, \n",
    "             index = model.metrics_names + ['f1', 'precision_neg', 'recall_neg', 'f1_neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dfb3be",
   "metadata": {},
   "source": [
    "## Reduce Batch Size to 1 and Try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1de7b0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 4716s 59ms/step - loss: 0.0907 - accuracy: 0.9639 - precision: 0.9654 - recall: 0.9623 - TP: 38491.0000 - TN: 38622.0000 - FP: 1378.0000 - FN: 1509.0000\n",
      "625/625 [==============================] - 154s 247ms/step - loss: 1.0531 - accuracy: 0.7186 - precision: 0.6674 - recall: 0.8713 - TP: 8713.0000 - TN: 5658.0000 - FP: 4342.0000 - FN: 1287.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.053124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.718550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.667407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.871300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>8713.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>5658.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>4342.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>1287.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.755845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_neg</th>\n",
       "      <td>0.814687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_neg</th>\n",
       "      <td>0.565800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_neg</th>\n",
       "      <td>0.667808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "loss              1.053124\n",
       "accuracy          0.718550\n",
       "precision         0.667407\n",
       "recall            0.871300\n",
       "TP             8713.000000\n",
       "TN             5658.000000\n",
       "FP             4342.000000\n",
       "FN             1287.000000\n",
       "f1                0.755845\n",
       "precision_neg     0.814687\n",
       "recall_neg        0.565800\n",
       "f1_neg            0.667808"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = get_model(lr = 3e-5/32)\n",
    "        \n",
    "train_input1 = np.vstack([x['input_ids'] for x in X_train])\n",
    "train_input2 = np.vstack([x['attention_mask'] for x in X_train])\n",
    "model.fit(x=[train_input1, train_input2], \n",
    "          y=np.asarray(y_train),\n",
    "          batch_size = 1,\n",
    "          epochs = 1,\n",
    "          class_weight={0: 1., 1: 1.})\n",
    "\n",
    "eval_input1 = np.vstack([x['input_ids'] for x in X_val])\n",
    "eval_input2 = np.vstack([x['attention_mask'] for x in X_val])\n",
    "er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                    y=np.asarray(y_val), return_dict=True)\n",
    "f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "\n",
    "precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "recall_neg = er['TN'] / (er['TN']+er['FP'])\n",
    "f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "\n",
    "er = list(er.values())\n",
    "er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "pd.DataFrame(data = er, \n",
    "             index = model.metrics_names + ['f1', 'precision_neg', 'recall_neg', 'f1_neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103d4d0",
   "metadata": {},
   "source": [
    "## Try see if shrink learning rate more would influence the training outcome\n",
    "### Split by Country Name Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f71d6a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 761ms/step - loss: 0.2646 - accuracy: 0.8817 - precision: 0.8491 - recall: 0.9285 - TP: 3714.0000 - TN: 3340.0000 - FP: 660.0000 - FN: 286.0000\n",
      "63/63 [==============================] - 15s 246ms/step - loss: 0.7354 - accuracy: 0.7470 - precision: 0.6849 - recall: 0.9150 - TP: 915.0000 - TN: 579.0000 - FP: 421.0000 - FN: 85.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 188s 753ms/step - loss: 0.2195 - accuracy: 0.8980 - precision: 0.8618 - recall: 0.9480 - TP: 3792.0000 - TN: 3392.0000 - FP: 608.0000 - FN: 208.0000\n",
      "63/63 [==============================] - 15s 244ms/step - loss: 0.7700 - accuracy: 0.7455 - precision: 0.6861 - recall: 0.9050 - TP: 905.0000 - TN: 586.0000 - FP: 414.0000 - FN: 95.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 191s 762ms/step - loss: 0.2182 - accuracy: 0.9053 - precision: 0.8839 - recall: 0.9330 - TP: 3732.0000 - TN: 3510.0000 - FP: 490.0000 - FN: 268.0000\n",
      "63/63 [==============================] - 15s 239ms/step - loss: 0.6608 - accuracy: 0.7535 - precision: 0.7286 - recall: 0.8080 - TP: 808.0000 - TN: 699.0000 - FP: 301.0000 - FN: 192.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 192s 766ms/step - loss: 0.2085 - accuracy: 0.9124 - precision: 0.9244 - recall: 0.8982 - TP: 3593.0000 - TN: 3706.0000 - FP: 294.0000 - FN: 407.0000\n",
      "63/63 [==============================] - 15s 239ms/step - loss: 0.9951 - accuracy: 0.7185 - precision: 0.6716 - recall: 0.8550 - TP: 855.0000 - TN: 582.0000 - FP: 418.0000 - FN: 145.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 761ms/step - loss: 0.2004 - accuracy: 0.9144 - precision: 0.9044 - recall: 0.9268 - TP: 3707.0000 - TN: 3608.0000 - FP: 392.0000 - FN: 293.0000\n",
      "63/63 [==============================] - 16s 251ms/step - loss: 1.2437 - accuracy: 0.7060 - precision: 0.6717 - recall: 0.8060 - TP: 806.0000 - TN: 606.0000 - FP: 394.0000 - FN: 194.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 187s 749ms/step - loss: 0.1965 - accuracy: 0.9184 - precision: 0.9135 - recall: 0.9243 - TP: 3697.0000 - TN: 3650.0000 - FP: 350.0000 - FN: 303.0000\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.9702 - accuracy: 0.6765 - precision: 0.6583 - recall: 0.7340 - TP: 734.0000 - TN: 619.0000 - FP: 381.0000 - FN: 266.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 186s 744ms/step - loss: 0.2233 - accuracy: 0.9080 - precision: 0.9123 - recall: 0.9028 - TP: 3611.0000 - TN: 3653.0000 - FP: 347.0000 - FN: 389.0000\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 0.9344 - accuracy: 0.6435 - precision: 0.6245 - recall: 0.7200 - TP: 720.0000 - TN: 567.0000 - FP: 433.0000 - FN: 280.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 191s 766ms/step - loss: 0.2231 - accuracy: 0.9059 - precision: 0.9028 - recall: 0.9097 - TP: 3639.0000 - TN: 3608.0000 - FP: 392.0000 - FN: 361.0000\n",
      "63/63 [==============================] - 15s 239ms/step - loss: 0.8264 - accuracy: 0.7205 - precision: 0.6632 - recall: 0.8960 - TP: 896.0000 - TN: 545.0000 - FP: 455.0000 - FN: 104.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 189s 756ms/step - loss: 0.2740 - accuracy: 0.8839 - precision: 0.8904 - recall: 0.8755 - TP: 3502.0000 - TN: 3569.0000 - FP: 431.0000 - FN: 498.0000\n",
      "63/63 [==============================] - 16s 247ms/step - loss: 1.5849 - accuracy: 0.6695 - precision: 0.6348 - recall: 0.7980 - TP: 798.0000 - TN: 541.0000 - FP: 459.0000 - FN: 202.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 189s 755ms/step - loss: 0.2182 - accuracy: 0.9084 - precision: 0.9038 - recall: 0.9140 - TP: 3656.0000 - TN: 3611.0000 - FP: 389.0000 - FN: 344.0000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.8516 - accuracy: 0.6830 - precision: 0.6794 - recall: 0.6930 - TP: 693.0000 - TN: 673.0000 - FP: 327.0000 - FN: 307.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision_neg</th>\n",
       "      <th>recall_neg</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00001</th>\n",
       "      <td>0.735443</td>\n",
       "      <td>0.7470</td>\n",
       "      <td>0.684880</td>\n",
       "      <td>0.915</td>\n",
       "      <td>915.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.783390</td>\n",
       "      <td>0.871988</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.695913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00002</th>\n",
       "      <td>0.769989</td>\n",
       "      <td>0.7455</td>\n",
       "      <td>0.686126</td>\n",
       "      <td>0.905</td>\n",
       "      <td>905.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.780509</td>\n",
       "      <td>0.860499</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.697204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00003</th>\n",
       "      <td>0.660758</td>\n",
       "      <td>0.7535</td>\n",
       "      <td>0.728584</td>\n",
       "      <td>0.808</td>\n",
       "      <td>808.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.766240</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.739291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00004</th>\n",
       "      <td>0.995074</td>\n",
       "      <td>0.7185</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.855</td>\n",
       "      <td>855.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.752310</td>\n",
       "      <td>0.800550</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.674001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00005</th>\n",
       "      <td>1.243675</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.671667</td>\n",
       "      <td>0.806</td>\n",
       "      <td>806.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>0.732727</td>\n",
       "      <td>0.757500</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.673333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00006</th>\n",
       "      <td>0.970194</td>\n",
       "      <td>0.6765</td>\n",
       "      <td>0.658296</td>\n",
       "      <td>0.734</td>\n",
       "      <td>734.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.694090</td>\n",
       "      <td>0.699435</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.656764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00007</th>\n",
       "      <td>0.934354</td>\n",
       "      <td>0.6435</td>\n",
       "      <td>0.624458</td>\n",
       "      <td>0.720</td>\n",
       "      <td>720.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.668834</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.613969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00008</th>\n",
       "      <td>0.826355</td>\n",
       "      <td>0.7205</td>\n",
       "      <td>0.663212</td>\n",
       "      <td>0.896</td>\n",
       "      <td>896.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.762229</td>\n",
       "      <td>0.839753</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.661007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00009</th>\n",
       "      <td>1.584894</td>\n",
       "      <td>0.6695</td>\n",
       "      <td>0.634845</td>\n",
       "      <td>0.798</td>\n",
       "      <td>798.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0.707133</td>\n",
       "      <td>0.728129</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.620769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00010</th>\n",
       "      <td>0.851595</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.679412</td>\n",
       "      <td>0.693</td>\n",
       "      <td>693.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>0.686139</td>\n",
       "      <td>0.686735</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.679798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             loss  accuracy  precision  recall     TP     TN     FP     FN  \\\n",
       "0.00001  0.735443    0.7470   0.684880   0.915  915.0  579.0  421.0   85.0   \n",
       "0.00002  0.769989    0.7455   0.686126   0.905  905.0  586.0  414.0   95.0   \n",
       "0.00003  0.660758    0.7535   0.728584   0.808  808.0  699.0  301.0  192.0   \n",
       "0.00004  0.995074    0.7185   0.671642   0.855  855.0  582.0  418.0  145.0   \n",
       "0.00005  1.243675    0.7060   0.671667   0.806  806.0  606.0  394.0  194.0   \n",
       "0.00006  0.970194    0.6765   0.658296   0.734  734.0  619.0  381.0  266.0   \n",
       "0.00007  0.934354    0.6435   0.624458   0.720  720.0  567.0  433.0  280.0   \n",
       "0.00008  0.826355    0.7205   0.663212   0.896  896.0  545.0  455.0  104.0   \n",
       "0.00009  1.584894    0.6695   0.634845   0.798  798.0  541.0  459.0  202.0   \n",
       "0.00010  0.851595    0.6830   0.679412   0.693  693.0  673.0  327.0  307.0   \n",
       "\n",
       "               f1  precision_neg  recall_neg    f1_neg  \n",
       "0.00001  0.783390       0.871988       0.579  0.695913  \n",
       "0.00002  0.780509       0.860499       0.586  0.697204  \n",
       "0.00003  0.766240       0.784512       0.699  0.739291  \n",
       "0.00004  0.752310       0.800550       0.582  0.674001  \n",
       "0.00005  0.732727       0.757500       0.606  0.673333  \n",
       "0.00006  0.694090       0.699435       0.619  0.656764  \n",
       "0.00007  0.668834       0.669421       0.567  0.613969  \n",
       "0.00008  0.762229       0.839753       0.545  0.661007  \n",
       "0.00009  0.707133       0.728129       0.541  0.620769  \n",
       "0.00010  0.686139       0.686735       0.673  0.679798  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take 1 fold and run 10 different learning rate\n",
    "train_index = [val_split for _, val_split in skf_train.split(X_train,y_train)][0] \n",
    "val_index = [val_split for _, val_split in skf_val.split(X_val,y_val)][0] \n",
    "eval_results = []\n",
    "lr_list = np.arange(1e-5, 1.1e-4, 1e-5)\n",
    "\n",
    "for lr in lr_list:\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = get_model(lr)\n",
    "\n",
    "    train_input1 = np.vstack([x['input_ids'] for x in X_train[train_index]])\n",
    "    train_input2 = np.vstack([x['attention_mask'] for x in X_train[train_index]])\n",
    "    model.fit(x=[train_input1, train_input2], \n",
    "              y=np.asarray(y_train[train_index]),\n",
    "              epochs = 1,\n",
    "              class_weight={0: 1., 1: 1.})\n",
    "\n",
    "    eval_input1 = np.vstack([x['input_ids'] for x in X_val[val_index]])\n",
    "    eval_input2 = np.vstack([x['attention_mask'] for x in X_val[val_index]])\n",
    "    er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                        y=np.asarray(y_val[val_index]), return_dict=True)\n",
    "    f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "\n",
    "    precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "    recall_neg = er['TN'] / (er['TN']+er['FP'])\n",
    "    f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "\n",
    "    er = list(er.values())\n",
    "    er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "    eval_results.append(er)\n",
    "\n",
    "eval_results = np.array(eval_results)\n",
    "metrics_names = model.metrics_names\n",
    "metrics_names += ['f1', 'precision_neg', 'recall_neg', 'f1_neg']\n",
    "eval_results = pd.DataFrame(eval_results, columns=metrics_names, index = lr_list)\n",
    "eval_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d333a362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1280x320 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "fig, ax = plt.subplots(figsize=(16, 4), ncols=2, dpi=80)\n",
    "\n",
    "sns.lineplot(x=lr_list, y=eval_results['loss'], ax=ax[0])\n",
    "sns.lineplot(x=lr_list, y=eval_results['accuracy'], ax=ax[1])\n",
    "\n",
    "# fig.suptitle('Learning Rate vs. Validation Performance (Batch Size=32)')\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Learning Rate')\n",
    "    # ax.set_ylim([0, 1.7])\n",
    "    ax[i].set_xticks([(i+1) * 1e-5 for i in range(10)])\n",
    "    ax[i].xaxis.set_major_formatter(lambda y, p: f'{1e5*y:.0f}e-5')\n",
    "#     ax[i].legend(loc='best')\n",
    "    \n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "\n",
    "ax[0].set_title('Learning Rate vs. Validation Loss (Batch Size=32)')\n",
    "ax[1].set_title('Learning Rate vs. Validation Accuracy (Batch Size=32)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99dea9",
   "metadata": {},
   "source": [
    "## Try see if batch size would influence the training outcome\n",
    "### Split by Country Name Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "138b0a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 478s 60ms/step - loss: 0.6994 - accuracy: 0.5056 - precision: 0.5060 - recall: 0.4753 - TP: 1901.0000 - TN: 2144.0000 - FP: 1856.0000 - FN: 2099.0000\n",
      "63/63 [==============================] - 16s 252ms/step - loss: 0.6942 - accuracy: 0.5000 - precision: 0.5000 - recall: 1.0000 - TP: 1000.0000 - TN: 0.0000e+00 - FP: 1000.0000 - FN: 0.0000e+00\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-876c256a6636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                        \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                        epochs=1, batch_size=1)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-785346c59c33>\u001b[0m in \u001b[0;36mrun_test\u001b[0;34m(X_train, y_train, X_val, y_val, train_index, val_index, class_weight, epochs, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mprecision_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mrecall_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mf1_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprecision_neg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall_neg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision_neg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "train_index = [val_split for _, val_split in skf_train.split(X_train,y_train)] \n",
    "val_index = [val_split for _, val_split in skf_val.split(X_val,y_val)] \n",
    "\n",
    "test_result = run_test(X_train, y_train, X_val, y_val,\n",
    "                       train_index, val_index,\n",
    "                       class_weight = {0: 1., 1: 1.}, \n",
    "                       epochs=1, batch_size=1)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3803c3",
   "metadata": {},
   "source": [
    "For the sake of running time, we will just look at the validation accuracy of the first fold. We obtained an accuracy of 0.5 by setting learning rate = 5e-5, and batch_size = 1. This is exactly the by-chance prediction accuracy. In previous experiment, we obtained a confidence interval of (68.90, 73.11) of the model accuracy when using batch_size=32. This performance drop is significant enough for us to conclude that the model failed to learn the features when using a mini batch when sample size is small. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a7efc",
   "metadata": {},
   "source": [
    "## Try see if adjusting learning rate would help increase accuracy when batch size is 1\n",
    "### Split by Country Name Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02282085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 468s 59ms/step - loss: 0.2226 - accuracy: 0.9009 - precision: 0.9250 - recall: 0.8725 - TP: 3490.0000 - TN: 3717.0000 - FP: 283.0000 - FN: 510.0000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.6354 - accuracy: 0.7410 - precision: 0.6928 - recall: 0.8660 - TP: 866.0000 - TN: 616.0000 - FP: 384.0000 - FN: 134.0000\n",
      "[0.6354032754898071, 0.7409999966621399, 0.692799985408783, 0.8659999966621399, 866.0, 616.0, 384.0, 134.0, 0.7697777674521928, 0.8213333333333334, 0.616, 0.7040000000000001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 467s 58ms/step - loss: 0.2226 - accuracy: 0.9074 - precision: 0.9030 - recall: 0.9128 - TP: 3651.0000 - TN: 3608.0000 - FP: 392.0000 - FN: 349.0000\n",
      "63/63 [==============================] - 15s 243ms/step - loss: 0.8088 - accuracy: 0.7470 - precision: 0.6900 - recall: 0.8970 - TP: 897.0000 - TN: 597.0000 - FP: 403.0000 - FN: 103.0000\n",
      "[0.8087695837020874, 0.746999979019165, 0.6899999976158142, 0.8970000147819519, 897.0, 597.0, 403.0, 103.0, 0.7800000040652846, 0.8528571428571429, 0.597, 0.7023529411764706]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 467s 58ms/step - loss: 0.2214 - accuracy: 0.9030 - precision: 0.9119 - recall: 0.8923 - TP: 3569.0000 - TN: 3655.0000 - FP: 345.0000 - FN: 431.0000\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 0.7657 - accuracy: 0.7295 - precision: 0.7015 - recall: 0.7990 - TP: 799.0000 - TN: 660.0000 - FP: 340.0000 - FN: 201.0000\n",
      "[0.7656879425048828, 0.7294999957084656, 0.7014925479888916, 0.7990000247955322, 799.0, 660.0, 340.0, 201.0, 0.7470780907591091, 0.7665505226480837, 0.66, 0.709296077377754]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 468s 59ms/step - loss: 0.2232 - accuracy: 0.9045 - precision: 0.9209 - recall: 0.8850 - TP: 3540.0000 - TN: 3696.0000 - FP: 304.0000 - FN: 460.0000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.7240 - accuracy: 0.7775 - precision: 0.7088 - recall: 0.9420 - TP: 942.0000 - TN: 613.0000 - FP: 387.0000 - FN: 58.0000\n",
      "[0.7239514589309692, 0.7774999737739563, 0.7088035941123962, 0.9419999718666077, 942.0, 613.0, 387.0, 58.0, 0.8089308497669188, 0.9135618479880775, 0.613, 0.7336923997606223]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 476s 59ms/step - loss: 0.2158 - accuracy: 0.9130 - precision: 0.9172 - recall: 0.9080 - TP: 3632.0000 - TN: 3672.0000 - FP: 328.0000 - FN: 368.0000\n",
      "63/63 [==============================] - 16s 255ms/step - loss: 0.6364 - accuracy: 0.7480 - precision: 0.7279 - recall: 0.7920 - TP: 792.0000 - TN: 704.0000 - FP: 296.0000 - FN: 208.0000\n",
      "[0.6364432573318481, 0.7480000257492065, 0.7279411554336548, 0.7919999957084656, 792.0, 704.0, 296.0, 208.0, 0.7586206762626987, 0.7719298245614035, 0.704, 0.7364016736401674]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 470s 59ms/step - loss: 0.2150 - accuracy: 0.9114 - precision: 0.8856 - recall: 0.9448 - TP: 3779.0000 - TN: 3512.0000 - FP: 488.0000 - FN: 221.0000\n",
      "63/63 [==============================] - 15s 239ms/step - loss: 0.7222 - accuracy: 0.7220 - precision: 0.6993 - recall: 0.7790 - TP: 779.0000 - TN: 665.0000 - FP: 335.0000 - FN: 221.0000\n",
      "[0.7222388982772827, 0.722000002861023, 0.6992818713188171, 0.7789999842643738, 779.0, 665.0, 335.0, 221.0, 0.7369914806115468, 0.7505643340857788, 0.665, 0.7051961823966066]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 471s 59ms/step - loss: 0.2109 - accuracy: 0.9179 - precision: 0.9122 - recall: 0.9247 - TP: 3699.0000 - TN: 3644.0000 - FP: 356.0000 - FN: 301.0000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.9101 - accuracy: 0.7230 - precision: 0.6591 - recall: 0.9240 - TP: 924.0000 - TN: 522.0000 - FP: 478.0000 - FN: 76.0000\n",
      "[0.910067617893219, 0.7229999899864197, 0.6590585112571716, 0.9240000247955322, 924.0, 522.0, 478.0, 76.0, 0.7693588921377182, 0.8729096989966555, 0.522, 0.6533166458072591]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 468s 59ms/step - loss: 0.2188 - accuracy: 0.9055 - precision: 0.8809 - recall: 0.9377 - TP: 3751.0000 - TN: 3493.0000 - FP: 507.0000 - FN: 249.0000\n",
      "63/63 [==============================] - 15s 244ms/step - loss: 0.7241 - accuracy: 0.7305 - precision: 0.6932 - recall: 0.8270 - TP: 827.0000 - TN: 634.0000 - FP: 366.0000 - FN: 173.0000\n",
      "[0.7241113781929016, 0.7304999828338623, 0.6932104229927063, 0.8270000219345093, 827.0, 634.0, 366.0, 173.0, 0.7542179925591107, 0.7856257744733581, 0.634, 0.7017155506364139]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 469s 59ms/step - loss: 0.2239 - accuracy: 0.9018 - precision: 0.8711 - recall: 0.9430 - TP: 3772.0000 - TN: 3442.0000 - FP: 558.0000 - FN: 228.0000\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 0.8202 - accuracy: 0.7180 - precision: 0.6829 - recall: 0.8140 - TP: 814.0000 - TN: 622.0000 - FP: 378.0000 - FN: 186.0000\n",
      "[0.8201977610588074, 0.7179999947547913, 0.6828858852386475, 0.8140000104904175, 814.0, 622.0, 378.0, 186.0, 0.7427007219909417, 0.7698019801980198, 0.622, 0.6880530973451328]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 468s 58ms/step - loss: 0.2179 - accuracy: 0.9082 - precision: 0.9136 - recall: 0.9018 - TP: 3607.0000 - TN: 3659.0000 - FP: 341.0000 - FN: 393.0000\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.9008 - accuracy: 0.7320 - precision: 0.6671 - recall: 0.9260 - TP: 926.0000 - TN: 538.0000 - FP: 462.0000 - FN: 74.0000\n",
      "[0.9008121490478516, 0.7319999933242798, 0.6671469807624817, 0.9259999990463257, 926.0, 538.0, 462.0, 74.0, 0.7755443928016689, 0.8790849673202614, 0.538, 0.6674937965260547]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision_neg</th>\n",
       "      <th>recall_neg</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.764768</td>\n",
       "      <td>0.736850</td>\n",
       "      <td>0.692262</td>\n",
       "      <td>0.856600</td>\n",
       "      <td>856.600000</td>\n",
       "      <td>617.100000</td>\n",
       "      <td>382.900000</td>\n",
       "      <td>143.400000</td>\n",
       "      <td>0.764322</td>\n",
       "      <td>0.818422</td>\n",
       "      <td>0.617100</td>\n",
       "      <td>0.700152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.096009</td>\n",
       "      <td>0.017583</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.061982</td>\n",
       "      <td>61.982435</td>\n",
       "      <td>55.496647</td>\n",
       "      <td>55.496647</td>\n",
       "      <td>61.982435</td>\n",
       "      <td>0.021248</td>\n",
       "      <td>0.057534</td>\n",
       "      <td>0.055497</td>\n",
       "      <td>0.025770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.635403</td>\n",
       "      <td>0.718000</td>\n",
       "      <td>0.659059</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>779.000000</td>\n",
       "      <td>522.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.736991</td>\n",
       "      <td>0.750564</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.653317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.722667</td>\n",
       "      <td>0.724625</td>\n",
       "      <td>0.684664</td>\n",
       "      <td>0.802750</td>\n",
       "      <td>802.750000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>346.500000</td>\n",
       "      <td>82.750000</td>\n",
       "      <td>0.748863</td>\n",
       "      <td>0.770334</td>\n",
       "      <td>0.601000</td>\n",
       "      <td>0.691469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.744900</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.693005</td>\n",
       "      <td>0.846500</td>\n",
       "      <td>846.500000</td>\n",
       "      <td>619.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>153.500000</td>\n",
       "      <td>0.763990</td>\n",
       "      <td>0.803480</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>0.703176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.817341</td>\n",
       "      <td>0.745500</td>\n",
       "      <td>0.700940</td>\n",
       "      <td>0.917250</td>\n",
       "      <td>917.250000</td>\n",
       "      <td>653.500000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>197.250000</td>\n",
       "      <td>0.774103</td>\n",
       "      <td>0.867897</td>\n",
       "      <td>0.653500</td>\n",
       "      <td>0.708271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.910068</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>0.727941</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>942.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>0.808931</td>\n",
       "      <td>0.913562</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.736402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_lower</th>\n",
       "      <td>0.697120</td>\n",
       "      <td>0.724461</td>\n",
       "      <td>0.678350</td>\n",
       "      <td>0.812927</td>\n",
       "      <td>812.927215</td>\n",
       "      <td>577.997098</td>\n",
       "      <td>343.797098</td>\n",
       "      <td>99.727215</td>\n",
       "      <td>0.749351</td>\n",
       "      <td>0.777884</td>\n",
       "      <td>0.577997</td>\n",
       "      <td>0.681995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_upper</th>\n",
       "      <td>0.832416</td>\n",
       "      <td>0.749239</td>\n",
       "      <td>0.706174</td>\n",
       "      <td>0.900273</td>\n",
       "      <td>900.272785</td>\n",
       "      <td>656.202902</td>\n",
       "      <td>422.002902</td>\n",
       "      <td>187.072785</td>\n",
       "      <td>0.779293</td>\n",
       "      <td>0.858960</td>\n",
       "      <td>0.656203</td>\n",
       "      <td>0.718309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss   accuracy  precision     recall          TP          TN  \\\n",
       "count     10.000000  10.000000  10.000000  10.000000   10.000000   10.000000   \n",
       "mean       0.764768   0.736850   0.692262   0.856600  856.600000  617.100000   \n",
       "std        0.096009   0.017583   0.019744   0.061982   61.982435   55.496647   \n",
       "min        0.635403   0.718000   0.659059   0.779000  779.000000  522.000000   \n",
       "25%        0.722667   0.724625   0.684664   0.802750  802.750000  601.000000   \n",
       "50%        0.744900   0.731250   0.693005   0.846500  846.500000  619.000000   \n",
       "75%        0.817341   0.745500   0.700940   0.917250  917.250000  653.500000   \n",
       "max        0.910068   0.777500   0.727941   0.942000  942.000000  704.000000   \n",
       "ci_lower   0.697120   0.724461   0.678350   0.812927  812.927215  577.997098   \n",
       "ci_upper   0.832416   0.749239   0.706174   0.900273  900.272785  656.202902   \n",
       "\n",
       "                  FP          FN         f1  precision_neg  recall_neg  \\\n",
       "count      10.000000   10.000000  10.000000      10.000000   10.000000   \n",
       "mean      382.900000  143.400000   0.764322       0.818422    0.617100   \n",
       "std        55.496647   61.982435   0.021248       0.057534    0.055497   \n",
       "min       296.000000   58.000000   0.736991       0.750564    0.522000   \n",
       "25%       346.500000   82.750000   0.748863       0.770334    0.601000   \n",
       "50%       381.000000  153.500000   0.763990       0.803480    0.619000   \n",
       "75%       399.000000  197.250000   0.774103       0.867897    0.653500   \n",
       "max       478.000000  221.000000   0.808931       0.913562    0.704000   \n",
       "ci_lower  343.797098   99.727215   0.749351       0.777884    0.577997   \n",
       "ci_upper  422.002902  187.072785   0.779293       0.858960    0.656203   \n",
       "\n",
       "             f1_neg  \n",
       "count     10.000000  \n",
       "mean       0.700152  \n",
       "std        0.025770  \n",
       "min        0.653317  \n",
       "25%        0.691469  \n",
       "50%        0.703176  \n",
       "75%        0.708271  \n",
       "max        0.736402  \n",
       "ci_lower   0.681995  \n",
       "ci_upper   0.718309  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index = [val_split for _, val_split in skf_train.split(X_train,y_train)] \n",
    "val_index = [val_split for _, val_split in skf_val.split(X_val,y_val)] \n",
    "\n",
    "test_result = run_test(X_train, y_train, X_val, y_val,\n",
    "                       train_index, val_index,\n",
    "                       class_weight = {0: 1., 1: 1.}, \n",
    "                       epochs=1, batch_size=1, lr=1e-6)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3f9a1",
   "metadata": {},
   "source": [
    "## Load Shuffled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96124c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val [532, 468]\n",
      "val [1065, 935]\n",
      "val [1607, 1393]\n",
      "val [2144, 1856]\n",
      "val [2690, 2310]\n",
      "train [240, 760]\n",
      "val [3255, 2745]\n",
      "val [3805, 3195]\n",
      "val [4348, 3652]\n",
      "val [4906, 4094]\n",
      "val [5457, 4543]\n",
      "train [481, 1519]\n",
      "val [5984, 5016]\n",
      "val [6519, 5481]\n",
      "val [7059, 5941]\n",
      "val [7638, 6362]\n",
      "val [8191, 6809]\n",
      "train [749, 2251]\n",
      "val [8747, 7253]\n",
      "val [9315, 7685]\n",
      "val [9837, 8163]\n",
      "val [10000, 9000]\n",
      "train [1028, 2972]\n",
      "val [10000, 10000]\n",
      "train [1268, 3732]\n",
      "train [1522, 4478]\n",
      "train [1770, 5230]\n",
      "train [2027, 5973]\n",
      "train [2301, 6699]\n",
      "train [2562, 7438]\n",
      "train [2796, 8204]\n",
      "train [3071, 8929]\n",
      "train [3327, 9673]\n",
      "train [3601, 10399]\n",
      "train [3863, 11137]\n",
      "train [4128, 11872]\n",
      "train [4384, 12616]\n",
      "train [4619, 13381]\n",
      "train [4861, 14139]\n",
      "train [5101, 14899]\n",
      "train [5352, 15648]\n",
      "train [5588, 16412]\n",
      "train [5843, 17157]\n",
      "train [6113, 17887]\n",
      "train [6353, 18647]\n",
      "train [6634, 19366]\n",
      "train [6926, 20074]\n",
      "train [7193, 20807]\n",
      "train [7450, 21550]\n",
      "train [7714, 22286]\n",
      "train [7969, 23031]\n",
      "train [8237, 23763]\n",
      "train [8478, 24522]\n",
      "train [8739, 25261]\n",
      "train [8998, 26002]\n",
      "train [9246, 26754]\n",
      "train [9510, 27490]\n",
      "train [9770, 28230]\n",
      "train [10036, 28964]\n",
      "train [10282, 29718]\n",
      "train [10535, 30465]\n",
      "train [10780, 31220]\n",
      "train [11045, 31955]\n",
      "train [11295, 32705]\n",
      "train [11532, 33468]\n",
      "train [11790, 34210]\n",
      "train [12040, 34960]\n",
      "train [12289, 35711]\n",
      "train [12528, 36472]\n",
      "train [12795, 37205]\n",
      "train [13051, 37949]\n",
      "train [13300, 38700]\n",
      "train [13554, 39446]\n",
      "train [14000, 40000]\n",
      "train [15000, 40000]\n",
      "train [16000, 40000]\n",
      "train [17000, 40000]\n",
      "train [18000, 40000]\n",
      "train [19000, 40000]\n",
      "train [20000, 40000]\n",
      "train [21000, 40000]\n",
      "train [22000, 40000]\n",
      "train [23000, 40000]\n",
      "train [24000, 40000]\n",
      "train [25000, 40000]\n",
      "train [26000, 40000]\n",
      "train [27000, 40000]\n",
      "train [28000, 40000]\n",
      "train [29000, 40000]\n",
      "train [30000, 40000]\n",
      "train [31000, 40000]\n",
      "train [32000, 40000]\n",
      "train [33000, 40000]\n",
      "train [34000, 40000]\n",
      "train [35000, 40000]\n",
      "train [36000, 40000]\n",
      "train [37000, 40000]\n",
      "train [38000, 40000]\n",
      "train [39000, 40000]\n",
      "train [40000, 40000]\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "train_count = 0  \n",
    "val_count = 0  \n",
    "train_label_counter = [0, 0]\n",
    "val_label_counter = [0, 0]\n",
    "\n",
    "train_label_count_max = 4e4 \n",
    "val_label_count_max = 1e4 \n",
    "total_train = 2 * train_label_count_max\n",
    "total_val = 2 * val_label_count_max\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for line in fs.open('s3://compressed-data-sample/shuffled_train.json'):\n",
    "    if train_count >= total_train and val_count >= total_val:\n",
    "        break\n",
    "    json_file = json.loads(line)\n",
    "    country = json_file['country']\n",
    "    label =  int(json_file['country'] in PEACE_COUNTRY)\n",
    "    \n",
    "    if not country in MAJOR_COUNTRY:\n",
    "        if train_label_counter[label] < train_label_count_max :\n",
    "            sent = json_file['content_cleaned_shuffled']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_train.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_train.append(label)\n",
    "            train_count += 1\n",
    "            train_label_counter[label] += 1\n",
    "            if sum(train_label_counter) % 1e3 == 0:\n",
    "                print('train', train_label_counter)\n",
    "    else:\n",
    "        if val_label_counter[label] < val_label_count_max :\n",
    "            sent = json_file['content_cleaned_shuffled']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_val.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_val.append(label)\n",
    "            val_count += 1\n",
    "            val_label_counter[label] += 1\n",
    "            if sum(val_label_counter) % 1e3 == 0:\n",
    "                print('val',  val_label_counter)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c678a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_all = np.hstack([X_train, X_val])\n",
    "y_all = np.hstack([y_train, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d555213",
   "metadata": {},
   "source": [
    "## Randomly split, train on shuffled\n",
    "Train: 8K, Val: 2K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "017f123b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 186s 745ms/step - loss: 0.3399 - accuracy: 0.8457 - precision: 0.8253 - recall: 0.8773 - TP: 3509.0000 - TN: 3257.0000 - FP: 743.0000 - FN: 491.0000\n",
      "63/63 [==============================] - 15s 243ms/step - loss: 0.2497 - accuracy: 0.8935 - precision: 0.8594 - recall: 0.9410 - TP: 941.0000 - TN: 846.0000 - FP: 154.0000 - FN: 59.0000\n",
      "[0.24974550306797028, 0.8934999704360962, 0.8593607544898987, 0.9409999847412109, 941.0, 846.0, 154.0, 59.0, 0.8983293617117518, 0.9348066298342541, 0.846, 0.8881889763779527]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 761ms/step - loss: 0.3456 - accuracy: 0.8413 - precision: 0.8170 - recall: 0.8795 - TP: 3518.0000 - TN: 3212.0000 - FP: 788.0000 - FN: 482.0000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.2183 - accuracy: 0.9180 - precision: 0.9130 - recall: 0.9240 - TP: 924.0000 - TN: 912.0000 - FP: 88.0000 - FN: 76.0000\n",
      "[0.21827912330627441, 0.9179999828338623, 0.9130434989929199, 0.9240000247955322, 924.0, 912.0, 88.0, 76.0, 0.9184890883467274, 0.9230769230769231, 0.912, 0.9175050301810865]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 191s 764ms/step - loss: 0.3898 - accuracy: 0.8232 - precision: 0.7999 - recall: 0.8622 - TP: 3449.0000 - TN: 3137.0000 - FP: 863.0000 - FN: 551.0000\n",
      "63/63 [==============================] - 15s 239ms/step - loss: 0.2354 - accuracy: 0.9035 - precision: 0.9316 - recall: 0.8710 - TP: 871.0000 - TN: 936.0000 - FP: 64.0000 - FN: 129.0000\n",
      "[0.23537808656692505, 0.9035000205039978, 0.9315508008003235, 0.8709999918937683, 871.0, 936.0, 64.0, 129.0, 0.9002583929776822, 0.8788732394366198, 0.936, 0.9065375302663439]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 189s 756ms/step - loss: 0.3329 - accuracy: 0.8537 - precision: 0.8333 - recall: 0.8845 - TP: 3538.0000 - TN: 3292.0000 - FP: 708.0000 - FN: 462.0000\n",
      "63/63 [==============================] - 16s 250ms/step - loss: 0.2229 - accuracy: 0.9155 - precision: 0.9022 - recall: 0.9320 - TP: 932.0000 - TN: 899.0000 - FP: 101.0000 - FN: 68.0000\n",
      "[0.2229403406381607, 0.9154999852180481, 0.9022265076637268, 0.9319999814033508, 932.0, 899.0, 101.0, 68.0, 0.9168716005098024, 0.9296794208893485, 0.899, 0.9140823589222167]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 188s 750ms/step - loss: 0.3379 - accuracy: 0.8520 - precision: 0.8301 - recall: 0.8852 - TP: 3541.0000 - TN: 3275.0000 - FP: 725.0000 - FN: 459.0000\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.2048 - accuracy: 0.9235 - precision: 0.8918 - recall: 0.9640 - TP: 964.0000 - TN: 883.0000 - FP: 117.0000 - FN: 36.0000\n",
      "[0.20475022494792938, 0.9235000014305115, 0.8917669057846069, 0.9639999866485596, 964.0, 883.0, 117.0, 36.0, 0.9264776613649478, 0.9608269858541894, 0.883, 0.9202709744658676]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 186s 742ms/step - loss: 0.3432 - accuracy: 0.8464 - precision: 0.8217 - recall: 0.8848 - TP: 3539.0000 - TN: 3232.0000 - FP: 768.0000 - FN: 461.0000\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 0.2255 - accuracy: 0.9135 - precision: 0.8949 - recall: 0.9370 - TP: 937.0000 - TN: 890.0000 - FP: 110.0000 - FN: 63.0000\n",
      "[0.22545872628688812, 0.9135000109672546, 0.8949379324913025, 0.9369999766349792, 937.0, 890.0, 110.0, 63.0, 0.9154860736890864, 0.9338929695697796, 0.89, 0.9114183307731695]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 191s 764ms/step - loss: 0.3423 - accuracy: 0.8520 - precision: 0.8454 - recall: 0.8615 - TP: 3446.0000 - TN: 3370.0000 - FP: 630.0000 - FN: 554.0000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.2467 - accuracy: 0.9040 - precision: 0.9262 - recall: 0.8780 - TP: 878.0000 - TN: 930.0000 - FP: 70.0000 - FN: 122.0000\n",
      "[0.2467280775308609, 0.9039999842643738, 0.9261603355407715, 0.878000020980835, 878.0, 930.0, 70.0, 122.0, 0.9014373817682054, 0.8840304182509505, 0.93, 0.9064327485380117]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 188s 753ms/step - loss: 0.3550 - accuracy: 0.8474 - precision: 0.8198 - recall: 0.8905 - TP: 3562.0000 - TN: 3217.0000 - FP: 783.0000 - FN: 438.0000\n",
      "63/63 [==============================] - 15s 245ms/step - loss: 0.2538 - accuracy: 0.8955 - precision: 0.8920 - recall: 0.9000 - TP: 900.0000 - TN: 891.0000 - FP: 109.0000 - FN: 100.0000\n",
      "[0.2537692189216614, 0.8955000042915344, 0.8919722437858582, 0.8999999761581421, 900.0, 891.0, 109.0, 100.0, 0.8959681285305682, 0.8990918264379415, 0.891, 0.8950276243093923]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 760ms/step - loss: 0.3428 - accuracy: 0.8506 - precision: 0.8279 - recall: 0.8852 - TP: 3541.0000 - TN: 3264.0000 - FP: 736.0000 - FN: 459.0000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.2496 - accuracy: 0.9035 - precision: 0.8991 - recall: 0.9090 - TP: 909.0000 - TN: 898.0000 - FP: 102.0000 - FN: 91.0000\n",
      "[0.2495790272951126, 0.9035000205039978, 0.8991097807884216, 0.9089999794960022, 909.0, 898.0, 102.0, 91.0, 0.9040278308909374, 0.9079878665318504, 0.898, 0.9029663147310206]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 191s 764ms/step - loss: 0.3434 - accuracy: 0.8407 - precision: 0.8168 - recall: 0.8785 - TP: 3514.0000 - TN: 3212.0000 - FP: 788.0000 - FN: 486.0000\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 0.2549 - accuracy: 0.9035 - precision: 0.9147 - recall: 0.8900 - TP: 890.0000 - TN: 917.0000 - FP: 83.0000 - FN: 110.0000\n",
      "[0.2549424171447754, 0.9035000205039978, 0.9146968126296997, 0.8899999856948853, 890.0, 917.0, 83.0, 110.0, 0.9021794141945089, 0.8928919182083739, 0.917, 0.9047853971386286]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision_neg</th>\n",
       "      <th>recall_neg</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.236157</td>\n",
       "      <td>0.907400</td>\n",
       "      <td>0.902483</td>\n",
       "      <td>0.914600</td>\n",
       "      <td>914.600000</td>\n",
       "      <td>900.200000</td>\n",
       "      <td>99.800000</td>\n",
       "      <td>85.400000</td>\n",
       "      <td>0.907952</td>\n",
       "      <td>0.914516</td>\n",
       "      <td>0.900200</td>\n",
       "      <td>0.906722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.017434</td>\n",
       "      <td>0.009812</td>\n",
       "      <td>0.020601</td>\n",
       "      <td>0.030038</td>\n",
       "      <td>30.037754</td>\n",
       "      <td>25.862027</td>\n",
       "      <td>25.862027</td>\n",
       "      <td>30.037754</td>\n",
       "      <td>0.010420</td>\n",
       "      <td>0.026211</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.009856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.204750</td>\n",
       "      <td>0.893500</td>\n",
       "      <td>0.859361</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.895968</td>\n",
       "      <td>0.878873</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.888189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.223570</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.892714</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>892.500000</td>\n",
       "      <td>890.250000</td>\n",
       "      <td>84.250000</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>0.900553</td>\n",
       "      <td>0.894442</td>\n",
       "      <td>0.890250</td>\n",
       "      <td>0.903421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.241053</td>\n",
       "      <td>0.903750</td>\n",
       "      <td>0.900668</td>\n",
       "      <td>0.916500</td>\n",
       "      <td>916.500000</td>\n",
       "      <td>898.500000</td>\n",
       "      <td>101.500000</td>\n",
       "      <td>83.500000</td>\n",
       "      <td>0.903104</td>\n",
       "      <td>0.915532</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>0.906485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.249704</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.914283</td>\n",
       "      <td>0.935750</td>\n",
       "      <td>935.750000</td>\n",
       "      <td>915.750000</td>\n",
       "      <td>109.750000</td>\n",
       "      <td>107.500000</td>\n",
       "      <td>0.916525</td>\n",
       "      <td>0.932840</td>\n",
       "      <td>0.915750</td>\n",
       "      <td>0.913416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.931551</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>964.000000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>0.926478</td>\n",
       "      <td>0.960827</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.920271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_lower</th>\n",
       "      <td>0.223873</td>\n",
       "      <td>0.900487</td>\n",
       "      <td>0.887967</td>\n",
       "      <td>0.893435</td>\n",
       "      <td>893.435417</td>\n",
       "      <td>881.977632</td>\n",
       "      <td>81.577632</td>\n",
       "      <td>64.235417</td>\n",
       "      <td>0.900611</td>\n",
       "      <td>0.896048</td>\n",
       "      <td>0.881978</td>\n",
       "      <td>0.899777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_upper</th>\n",
       "      <td>0.248441</td>\n",
       "      <td>0.914313</td>\n",
       "      <td>0.916998</td>\n",
       "      <td>0.935765</td>\n",
       "      <td>935.764583</td>\n",
       "      <td>918.422368</td>\n",
       "      <td>118.022368</td>\n",
       "      <td>106.564583</td>\n",
       "      <td>0.915294</td>\n",
       "      <td>0.932984</td>\n",
       "      <td>0.918422</td>\n",
       "      <td>0.913666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss   accuracy  precision     recall          TP          TN  \\\n",
       "count     10.000000  10.000000  10.000000  10.000000   10.000000   10.000000   \n",
       "mean       0.236157   0.907400   0.902483   0.914600  914.600000  900.200000   \n",
       "std        0.017434   0.009812   0.020601   0.030038   30.037754   25.862027   \n",
       "min        0.204750   0.893500   0.859361   0.871000  871.000000  846.000000   \n",
       "25%        0.223570   0.903500   0.892714   0.892500  892.500000  890.250000   \n",
       "50%        0.241053   0.903750   0.900668   0.916500  916.500000  898.500000   \n",
       "75%        0.249704   0.915000   0.914283   0.935750  935.750000  915.750000   \n",
       "max        0.254942   0.923500   0.931551   0.964000  964.000000  936.000000   \n",
       "ci_lower   0.223873   0.900487   0.887967   0.893435  893.435417  881.977632   \n",
       "ci_upper   0.248441   0.914313   0.916998   0.935765  935.764583  918.422368   \n",
       "\n",
       "                  FP          FN         f1  precision_neg  recall_neg  \\\n",
       "count      10.000000   10.000000  10.000000      10.000000   10.000000   \n",
       "mean       99.800000   85.400000   0.907952       0.914516    0.900200   \n",
       "std        25.862027   30.037754   0.010420       0.026211    0.025862   \n",
       "min        64.000000   36.000000   0.895968       0.878873    0.846000   \n",
       "25%        84.250000   64.250000   0.900553       0.894442    0.890250   \n",
       "50%       101.500000   83.500000   0.903104       0.915532    0.898500   \n",
       "75%       109.750000  107.500000   0.916525       0.932840    0.915750   \n",
       "max       154.000000  129.000000   0.926478       0.960827    0.936000   \n",
       "ci_lower   81.577632   64.235417   0.900611       0.896048    0.881978   \n",
       "ci_upper  118.022368  106.564583   0.915294       0.932984    0.918422   \n",
       "\n",
       "             f1_neg  \n",
       "count     10.000000  \n",
       "mean       0.906722  \n",
       "std        0.009856  \n",
       "min        0.888189  \n",
       "25%        0.903421  \n",
       "50%        0.906485  \n",
       "75%        0.913416  \n",
       "max        0.920271  \n",
       "ci_lower   0.899777  \n",
       "ci_upper   0.913666  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Throw out some data to make sample # same as split by country\n",
    "_, train_index = train_test_split(np.array(range(len(X_all))),test_size=0.1, stratify=y_all)\n",
    "\n",
    "X_all_sample, y_all_sample = X_all[train_index], y_all[train_index]\n",
    "\n",
    "train_index, val_index = list(zip(*[train_test_split(np.array(range(len(X_all_sample))), \n",
    "                            test_size=0.2, stratify=y_all_sample) for i in range(10)]))\n",
    "\n",
    "test_result = run_test(X_all_sample, y_all_sample, X_all_sample, y_all_sample, \n",
    "                       train_index, val_index, class_weight = {0: 1., 1: 1.}, epochs=1)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7030b8",
   "metadata": {},
   "source": [
    "## Split by country, train on shuffled\n",
    "Train: 8K, Val: 2K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4fe0750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 191s 763ms/step - loss: 0.3033 - accuracy: 0.8680 - precision: 0.8587 - recall: 0.8810 - TP: 3524.0000 - TN: 3420.0000 - FP: 580.0000 - FN: 476.0000\n",
      "63/63 [==============================] - 15s 245ms/step - loss: 0.7161 - accuracy: 0.7070 - precision: 0.6791 - recall: 0.7850 - TP: 785.0000 - TN: 629.0000 - FP: 371.0000 - FN: 215.0000\n",
      "[0.7161105871200562, 0.7070000171661377, 0.6790657639503479, 0.7850000262260437, 785.0, 629.0, 371.0, 215.0, 0.728200393844333, 0.745260663507109, 0.629, 0.6822125813449024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 188s 751ms/step - loss: 0.3146 - accuracy: 0.8630 - precision: 0.8377 - recall: 0.9005 - TP: 3602.0000 - TN: 3302.0000 - FP: 698.0000 - FN: 398.0000\n",
      "63/63 [==============================] - 15s 243ms/step - loss: 0.7216 - accuracy: 0.6885 - precision: 0.6626 - recall: 0.7680 - TP: 768.0000 - TN: 609.0000 - FP: 391.0000 - FN: 232.0000\n",
      "[0.7215924859046936, 0.6884999871253967, 0.6626402139663696, 0.7680000066757202, 768.0, 609.0, 391.0, 232.0, 0.7114404885406733, 0.7241379310344828, 0.609, 0.661596958174905]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 760ms/step - loss: 0.2969 - accuracy: 0.8664 - precision: 0.8514 - recall: 0.8878 - TP: 3551.0000 - TN: 3380.0000 - FP: 620.0000 - FN: 449.0000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.8174 - accuracy: 0.6665 - precision: 0.6344 - recall: 0.7860 - TP: 786.0000 - TN: 547.0000 - FP: 453.0000 - FN: 214.0000\n",
      "[0.8173850178718567, 0.6664999723434448, 0.6343825459480286, 0.7860000133514404, 786.0, 547.0, 453.0, 214.0, 0.7020991440939578, 0.7187910643889619, 0.547, 0.6212379329926179]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 191s 766ms/step - loss: 0.2843 - accuracy: 0.8779 - precision: 0.8571 - recall: 0.9070 - TP: 3628.0000 - TN: 3395.0000 - FP: 605.0000 - FN: 372.0000\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.7499 - accuracy: 0.6980 - precision: 0.6658 - recall: 0.7950 - TP: 795.0000 - TN: 601.0000 - FP: 399.0000 - FN: 205.0000\n",
      "[0.7499043941497803, 0.6980000138282776, 0.6658291220664978, 0.7950000166893005, 795.0, 601.0, 399.0, 205.0, 0.724703730384139, 0.7456575682382134, 0.601, 0.6655592469545958]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 759ms/step - loss: 0.2895 - accuracy: 0.8770 - precision: 0.8774 - recall: 0.8765 - TP: 3506.0000 - TN: 3510.0000 - FP: 490.0000 - FN: 494.0000\n",
      "63/63 [==============================] - 16s 251ms/step - loss: 0.6349 - accuracy: 0.6490 - precision: 0.6215 - recall: 0.7620 - TP: 762.0000 - TN: 536.0000 - FP: 464.0000 - FN: 238.0000\n",
      "[0.6349123120307922, 0.6489999890327454, 0.6215334534645081, 0.7620000243186951, 762.0, 536.0, 464.0, 238.0, 0.6846361353159119, 0.6925064599483204, 0.536, 0.6042841037204059]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 188s 751ms/step - loss: 0.2987 - accuracy: 0.8636 - precision: 0.8461 - recall: 0.8890 - TP: 3556.0000 - TN: 3353.0000 - FP: 647.0000 - FN: 444.0000\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.8718 - accuracy: 0.6830 - precision: 0.6195 - recall: 0.9490 - TP: 949.0000 - TN: 417.0000 - FP: 583.0000 - FN: 51.0000\n",
      "[0.871759831905365, 0.6830000281333923, 0.6194517016410828, 0.9490000009536743, 949.0, 417.0, 583.0, 51.0, 0.7496050588942219, 0.8910256410256411, 0.417, 0.5681198910081744]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 761ms/step - loss: 0.3017 - accuracy: 0.8680 - precision: 0.8459 - recall: 0.9000 - TP: 3600.0000 - TN: 3344.0000 - FP: 656.0000 - FN: 400.0000\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 1.0394 - accuracy: 0.6940 - precision: 0.6414 - recall: 0.8800 - TP: 880.0000 - TN: 508.0000 - FP: 492.0000 - FN: 120.0000\n",
      "[1.0393527746200562, 0.6940000057220459, 0.6413994431495667, 0.8799999952316284, 880.0, 508.0, 492.0, 120.0, 0.7419898978190188, 0.8089171974522293, 0.508, 0.624078624078624]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 192s 766ms/step - loss: 0.2873 - accuracy: 0.8791 - precision: 0.8675 - recall: 0.8950 - TP: 3580.0000 - TN: 3453.0000 - FP: 547.0000 - FN: 420.0000\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 0.8191 - accuracy: 0.7160 - precision: 0.6598 - recall: 0.8920 - TP: 892.0000 - TN: 540.0000 - FP: 460.0000 - FN: 108.0000\n",
      "[0.8190885186195374, 0.7160000205039978, 0.6597633361816406, 0.8920000195503235, 892.0, 540.0, 460.0, 108.0, 0.7585034233457736, 0.8333333333333334, 0.54, 0.6553398058252428]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 190s 758ms/step - loss: 0.2987 - accuracy: 0.8709 - precision: 0.8433 - recall: 0.9110 - TP: 3644.0000 - TN: 3323.0000 - FP: 677.0000 - FN: 356.0000\n",
      "63/63 [==============================] - 16s 248ms/step - loss: 0.9214 - accuracy: 0.6615 - precision: 0.6454 - recall: 0.7170 - TP: 717.0000 - TN: 606.0000 - FP: 394.0000 - FN: 283.0000\n",
      "[0.9214298129081726, 0.6614999771118164, 0.64536452293396, 0.7170000076293945, 717.0, 606.0, 394.0, 283.0, 0.6792989064036287, 0.6816647919010124, 0.606, 0.6416093170989943]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 202s 807ms/step - loss: 0.2959 - accuracy: 0.8742 - precision: 0.8580 - recall: 0.8970 - TP: 3588.0000 - TN: 3406.0000 - FP: 594.0000 - FN: 412.0000\n",
      "63/63 [==============================] - 16s 255ms/step - loss: 0.7422 - accuracy: 0.7085 - precision: 0.6480 - recall: 0.9130 - TP: 913.0000 - TN: 504.0000 - FP: 496.0000 - FN: 87.0000\n",
      "[0.7421846985816956, 0.7085000276565552, 0.6479772925376892, 0.9129999876022339, 913.0, 504.0, 496.0, 87.0, 0.7579908658252977, 0.8527918781725888, 0.504, 0.6335637963544941]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision_neg</th>\n",
       "      <th>recall_neg</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.803372</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>0.647741</td>\n",
       "      <td>0.824700</td>\n",
       "      <td>824.700000</td>\n",
       "      <td>549.700000</td>\n",
       "      <td>450.300000</td>\n",
       "      <td>175.300000</td>\n",
       "      <td>0.723847</td>\n",
       "      <td>0.769409</td>\n",
       "      <td>0.549700</td>\n",
       "      <td>0.635760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.117392</td>\n",
       "      <td>0.022125</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>0.077126</td>\n",
       "      <td>77.126231</td>\n",
       "      <td>64.405055</td>\n",
       "      <td>64.405055</td>\n",
       "      <td>77.126231</td>\n",
       "      <td>0.028922</td>\n",
       "      <td>0.072082</td>\n",
       "      <td>0.064405</td>\n",
       "      <td>0.033415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.634912</td>\n",
       "      <td>0.649000</td>\n",
       "      <td>0.619452</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>717.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>371.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.679299</td>\n",
       "      <td>0.681665</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.568120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.726741</td>\n",
       "      <td>0.670625</td>\n",
       "      <td>0.636137</td>\n",
       "      <td>0.772250</td>\n",
       "      <td>772.250000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>395.250000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>0.704434</td>\n",
       "      <td>0.720128</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.621948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.783645</td>\n",
       "      <td>0.691250</td>\n",
       "      <td>0.646671</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>790.500000</td>\n",
       "      <td>543.500000</td>\n",
       "      <td>456.500000</td>\n",
       "      <td>209.500000</td>\n",
       "      <td>0.726452</td>\n",
       "      <td>0.745459</td>\n",
       "      <td>0.543500</td>\n",
       "      <td>0.637587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.858592</td>\n",
       "      <td>0.704750</td>\n",
       "      <td>0.661921</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>889.000000</td>\n",
       "      <td>604.750000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>227.750000</td>\n",
       "      <td>0.747701</td>\n",
       "      <td>0.827229</td>\n",
       "      <td>0.604750</td>\n",
       "      <td>0.660033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.039353</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.679066</td>\n",
       "      <td>0.949000</td>\n",
       "      <td>949.000000</td>\n",
       "      <td>629.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>0.758503</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.629000</td>\n",
       "      <td>0.682213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_lower</th>\n",
       "      <td>0.720657</td>\n",
       "      <td>0.671611</td>\n",
       "      <td>0.634073</td>\n",
       "      <td>0.770357</td>\n",
       "      <td>770.356906</td>\n",
       "      <td>504.320239</td>\n",
       "      <td>404.920239</td>\n",
       "      <td>120.956906</td>\n",
       "      <td>0.703469</td>\n",
       "      <td>0.718620</td>\n",
       "      <td>0.504320</td>\n",
       "      <td>0.612216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_upper</th>\n",
       "      <td>0.886087</td>\n",
       "      <td>0.702789</td>\n",
       "      <td>0.661408</td>\n",
       "      <td>0.879043</td>\n",
       "      <td>879.043094</td>\n",
       "      <td>595.079761</td>\n",
       "      <td>495.679761</td>\n",
       "      <td>229.643094</td>\n",
       "      <td>0.744225</td>\n",
       "      <td>0.820198</td>\n",
       "      <td>0.595080</td>\n",
       "      <td>0.659304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss   accuracy  precision     recall          TP          TN  \\\n",
       "count     10.000000  10.000000  10.000000  10.000000   10.000000   10.000000   \n",
       "mean       0.803372   0.687200   0.647741   0.824700  824.700000  549.700000   \n",
       "std        0.117392   0.022125   0.019398   0.077126   77.126231   64.405055   \n",
       "min        0.634912   0.649000   0.619452   0.717000  717.000000  417.000000   \n",
       "25%        0.726741   0.670625   0.636137   0.772250  772.250000  515.000000   \n",
       "50%        0.783645   0.691250   0.646671   0.790500  790.500000  543.500000   \n",
       "75%        0.858592   0.704750   0.661921   0.889000  889.000000  604.750000   \n",
       "max        1.039353   0.716000   0.679066   0.949000  949.000000  629.000000   \n",
       "ci_lower   0.720657   0.671611   0.634073   0.770357  770.356906  504.320239   \n",
       "ci_upper   0.886087   0.702789   0.661408   0.879043  879.043094  595.079761   \n",
       "\n",
       "                  FP          FN         f1  precision_neg  recall_neg  \\\n",
       "count      10.000000   10.000000  10.000000      10.000000   10.000000   \n",
       "mean      450.300000  175.300000   0.723847       0.769409    0.549700   \n",
       "std        64.405055   77.126231   0.028922       0.072082    0.064405   \n",
       "min       371.000000   51.000000   0.679299       0.681665    0.417000   \n",
       "25%       395.250000  111.000000   0.704434       0.720128    0.515000   \n",
       "50%       456.500000  209.500000   0.726452       0.745459    0.543500   \n",
       "75%       485.000000  227.750000   0.747701       0.827229    0.604750   \n",
       "max       583.000000  283.000000   0.758503       0.891026    0.629000   \n",
       "ci_lower  404.920239  120.956906   0.703469       0.718620    0.504320   \n",
       "ci_upper  495.679761  229.643094   0.744225       0.820198    0.595080   \n",
       "\n",
       "             f1_neg  \n",
       "count     10.000000  \n",
       "mean       0.635760  \n",
       "std        0.033415  \n",
       "min        0.568120  \n",
       "25%        0.621948  \n",
       "50%        0.637587  \n",
       "75%        0.660033  \n",
       "max        0.682213  \n",
       "ci_lower   0.612216  \n",
       "ci_upper   0.659304  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index = [val_split for _, val_split in skf_train.split(X_train,y_train)] \n",
    "val_index = [val_split for _, val_split in skf_val.split(X_val,y_val)] \n",
    "\n",
    "test_result = run_test(X_train, y_train, X_val, y_val,\n",
    "                       train_index, val_index, class_weight = {0: 1., 1: 1.}, epochs=1)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42350bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "250/250 [==============================] - 202s 809ms/step - loss: 0.3232 - accuracy: 0.8629 - precision: 0.8432 - recall: 0.8915 - TP: 3566.0000 - TN: 3337.0000 - FP: 663.0000 - FN: 434.0000\n",
      "Epoch 2/2\n",
      "250/250 [==============================] - 201s 803ms/step - loss: 0.1615 - accuracy: 0.9379 - precision: 0.9334 - recall: 0.9430 - TP: 3772.0000 - TN: 3731.0000 - FP: 269.0000 - FN: 228.0000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 1.3473 - accuracy: 0.6765 - precision: 0.6210 - recall: 0.9060 - TP: 906.0000 - TN: 447.0000 - FP: 553.0000 - FN: 94.0000\n",
      "[1.3473315238952637, 0.6765000224113464, 0.6209732890129089, 0.906000018119812, 906.0, 447.0, 553.0, 94.0, 0.736884932394846, 0.8262476894639557, 0.447, 0.5801427644386762]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 76/250 [========>.....................] - ETA: 2:09 - loss: 0.4490 - accuracy: 0.7788 - precision: 0.7554 - recall: 0.8135 - TP: 973.0000 - TN: 921.0000 - FP: 315.0000 - FN: 223.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-45ffd3878e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m test_result = run_test(X_train, y_train, X_val, y_val,\n\u001b[0;32m----> 5\u001b[0;31m                        train_index, val_index, class_weight = {0: 1., 1: 1.}, epochs=2, lr=5e-5)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-258a6f27bed6>\u001b[0m in \u001b[0;36mrun_test\u001b[0;34m(X_train, y_train, X_val, y_val, train_index, val_index, class_weight, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     21\u001b[0m                   \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                   class_weight=class_weight)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0meval_input1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_val_fold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_index = [val_split for _, val_split in skf_train.split(X_train,y_train)] \n",
    "val_index = [val_split for _, val_split in skf_val.split(X_val,y_val)] \n",
    "\n",
    "test_result = run_test(X_train, y_train, X_val, y_val,\n",
    "                       train_index, val_index, class_weight = {0: 1., 1: 1.}, epochs=2, lr=5e-5)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab5e2e",
   "metadata": {},
   "source": [
    "## Split by country, all data in one fold\n",
    "Train: 80K, Val: 20K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21e483df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 1999s 799ms/step - loss: 0.1632 - accuracy: 0.9342 - precision: 0.9289 - recall: 0.9405 - TP: 37618.0000 - TN: 37119.0000 - FP: 2881.0000 - FN: 2382.0000\n",
      "625/625 [==============================] - 155s 247ms/step - loss: 1.1408 - accuracy: 0.6951 - precision: 0.6407 - recall: 0.8881 - TP: 8881.0000 - TN: 5020.0000 - FP: 4980.0000 - FN: 1119.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.140769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.695050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.640719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.888100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>8881.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>5020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>4980.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>1119.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.744395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_neg</th>\n",
       "      <td>0.817723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_neg</th>\n",
       "      <td>0.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_neg</th>\n",
       "      <td>0.622096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "loss              1.140769\n",
       "accuracy          0.695050\n",
       "precision         0.640719\n",
       "recall            0.888100\n",
       "TP             8881.000000\n",
       "TN             5020.000000\n",
       "FP             4980.000000\n",
       "FN             1119.000000\n",
       "f1                0.744395\n",
       "precision_neg     0.817723\n",
       "recall_neg        0.502000\n",
       "f1_neg            0.622096"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "        \n",
    "train_input1 = np.vstack([x['input_ids'] for x in X_train])\n",
    "train_input2 = np.vstack([x['attention_mask'] for x in X_train])\n",
    "model.fit(x=[train_input1, train_input2], \n",
    "          y=np.asarray(y_train),\n",
    "          epochs = 1,\n",
    "          class_weight={0: 1., 1: 1.})\n",
    "\n",
    "eval_input1 = np.vstack([x['input_ids'] for x in X_val])\n",
    "eval_input2 = np.vstack([x['attention_mask'] for x in X_val])\n",
    "er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                    y=np.asarray(y_val), return_dict=True)\n",
    "f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "\n",
    "precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "recall_neg = er['TN'] / (er['TN'] + er['FP'])\n",
    "f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "\n",
    "er = list(er.values())\n",
    "er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "pd.DataFrame(data = er, \n",
    "             index = model.metrics_names + ['f1', 'precision_neg', 'recall_neg', 'f1_neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04e86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
