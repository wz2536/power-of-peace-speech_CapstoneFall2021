{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257451b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 126.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 14.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 78.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (21.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.1.2 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow==2.3.0\n",
      "  Downloading tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4 MB)\n",
      "\u001b[K     |███████████████████████████▉    | 278.6 MB 124.5 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 320.4 MB 43 kB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.36.2)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 59.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 120.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (3.19.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.41.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.14.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (2.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (58.5.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.35.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.8.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.6.0)\n",
      "Installing collected packages: numpy, scipy, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.3\n",
      "    Uninstalling scipy-1.5.3:\n",
      "      Successfully uninstalled scipy-1.5.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.3.4\n",
      "    Uninstalling tensorflow-2.3.4:\n",
      "      Successfully uninstalled tensorflow-2.3.4\n",
      "Successfully installed numpy-1.18.5 scipy-1.4.1 tensorflow-2.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install tensorflow==2.3.0\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaConfig, AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification\n",
    "# import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "08ddac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from xgboost) (1.18.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709a1d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import s3fs\n",
    "import json\n",
    "import re\n",
    "import gc\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "MAX_LEN = 128\n",
    "PEACE_COUNTRY = set(['Australia', 'New Zealand', \n",
    "                 'Belgium', 'Sweden', 'Denmark', \n",
    "                 'Norway', 'Finland', 'Czech Republic', \n",
    "                 'Netherlands', 'Austria'])\n",
    "MAJOR_COUNTRY = set(['Australia', 'India'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38ae47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a827b13e26884862b9a342ce988af005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3ccf91415e4fe6925f529edaf5f8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0f348e4989476cb572b5a5babe40a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a7df78b5b3424c90ad3ea2264a1711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=1, #Binary Classification\n",
    "    dropout=0.1,\n",
    "    attention_dropout=0.1,\n",
    "    output_hidden_states=False,\n",
    "    output_attentions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42030603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoding function parser\n",
    "def regular_encode(texts, tokenizer, maxlen=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Function to encode the word\n",
    "    \"\"\"\n",
    "    # encode the word to vector of integer\n",
    "    enc_di = tokenizer.encode_plus(\n",
    "        texts, \n",
    "        return_attention_mask=True, \n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=maxlen)\n",
    "    \n",
    "    return np.array(enc_di['input_ids']), np.array(enc_di['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ded31",
   "metadata": {},
   "source": [
    "## Load Unshuffled, Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "270b4b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val [532, 468]\n",
      "Val [1065, 935]\n",
      "Val [1607, 1393]\n",
      "Val [2144, 1856]\n",
      "Val [2690, 2310]\n",
      "Train [240, 760]\n",
      "Val [3255, 2745]\n",
      "Val [3805, 3195]\n",
      "Val [4348, 3652]\n",
      "Val [4906, 4094]\n",
      "Val [5457, 4543]\n",
      "Train [481, 1519]\n",
      "Val [5984, 5016]\n",
      "Val [6519, 5481]\n",
      "Val [7059, 5941]\n",
      "Val [7638, 6362]\n",
      "Val [8191, 6809]\n",
      "Train [749, 2251]\n",
      "Val [8747, 7253]\n",
      "Val [9315, 7685]\n",
      "Val [9837, 8163]\n",
      "Val [10000, 9000]\n",
      "Train [1028, 2972]\n",
      "Val [10000, 10000]\n",
      "Train [1268, 3732]\n",
      "Train [1522, 4478]\n",
      "Train [1770, 5230]\n",
      "Train [2027, 5973]\n",
      "Train [2301, 6699]\n",
      "Train [2562, 7438]\n",
      "Train [2796, 8204]\n",
      "Train [3071, 8929]\n",
      "Train [3327, 9673]\n",
      "Train [3601, 10399]\n",
      "Train [3863, 11137]\n",
      "Train [4128, 11872]\n",
      "Train [4384, 12616]\n",
      "Train [4619, 13381]\n",
      "Train [4861, 14139]\n",
      "Train [5101, 14899]\n",
      "Train [5352, 15648]\n",
      "Train [5588, 16412]\n",
      "Train [5843, 17157]\n",
      "Train [6113, 17887]\n",
      "Train [6353, 18647]\n",
      "Train [6634, 19366]\n",
      "Train [6926, 20074]\n",
      "Train [7193, 20807]\n",
      "Train [7450, 21550]\n",
      "Train [7714, 22286]\n",
      "Train [7969, 23031]\n",
      "Train [8237, 23763]\n",
      "Train [8478, 24522]\n",
      "Train [8739, 25261]\n",
      "Train [8998, 26002]\n",
      "Train [9246, 26754]\n",
      "Train [9510, 27490]\n",
      "Train [9770, 28230]\n",
      "Train [10036, 28964]\n",
      "Train [10282, 29718]\n",
      "Train [10535, 30465]\n",
      "Train [10780, 31220]\n",
      "Train [11045, 31955]\n",
      "Train [11295, 32705]\n",
      "Train [11532, 33468]\n",
      "Train [11790, 34210]\n",
      "Train [12040, 34960]\n",
      "Train [12289, 35711]\n",
      "Train [12528, 36472]\n",
      "Train [12795, 37205]\n",
      "Train [13051, 37949]\n",
      "Train [13300, 38700]\n",
      "Train [13554, 39446]\n",
      "Train [14000, 40000]\n",
      "Train [15000, 40000]\n",
      "Train [16000, 40000]\n",
      "Train [17000, 40000]\n",
      "Train [18000, 40000]\n",
      "Train [19000, 40000]\n",
      "Train [20000, 40000]\n",
      "Train [21000, 40000]\n",
      "Train [22000, 40000]\n",
      "Train [23000, 40000]\n",
      "Train [24000, 40000]\n",
      "Train [25000, 40000]\n",
      "Train [26000, 40000]\n",
      "Train [27000, 40000]\n",
      "Train [28000, 40000]\n",
      "Train [29000, 40000]\n",
      "Train [30000, 40000]\n",
      "Train [31000, 40000]\n",
      "Train [32000, 40000]\n",
      "Train [33000, 40000]\n",
      "Train [34000, 40000]\n",
      "Train [35000, 40000]\n",
      "Train [36000, 40000]\n",
      "Train [37000, 40000]\n",
      "Train [38000, 40000]\n",
      "Train [39000, 40000]\n",
      "Train [40000, 40000]\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "train_count = 0  \n",
    "val_count = 0  \n",
    "train_label_counter = [0, 0]\n",
    "val_label_counter = [0, 0]\n",
    "\n",
    "train_label_count_max = 4e4\n",
    "val_label_count_max = 1e4\n",
    "total_train = 2 * train_label_count_max\n",
    "total_val = 2 * val_label_count_max\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for line in fs.open('s3://compressed-data-sample/processed_train.json'):\n",
    "    if train_count >= total_train and val_count >= total_val:\n",
    "        break\n",
    "    json_file = json.loads(line)\n",
    "    country = json_file['country']\n",
    "    label =  int(json_file['country'] in PEACE_COUNTRY)\n",
    "    \n",
    "    if not country in MAJOR_COUNTRY:\n",
    "        if train_label_counter[label] < train_label_count_max :\n",
    "            sent = json_file['content_cleaned']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_train.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_train.append(label)\n",
    "            train_count += 1\n",
    "            train_label_counter[label] += 1\n",
    "            if sum(train_label_counter) % 1e3 == 0:\n",
    "                print('Train', train_label_counter)\n",
    "    else:\n",
    "        if val_label_counter[label] < val_label_count_max :\n",
    "            sent = json_file['content_cleaned']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_val.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_val.append(label)\n",
    "            val_count += 1\n",
    "            val_label_counter[label] += 1\n",
    "            if sum(val_label_counter) % 1e3 == 0:\n",
    "                print('Val', val_label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "44c257b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_all = np.hstack([X_train, X_val])\n",
    "y_all = np.hstack([y_train, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df944e3c",
   "metadata": {},
   "source": [
    "## Prepare for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1898abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def get_model(lr = 3e-5):\n",
    "    bert_model = TFAutoModelForSequenceClassification.from_pretrained('roberta-base', trainable=True, config=config)\n",
    "    input_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='input_ids', dtype='int32')\n",
    "    input_masks_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='attention_mask', dtype='int32')\n",
    "    output_layer = bert_model(input_ids_in, input_masks_ids_in)[0]\n",
    "    output_layer = tf.keras.layers.Activation(activation='sigmoid')(output_layer)\n",
    "    model = tf.keras.Model(inputs=[input_ids_in, input_masks_ids_in], outputs = output_layer)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.5),\n",
    "               tf.keras.metrics.Precision(name='precision', thresholds=0.5),\n",
    "               tf.keras.metrics.Recall(name='recall', thresholds=0.5),\n",
    "               tf.keras.metrics.TruePositives(name='TP', thresholds=0.5),\n",
    "               tf.keras.metrics.TrueNegatives(name='TN', thresholds=0.5),\n",
    "               tf.keras.metrics.FalsePositives(name='FP', thresholds=0.5),\n",
    "               tf.keras.metrics.FalseNegatives(name='FN', thresholds=0.5)]\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define Splits\n",
    "skf_train = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "skf_val = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2b7c5",
   "metadata": {},
   "source": [
    "## Define test runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "456bded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct test function\n",
    "def run_test(X_train, y_train, \n",
    "             X_val, y_val,\n",
    "             train_index, val_index, \n",
    "             class_weight, epochs, batch_size=32, lr=3e-5):\n",
    "    eval_results = []\n",
    "    eval_results_xgb = []\n",
    "    eval_results_svm = []\n",
    "    metrics_names = None\n",
    "\n",
    "    for train_index, val_index in list(zip(train_index, val_index)):\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_val[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_val[val_index]\n",
    "        y_train_fold = np.asarray(y_train_fold)\n",
    "        y_val_fold = np.asarray(y_val_fold)\n",
    "\n",
    "        model = get_model(lr)\n",
    "        \n",
    "        train_input1 = np.vstack([x['input_ids'] for x in X_train_fold])\n",
    "        train_input2 = np.vstack([x['attention_mask'] for x in X_train_fold])\n",
    "        model.fit(x=[train_input1, train_input2], \n",
    "                  y=y_train_fold,\n",
    "                  epochs = epochs, \n",
    "                  batch_size = batch_size,\n",
    "                  class_weight=class_weight)\n",
    "\n",
    "        eval_input1 = np.vstack([x['input_ids'] for x in X_val_fold])\n",
    "        eval_input2 = np.vstack([x['attention_mask'] for x in X_val_fold])\n",
    "        er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                            y=y_val_fold, return_dict=True)\n",
    "        f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "        \n",
    "        precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "        recall_neg = er['TN'] / (er['TN']+er['FP'])\n",
    "        f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "        \n",
    "        er = list(er.values())\n",
    "        er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "        eval_results.append(er)\n",
    "        metrics_names = model.metrics_names\n",
    "        print(er)\n",
    "        \n",
    "        # Get Embeddings \n",
    "        input_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='input_ids', dtype='int32')\n",
    "        input_masks_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='attention_mask', dtype='int32')\n",
    "        output_layer = model.layers[2].layers[0](input_ids_in, input_masks_ids_in)[0]\n",
    "        embedding_model = tf.keras.Model(inputs=[input_ids_in, input_masks_ids_in], outputs = output_layer)\n",
    "        \n",
    "        train_embedding = embedding_model.predict(x=[train_input1, train_input2])[:, 0]\n",
    "        val_embedding = embedding_model.predict(x=[eval_input1, eval_input2])[:, 0]\n",
    "        \n",
    "        # Get XGB results\n",
    "        dtrain = xgb.DMatrix(train_embedding, label=y_train_fold)\n",
    "        dval = xgb.DMatrix(val_embedding, label=y_val_fold)\n",
    "        param = {'max_depth': 2, 'eta': 1,'objective': 'binary:logistic'}\n",
    "        num_round = 10\n",
    "        evallist = [(dval, 'eval'), (dtrain, 'train')]\n",
    "        bst = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)\n",
    "        y_pred = (bst.predict(dval) > 0.5).astype(int)\n",
    "        xgb_acc = np.mean(y_val_fold == y_pred)\n",
    "        xgb_score = precision_recall_fscore_support(y_val_fold, y_pred)\n",
    "        # Accuracy, Precision+, Recall+, F1+, Precision-, Recall-, F1-\n",
    "        er = [xgb_acc, xgb_score[0][1], xgb_score[1][1], xgb_score[2][1], \n",
    "                                 xgb_score[0][0], xgb_score[1][0], xgb_score[2][0]]\n",
    "        print(er)\n",
    "        eval_results_xgb.append(er)\n",
    "        \n",
    "        \n",
    "        # Get SVM results\n",
    "        pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('pca', PCA()),\n",
    "                         ('svc', SVC())])\n",
    "        pipe.fit(train_embedding, y_train_fold)\n",
    "        y_pred = (pipe.predict(val_embedding) > 0.5).astype(int)\n",
    "        svm_acc = np.mean(y_val_fold == y_pred)\n",
    "        svm_score = precision_recall_fscore_support(y_val_fold, y_pred)\n",
    "        # Accuracy, Precision+, Recall+, F1+, Precision-, Recall-, F1-\n",
    "        er = [svm_acc, svm_score[0][1], svm_score[1][1], svm_score[2][1], \n",
    "                                 svm_score[0][0], svm_score[1][0], svm_score[2][0]]\n",
    "        print(er)\n",
    "        eval_results_svm.append(er)\n",
    "        \n",
    "        \n",
    "        ## clear memory\n",
    "        del model\n",
    "        del X_train_fold, X_val_fold, y_train_fold, y_val_fold\n",
    "        del train_input1, train_input2, eval_input1, eval_input2\n",
    "        gc.collect() \n",
    "\n",
    "    \n",
    "    eval_results = np.array(eval_results)\n",
    "    metrics_names += ['f1', 'precision_neg', 'recall_neg', 'f1_neg']\n",
    "    eval_results = pd.DataFrame(eval_results, columns=metrics_names)\n",
    "    ci = eval_results.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "    ci.index = ['ci_lower', 'ci_upper']\n",
    "    eval_results = pd.concat([eval_results.describe(), ci])\n",
    "    \n",
    "    eval_results_xgb = np.array(eval_results_xgb)\n",
    "    eval_results_xgb = pd.DataFrame(eval_results_xgb, columns=['accuracy', 'precision', 'recall', 'f1', 'precision_neg', 'recall_neg', 'f1_neg'])\n",
    "    ci = eval_results_xgb.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "    ci.index = ['ci_lower', 'ci_upper']\n",
    "    eval_results_xgb = pd.concat([eval_results_xgb.describe(), ci])\n",
    "    \n",
    "    eval_results_svm = np.array(eval_results_svm)\n",
    "    eval_results_svm = pd.DataFrame(eval_results_svm, columns=['accuracy', 'precision', 'recall', 'f1', 'precision_neg', 'recall_neg', 'f1_neg'])\n",
    "    ci = eval_results_svm.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "    ci.index = ['ci_lower', 'ci_upper']\n",
    "    eval_results_svm = pd.concat([eval_results_svm.describe(), ci])\n",
    "    \n",
    "    \n",
    "    return eval_results, eval_results_xgb, eval_results_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f34f1",
   "metadata": {},
   "source": [
    "### Random Split - Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c4fc1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 205s 821ms/step - loss: 0.2361 - accuracy: 0.9029 - precision: 0.8916 - recall: 0.9172 - TP: 3669.0000 - TN: 3554.0000 - FP: 446.0000 - FN: 331.0000\n",
      "63/63 [==============================] - 16s 261ms/step - loss: 0.1331 - accuracy: 0.9540 - precision: 0.9809 - recall: 0.9260 - TP: 926.0000 - TN: 982.0000 - FP: 18.0000 - FN: 74.0000\n",
      "[0.13308829069137573, 0.9539999961853027, 0.9809321761131287, 0.9259999990463257, 926.0, 982.0, 18.0, 74.0, 0.9526748837507167, 0.9299242424242424, 0.982, 0.9552529182879378]\n",
      "[22:43:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9475, 0.9470529470529471, 0.948, 0.9475262368815592, 0.9479479479479479, 0.947, 0.9474737368684342]\n",
      "[0.9585, 0.9598796389167502, 0.957, 0.958437656484727, 0.9571286141575274, 0.96, 0.9585621567648526]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 824ms/step - loss: 0.2304 - accuracy: 0.9040 - precision: 0.8955 - recall: 0.9147 - TP: 3659.0000 - TN: 3573.0000 - FP: 427.0000 - FN: 341.0000\n",
      "63/63 [==============================] - 16s 255ms/step - loss: 0.1488 - accuracy: 0.9370 - precision: 0.9740 - recall: 0.8980 - TP: 898.0000 - TN: 976.0000 - FP: 24.0000 - FN: 102.0000\n",
      "[0.14880239963531494, 0.9369999766349792, 0.9739696383476257, 0.8980000019073486, 898.0, 976.0, 24.0, 102.0, 0.9344432925469219, 0.9053803339517625, 0.976, 0.9393647738209817]\n",
      "[22:49:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9455, 0.953204476093591, 0.937, 0.9450327786182552, 0.9380530973451328, 0.954, 0.945959345562717]\n",
      "[0.952, 0.9510978043912176, 0.953, 0.952047952047952, 0.9529058116232465, 0.951, 0.951951951951952]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 825ms/step - loss: 0.2605 - accuracy: 0.8901 - precision: 0.8768 - recall: 0.9078 - TP: 3631.0000 - TN: 3490.0000 - FP: 510.0000 - FN: 369.0000\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.1611 - accuracy: 0.9335 - precision: 0.9790 - recall: 0.8860 - TP: 886.0000 - TN: 981.0000 - FP: 19.0000 - FN: 114.0000\n",
      "[0.16105709969997406, 0.9334999918937683, 0.9790055155754089, 0.8859999775886536, 886.0, 981.0, 19.0, 114.0, 0.9301837104912767, 0.8958904109589041, 0.981, 0.9365155131264916]\n",
      "[22:55:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9485, 0.9498495486459378, 0.947, 0.9484226339509263, 0.9471585244267199, 0.95, 0.9485771342985521]\n",
      "[0.9505, 0.9473684210526315, 0.954, 0.9506726457399103, 0.9536757301107754, 0.947, 0.9503261414952332]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 824ms/step - loss: 0.2522 - accuracy: 0.8961 - precision: 0.8817 - recall: 0.9150 - TP: 3660.0000 - TN: 3509.0000 - FP: 491.0000 - FN: 340.0000\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.1752 - accuracy: 0.9320 - precision: 0.9655 - recall: 0.8960 - TP: 896.0000 - TN: 968.0000 - FP: 32.0000 - FN: 104.0000\n",
      "[0.17521613836288452, 0.9319999814033508, 0.9655172228813171, 0.8960000276565552, 896.0, 968.0, 32.0, 104.0, 0.9294605872221436, 0.9029850746268657, 0.968, 0.9343629343629344]\n",
      "[23:00:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9425, 0.9547790339157246, 0.929, 0.9417131272174355, 0.9308666017526777, 0.956, 0.9432659102121361]\n",
      "[0.9415, 0.9500509683995922, 0.932, 0.9409389197375063, 0.9332679097154073, 0.951, 0.9420505200594353]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 824ms/step - loss: 0.2618 - accuracy: 0.8825 - precision: 0.8780 - recall: 0.8885 - TP: 3554.0000 - TN: 3506.0000 - FP: 494.0000 - FN: 446.0000\n",
      "63/63 [==============================] - 16s 254ms/step - loss: 0.1325 - accuracy: 0.9500 - precision: 0.9620 - recall: 0.9370 - TP: 937.0000 - TN: 963.0000 - FP: 37.0000 - FN: 63.0000\n",
      "[0.13253816962242126, 0.949999988079071, 0.9620122909545898, 0.9369999766349792, 937.0, 963.0, 37.0, 63.0, 0.9493414124082247, 0.9385964912280702, 0.963, 0.9506416584402763]\n",
      "[23:06:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9515, 0.9492537313432836, 0.954, 0.9516209476309226, 0.9537688442211055, 0.949, 0.9513784461152882]\n",
      "[0.959, 0.9535573122529645, 0.965, 0.9592445328031809, 0.9645748987854251, 0.953, 0.9587525150905433]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 826ms/step - loss: 0.2435 - accuracy: 0.8989 - precision: 0.8794 - recall: 0.9245 - TP: 3698.0000 - TN: 3493.0000 - FP: 507.0000 - FN: 302.0000\n",
      "63/63 [==============================] - 16s 257ms/step - loss: 0.1297 - accuracy: 0.9545 - precision: 0.9586 - recall: 0.9500 - TP: 950.0000 - TN: 959.0000 - FP: 41.0000 - FN: 50.0000\n",
      "[0.12968729436397552, 0.9545000195503235, 0.9586276412010193, 0.949999988079071, 950.0, 959.0, 41.0, 50.0, 0.9542943146607797, 0.9504459861248761, 0.959, 0.9547038327526132]\n",
      "[23:11:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9525, 0.9566094853683148, 0.948, 0.9522852837769965, 0.9484638255698712, 0.957, 0.9527127924340467]\n",
      "[0.9615, 0.9675785207700102, 0.955, 0.9612481127327629, 0.9555774925962488, 0.968, 0.9617486338797814]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 823ms/step - loss: 0.2512 - accuracy: 0.8928 - precision: 0.8630 - recall: 0.9337 - TP: 3735.0000 - TN: 3407.0000 - FP: 593.0000 - FN: 265.0000\n",
      "63/63 [==============================] - 17s 263ms/step - loss: 0.1604 - accuracy: 0.9295 - precision: 0.8930 - recall: 0.9760 - TP: 976.0000 - TN: 883.0000 - FP: 117.0000 - FN: 24.0000\n",
      "[0.1603998988866806, 0.9294999837875366, 0.8929551839828491, 0.9760000109672546, 976.0, 883.0, 117.0, 24.0, 0.9326325978443748, 0.9735391400220507, 0.883, 0.9260618772941793]\n",
      "[23:17:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9465, 0.9442786069651742, 0.949, 0.9466334164588529, 0.9487437185929648, 0.944, 0.9463659147869674]\n",
      "[0.952, 0.9457593688362919, 0.959, 0.9523336643495531, 0.9584178498985801, 0.945, 0.9516616314199395]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 197s 787ms/step - loss: 0.2573 - accuracy: 0.8829 - precision: 0.8691 - recall: 0.9015 - TP: 3606.0000 - TN: 3457.0000 - FP: 543.0000 - FN: 394.0000\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.1735 - accuracy: 0.9340 - precision: 0.9687 - recall: 0.8970 - TP: 897.0000 - TN: 971.0000 - FP: 29.0000 - FN: 103.0000\n",
      "[0.17351080477237701, 0.9340000152587891, 0.9686825275421143, 0.8970000147819519, 897.0, 971.0, 29.0, 103.0, 0.9314641926615262, 0.904096834264432, 0.971, 0.9363548698167793]\n",
      "[23:22:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.945, 0.9405940594059405, 0.95, 0.9452736318407959, 0.9494949494949495, 0.94, 0.9447236180904524]\n",
      "[0.949, 0.9472111553784861, 0.951, 0.9491017964071856, 0.9508032128514057, 0.947, 0.9488977955911825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 199s 797ms/step - loss: 0.2623 - accuracy: 0.8879 - precision: 0.8743 - recall: 0.9060 - TP: 3624.0000 - TN: 3479.0000 - FP: 521.0000 - FN: 376.0000\n",
      "63/63 [==============================] - 16s 257ms/step - loss: 0.1354 - accuracy: 0.9490 - precision: 0.9563 - recall: 0.9410 - TP: 941.0000 - TN: 957.0000 - FP: 43.0000 - FN: 59.0000\n",
      "[0.13537751138210297, 0.9490000009536743, 0.9563007950782776, 0.9409999847412109, 941.0, 957.0, 43.0, 59.0, 0.9485886931035602, 0.9419291338582677, 0.957, 0.9494047619047619]\n",
      "[23:28:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9545, 0.9513406156901688, 0.958, 0.9546586945690084, 0.9577039274924471, 0.951, 0.9543401906673357]\n",
      "[0.957, 0.9489194499017681, 0.966, 0.9573835480673935, 0.9653767820773931, 0.948, 0.9566094853683148]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 192s 768ms/step - loss: 0.2446 - accuracy: 0.8978 - precision: 0.8788 - recall: 0.9227 - TP: 3691.0000 - TN: 3491.0000 - FP: 509.0000 - FN: 309.0000\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.1581 - accuracy: 0.9450 - precision: 0.9754 - recall: 0.9130 - TP: 913.0000 - TN: 977.0000 - FP: 23.0000 - FN: 87.0000\n",
      "[0.15811298787593842, 0.9449999928474426, 0.9754273295402527, 0.9129999876022339, 913.0, 977.0, 23.0, 87.0, 0.9431818018018381, 0.918233082706767, 0.977, 0.9467054263565892]\n",
      "[23:33:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.952, 0.9565656565656566, 0.947, 0.9517587939698493, 0.9475247524752475, 0.957, 0.9522388059701493]\n",
      "[0.9545, 0.9577039274924471, 0.951, 0.9543401906673357, 0.9513406156901688, 0.958, 0.9546586945690084]\n",
      "               loss   accuracy  precision     recall          TP          TN  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000   10.000000   10.000000   \n",
      "mean       0.150779   0.941850   0.961343   0.922000  922.000000  961.700000   \n",
      "std        0.017313   0.009658   0.025462   0.028975   28.975085   29.009768   \n",
      "min        0.129687   0.929500   0.892955   0.886000  886.000000  883.000000   \n",
      "25%        0.133661   0.933625   0.959474   0.897250  897.250000  960.000000   \n",
      "50%        0.153458   0.941000   0.967100   0.919500  919.500000  969.500000   \n",
      "75%        0.160893   0.949750   0.975063   0.940000  940.000000  976.750000   \n",
      "max        0.175216   0.954500   0.980932   0.976000  976.000000  982.000000   \n",
      "ci_lower   0.138581   0.935045   0.943402   0.901584  901.584173  941.259735   \n",
      "ci_upper   0.162978   0.948655   0.979284   0.942416  942.415827  982.140265   \n",
      "\n",
      "                  FP          FN         f1  precision_neg  recall_neg  \\\n",
      "count      10.000000   10.000000  10.000000      10.000000    10.00000   \n",
      "mean       38.300000   78.000000   0.940627       0.926102     0.96170   \n",
      "std        29.009768   28.975085   0.009987       0.025137     0.02901   \n",
      "min        18.000000   24.000000   0.929461       0.895890     0.88300   \n",
      "25%        23.250000   60.000000   0.931756       0.904418     0.96000   \n",
      "50%        30.500000   80.500000   0.938813       0.924079     0.96950   \n",
      "75%        40.000000  102.750000   0.949153       0.941096     0.97675   \n",
      "max       117.000000  114.000000   0.954294       0.973539     0.98200   \n",
      "ci_lower   17.859735   57.584173   0.933590       0.908391     0.94126   \n",
      "ci_upper   58.740265   98.415827   0.947663       0.943814     0.98214   \n",
      "\n",
      "             f1_neg  \n",
      "count     10.000000  \n",
      "mean       0.942937  \n",
      "std        0.009782  \n",
      "min        0.926062  \n",
      "25%        0.936395  \n",
      "50%        0.943035  \n",
      "75%        0.950332  \n",
      "max        0.955253  \n",
      "ci_lower   0.936045  \n",
      "ci_upper   0.949829  \n",
      "           accuracy  precision     recall         f1  precision_neg  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
      "mean       0.948600   0.950353   0.946700   0.948493       0.946973   \n",
      "std        0.003879   0.005280   0.008220   0.004021       0.007552   \n",
      "min        0.942500   0.940594   0.929000   0.941713       0.930867   \n",
      "25%        0.945750   0.947603   0.947000   0.945614       0.947250   \n",
      "50%        0.948000   0.950595   0.948000   0.947974       0.948206   \n",
      "75%        0.951875   0.954385   0.949750   0.951724       0.949307   \n",
      "max        0.954500   0.956609   0.958000   0.954659       0.957704   \n",
      "ci_lower   0.945867   0.946632   0.940908   0.945659       0.941651   \n",
      "ci_upper   0.951333   0.954073   0.952492   0.951326       0.952294   \n",
      "\n",
      "          recall_neg     f1_neg  \n",
      "count      10.000000  10.000000  \n",
      "mean        0.950500   0.948704  \n",
      "std         0.005720   0.003763  \n",
      "min         0.940000   0.943266  \n",
      "25%         0.947500   0.946061  \n",
      "50%         0.950500   0.948025  \n",
      "75%         0.955500   0.952024  \n",
      "max         0.957000   0.954340  \n",
      "ci_lower    0.946469   0.946052  \n",
      "ci_upper    0.954531   0.951355  \n",
      "           accuracy  precision     recall         f1  precision_neg  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
      "mean       0.953550   0.952913   0.954300   0.953575       0.954307   \n",
      "std        0.005871   0.006903   0.009440   0.005979       0.008948   \n",
      "min        0.941500   0.945759   0.932000   0.940939       0.933268   \n",
      "25%        0.950875   0.947756   0.951500   0.951016       0.951732   \n",
      "50%        0.953250   0.950574   0.954500   0.953337       0.954627   \n",
      "75%        0.958125   0.956667   0.958500   0.958174       0.958096   \n",
      "max        0.961500   0.967579   0.966000   0.961248       0.965377   \n",
      "ci_lower   0.949413   0.948049   0.947648   0.949362       0.948002   \n",
      "ci_upper   0.957687   0.957776   0.960952   0.957788       0.960611   \n",
      "\n",
      "          recall_neg     f1_neg  \n",
      "count      10.000000  10.000000  \n",
      "mean        0.952800   0.953522  \n",
      "std         0.007208   0.005777  \n",
      "min         0.945000   0.942051  \n",
      "25%         0.947250   0.950660  \n",
      "50%         0.951000   0.953305  \n",
      "75%         0.956750   0.958074  \n",
      "max         0.968000   0.961749  \n",
      "ci_lower    0.947721   0.949451  \n",
      "ci_upper    0.957879   0.957593  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Throw out some data to make sample # same as split by country\n",
    "_, train_index = train_test_split(np.array(range(len(X_all))),test_size=0.1, stratify=y_all)\n",
    "\n",
    "train_index, val_index = list(zip(*[train_test_split(np.array(range(len(X_all))), \n",
    "                            test_size=0.2, stratify=y_all) for i in range(10)]))\n",
    "\n",
    "test_result, test_result_xgb, test_result_svm = run_test(X_all, y_all, X_all, y_all, \n",
    "                                                         train_index,val_index,class_weight = {0: 1., 1: 1.}, epochs=1)\n",
    "print(test_result)\n",
    "print(test_result_xgb)\n",
    "print(test_result_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983f864",
   "metadata": {},
   "source": [
    "### Split by country - Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a3db67b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 203s 811ms/step - loss: 0.2099 - accuracy: 0.9120 - precision: 0.8874 - recall: 0.9438 - TP: 3775.0000 - TN: 3521.0000 - FP: 479.0000 - FN: 225.0000\n",
      "63/63 [==============================] - 16s 262ms/step - loss: 0.9802 - accuracy: 0.6765 - precision: 0.6544 - recall: 0.7480 - TP: 748.0000 - TN: 605.0000 - FP: 395.0000 - FN: 252.0000\n",
      "[0.9802479147911072, 0.6765000224113464, 0.6544181704521179, 0.7480000257492065, 748.0, 605.0, 395.0, 252.0, 0.6980867899102214, 0.705950991831972, 0.605, 0.6515885837372105]\n",
      "[01:14:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.679, 0.6559233449477352, 0.753, 0.7011173184357542, 0.710093896713615, 0.605, 0.6533477321814255]\n",
      "[0.677, 0.6523235800344234, 0.758, 0.7012025901942646, 0.711217183770883, 0.596, 0.6485310119695321]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 825ms/step - loss: 0.2001 - accuracy: 0.9112 - precision: 0.9268 - recall: 0.8930 - TP: 3572.0000 - TN: 3718.0000 - FP: 282.0000 - FN: 428.0000\n",
      "63/63 [==============================] - 16s 255ms/step - loss: 0.9103 - accuracy: 0.6535 - precision: 0.6795 - recall: 0.5810 - TP: 581.0000 - TN: 726.0000 - FP: 274.0000 - FN: 419.0000\n",
      "[0.9102622866630554, 0.6535000205039978, 0.6795321702957153, 0.5809999704360962, 581.0, 726.0, 274.0, 419.0, 0.6264150799407273, 0.634061135371179, 0.726, 0.6769230769230768]\n",
      "[01:20:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.714, 0.6739837398373983, 0.829, 0.7434977578475335, 0.7779220779220779, 0.599, 0.6768361581920904]\n",
      "[0.721, 0.6823432343234324, 0.827, 0.7477396021699819, 0.7804568527918782, 0.615, 0.6879194630872483]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 824ms/step - loss: 0.1985 - accuracy: 0.9175 - precision: 0.9026 - recall: 0.9360 - TP: 3744.0000 - TN: 3596.0000 - FP: 404.0000 - FN: 256.0000\n",
      "63/63 [==============================] - 17s 263ms/step - loss: 1.1857 - accuracy: 0.7130 - precision: 0.6521 - recall: 0.9130 - TP: 913.0000 - TN: 513.0000 - FP: 487.0000 - FN: 87.0000\n",
      "[1.18565833568573, 0.7129999995231628, 0.6521428823471069, 0.9129999876022339, 913.0, 513.0, 487.0, 87.0, 0.7608333461814452, 0.855, 0.513, 0.64125]\n",
      "[01:25:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.6955, 0.652615144418423, 0.836, 0.7330118369136344, 0.7719054242002782, 0.555, 0.6457242582897034]\n",
      "[0.7205, 0.6646751306945482, 0.89, 0.7610089781958101, 0.8335854765506808, 0.551, 0.6634557495484649]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 207s 827ms/step - loss: 0.1867 - accuracy: 0.9222 - precision: 0.9062 - recall: 0.9420 - TP: 3768.0000 - TN: 3610.0000 - FP: 390.0000 - FN: 232.0000\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.9427 - accuracy: 0.6980 - precision: 0.6810 - recall: 0.7450 - TP: 745.0000 - TN: 651.0000 - FP: 349.0000 - FN: 255.0000\n",
      "[0.9427357912063599, 0.6980000138282776, 0.6809871792793274, 0.7450000047683716, 745.0, 651.0, 349.0, 255.0, 0.7115568183021321, 0.7185430463576159, 0.651, 0.683105981112277]\n",
      "[01:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.707, 0.6745362563237775, 0.8, 0.7319304666056726, 0.7542997542997543, 0.614, 0.6769570011025359]\n",
      "[0.7285, 0.6908939014202172, 0.827, 0.752844788347747, 0.7845579078455791, 0.63, 0.6988352745424294]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 207s 830ms/step - loss: 0.1939 - accuracy: 0.9195 - precision: 0.9187 - recall: 0.9205 - TP: 3682.0000 - TN: 3674.0000 - FP: 326.0000 - FN: 318.0000\n",
      "63/63 [==============================] - 16s 261ms/step - loss: 0.9428 - accuracy: 0.6850 - precision: 0.6911 - recall: 0.6690 - TP: 669.0000 - TN: 701.0000 - FP: 299.0000 - FN: 331.0000\n",
      "[0.9428409337997437, 0.6850000023841858, 0.69111567735672, 0.6690000295639038, 669.0, 701.0, 299.0, 331.0, 0.6798780518909281, 0.6792635658914729, 0.701, 0.6899606299212597]\n",
      "[01:36:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.6855, 0.6725581395348837, 0.723, 0.6968674698795181, 0.7005405405405405, 0.648, 0.6732467532467532]\n",
      "[0.6885, 0.6779981114258735, 0.718, 0.697425934919864, 0.7003188097768331, 0.659, 0.6790314270994333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 825ms/step - loss: 0.2179 - accuracy: 0.9028 - precision: 0.9062 - recall: 0.8985 - TP: 3594.0000 - TN: 3628.0000 - FP: 372.0000 - FN: 406.0000\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.6010 - accuracy: 0.6805 - precision: 0.6814 - recall: 0.6780 - TP: 678.0000 - TN: 683.0000 - FP: 317.0000 - FN: 322.0000\n",
      "[0.6009505391120911, 0.6804999709129333, 0.6814070343971252, 0.6779999732971191, 678.0, 683.0, 317.0, 322.0, 0.6796992343144239, 0.6796019900497512, 0.683, 0.6812967581047381]\n",
      "[01:42:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.7165, 0.6781893004115226, 0.824, 0.7440180586907449, 0.7757961783439491, 0.609, 0.6823529411764706]\n",
      "[0.7335, 0.6834249803613511, 0.87, 0.7655081390233172, 0.8211829436038515, 0.597, 0.6913723219455704]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 826ms/step - loss: 0.1996 - accuracy: 0.9129 - precision: 0.8982 - recall: 0.9312 - TP: 3725.0000 - TN: 3578.0000 - FP: 422.0000 - FN: 275.0000\n",
      "63/63 [==============================] - 16s 257ms/step - loss: 1.1919 - accuracy: 0.7230 - precision: 0.6562 - recall: 0.9370 - TP: 937.0000 - TN: 509.0000 - FP: 491.0000 - FN: 63.0000\n",
      "[1.1918773651123047, 0.7229999899864197, 0.656162440776825, 0.9369999766349792, 937.0, 509.0, 491.0, 63.0, 0.7718286408933218, 0.8898601398601399, 0.509, 0.6475826972010179]\n",
      "[01:48:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.7285, 0.6862265688671557, 0.842, 0.756174225415357, 0.795601552393273, 0.615, 0.6937394247038917]\n",
      "[0.7205, 0.6802943581357318, 0.832, 0.7485380116959064, 0.7837837837837838, 0.609, 0.6854248733821047]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 207s 827ms/step - loss: 0.1955 - accuracy: 0.9212 - precision: 0.9134 - recall: 0.9308 - TP: 3723.0000 - TN: 3647.0000 - FP: 353.0000 - FN: 277.0000\n",
      "63/63 [==============================] - 16s 255ms/step - loss: 0.9637 - accuracy: 0.6630 - precision: 0.6760 - recall: 0.6260 - TP: 626.0000 - TN: 700.0000 - FP: 300.0000 - FN: 374.0000\n",
      "[0.9636971354484558, 0.6629999876022339, 0.676025927066803, 0.6259999871253967, 626.0, 700.0, 300.0, 374.0, 0.6500519183641734, 0.6517690875232774, 0.7, 0.6750241080038571]\n",
      "[01:53:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.729, 0.6805993690851735, 0.863, 0.7610229276895942, 0.8128415300546448, 0.595, 0.6870669745958429]\n",
      "[0.736, 0.6900161030595813, 0.857, 0.7644959857270294, 0.8113456464379947, 0.615, 0.6996587030716723]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 207s 827ms/step - loss: 0.1874 - accuracy: 0.9244 - precision: 0.9241 - recall: 0.9247 - TP: 3699.0000 - TN: 3696.0000 - FP: 304.0000 - FN: 301.0000\n",
      "63/63 [==============================] - 16s 257ms/step - loss: 0.9279 - accuracy: 0.7195 - precision: 0.6644 - recall: 0.8870 - TP: 887.0000 - TN: 552.0000 - FP: 448.0000 - FN: 113.0000\n",
      "[0.9279272556304932, 0.7195000052452087, 0.6644194722175598, 0.8870000243186951, 887.0, 552.0, 448.0, 113.0, 0.7597430473583299, 0.8300751879699249, 0.552, 0.6630630630630631]\n",
      "[01:59:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.7245, 0.6794564348521183, 0.85, 0.7552199022656596, 0.7997329773030708, 0.599, 0.6849628359062321]\n",
      "[0.7285, 0.6853203568532036, 0.845, 0.7568293775190327, 0.7979139504563233, 0.612, 0.6926994906621391]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 207s 828ms/step - loss: 0.2300 - accuracy: 0.8979 - precision: 0.9034 - recall: 0.8910 - TP: 3564.0000 - TN: 3619.0000 - FP: 381.0000 - FN: 436.0000\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.8733 - accuracy: 0.6950 - precision: 0.6653 - recall: 0.7850 - TP: 785.0000 - TN: 605.0000 - FP: 395.0000 - FN: 215.0000\n",
      "[0.8733049035072327, 0.6949999928474426, 0.6652542352676392, 0.7850000262260437, 785.0, 605.0, 395.0, 215.0, 0.7201834960915343, 0.7378048780487805, 0.605, 0.6648351648351648]\n",
      "[02:04:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.701, 0.6680602006688964, 0.799, 0.7276867030965392, 0.75, 0.603, 0.6685144124168514]\n",
      "[0.702, 0.6717687074829932, 0.79, 0.7261029411764707, 0.7451456310679612, 0.614, 0.6732456140350876]\n",
      "               loss   accuracy  precision     recall          TP          TN  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000   10.000000   10.000000   \n",
      "mean       0.951950   0.690700   0.670147   0.756900  756.900000  624.500000   \n",
      "std        0.164989   0.023407   0.013460   0.123273  123.273364   80.116928   \n",
      "min        0.600951   0.653500   0.652143   0.581000  581.000000  509.000000   \n",
      "25%        0.914679   0.677500   0.658227   0.671250  671.250000  565.250000   \n",
      "50%        0.942788   0.690000   0.670640   0.746500  746.500000  628.000000   \n",
      "75%        0.976110   0.709250   0.680623   0.861500  861.500000  695.750000   \n",
      "max        1.191877   0.723000   0.691116   0.937000  937.000000  726.000000   \n",
      "ci_lower   0.835699   0.674207   0.660663   0.670042  670.041665  568.049663   \n",
      "ci_upper   1.068201   0.707193   0.679630   0.843758  843.758335  680.950337   \n",
      "\n",
      "                  FP          FN         f1  precision_neg  recall_neg  \\\n",
      "count      10.000000   10.000000  10.000000      10.000000   10.000000   \n",
      "mean      375.500000  243.100000   0.705828       0.738193    0.624500   \n",
      "std        80.116928  123.273364   0.048718       0.089277    0.080117   \n",
      "min       274.000000   63.000000   0.626415       0.634061    0.509000   \n",
      "25%       304.250000  138.500000   0.679744       0.679348    0.565250   \n",
      "50%       372.000000  253.500000   0.704822       0.712247    0.628000   \n",
      "75%       434.750000  328.750000   0.749853       0.807008    0.695750   \n",
      "max       491.000000  419.000000   0.771829       0.889860    0.726000   \n",
      "ci_lower  319.049663  156.241665   0.671501       0.675289    0.568050   \n",
      "ci_upper  431.950337  329.958335   0.740154       0.801097    0.680950   \n",
      "\n",
      "             f1_neg  \n",
      "count     10.000000  \n",
      "mean       0.667463  \n",
      "std        0.016497  \n",
      "min        0.641250  \n",
      "25%        0.654457  \n",
      "50%        0.669930  \n",
      "75%        0.680203  \n",
      "max        0.689961  \n",
      "ci_lower   0.655839  \n",
      "ci_upper   0.679087  \n",
      "           accuracy  precision     recall         f1  precision_neg  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
      "mean       0.708050   0.672215   0.811900   0.735055       0.764873   \n",
      "std        0.017623   0.010699   0.044308   0.022002       0.036926   \n",
      "min        0.679000   0.652615   0.723000   0.696867       0.700541   \n",
      "25%        0.696875   0.669185   0.799250   0.728748       0.751075   \n",
      "50%        0.710500   0.674260   0.826500   0.738255       0.773851   \n",
      "75%        0.722500   0.679140   0.840500   0.752419       0.791182   \n",
      "max        0.729000   0.686227   0.863000   0.761023       0.812842   \n",
      "ci_lower   0.695633   0.664676   0.780681   0.719552       0.738855   \n",
      "ci_upper   0.720467   0.679753   0.843119   0.750557       0.790892   \n",
      "\n",
      "          recall_neg     f1_neg  \n",
      "count      10.000000  10.000000  \n",
      "mean        0.604200   0.674275  \n",
      "std         0.022890   0.014993  \n",
      "min         0.555000   0.645724  \n",
      "25%         0.599000   0.669697  \n",
      "50%         0.604000   0.676897  \n",
      "75%         0.612750   0.684310  \n",
      "max         0.648000   0.693739  \n",
      "ci_lower    0.588072   0.663711  \n",
      "ci_upper    0.620328   0.684839  \n",
      "           accuracy  precision     recall         f1  precision_neg  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
      "mean       0.715600   0.677906   0.821400   0.742170       0.776951   \n",
      "std        0.019877   0.011988   0.052456   0.025248       0.044856   \n",
      "min        0.677000   0.652324   0.718000   0.697426       0.700319   \n",
      "25%        0.706625   0.673326   0.799250   0.731512       0.753973   \n",
      "50%        0.720750   0.681319   0.829500   0.750691       0.784171   \n",
      "75%        0.728500   0.684847   0.854000   0.759964       0.807988   \n",
      "max        0.736000   0.690894   0.890000   0.765508       0.833585   \n",
      "ci_lower   0.701595   0.669459   0.784440   0.724380       0.745346   \n",
      "ci_upper   0.729605   0.686352   0.858360   0.759959       0.808556   \n",
      "\n",
      "          recall_neg     f1_neg  \n",
      "count       10.00000  10.000000  \n",
      "mean         0.60980   0.682017  \n",
      "std          0.02732   0.016309  \n",
      "min          0.55100   0.648531  \n",
      "25%          0.60000   0.674692  \n",
      "50%          0.61300   0.686672  \n",
      "75%          0.61500   0.692368  \n",
      "max          0.65900   0.699659  \n",
      "ci_lower     0.59055   0.670526  \n",
      "ci_upper     0.62905   0.693509  \n"
     ]
    }
   ],
   "source": [
    "train_index = [val_split for _, val_split in skf_train.split(X_train,y_train)] \n",
    "val_index = [val_split for _, val_split in skf_val.split(X_val,y_val)] \n",
    "\n",
    "test_result, test_result_xgb, test_result_svm = run_test(X_train, y_train, X_val, y_val,\n",
    "                       train_index, val_index, class_weight = {0: 1., 1: 1.}, epochs=1)\n",
    "print(test_result)\n",
    "print(test_result_xgb)\n",
    "print(test_result_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59338c28",
   "metadata": {},
   "source": [
    "### Balanced Random Split - Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2547e994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val [532, 468]\n",
      "Val [1000, 1000]\n",
      "Train [240, 760]\n",
      "Train [481, 1519]\n",
      "Train [749, 2251]\n",
      "Train [1028, 2972]\n",
      "Train [1268, 3732]\n",
      "Train [1522, 4478]\n",
      "Train [2000, 5000]\n",
      "Train [3000, 5000]\n",
      "Train [4000, 5000]\n",
      "Train [5000, 5000]\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "train_count = 0  \n",
    "val_count = 0  \n",
    "train_label_counter = [0, 0]\n",
    "val_label_counter = [0, 0]\n",
    "\n",
    "train_label_count_max = 5e3 \n",
    "val_label_count_max = 1e3\n",
    "total_train = 2 * train_label_count_max\n",
    "total_val = 2 * val_label_count_max\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for line in fs.open('s3://compressed-data-sample/processed_train.json'):\n",
    "    if train_count >= total_train and val_count >= total_val:\n",
    "        break\n",
    "    json_file = json.loads(line)\n",
    "    country = json_file['country']\n",
    "    label =  int(json_file['country'] in PEACE_COUNTRY)\n",
    "    \n",
    "    if not country in MAJOR_COUNTRY:\n",
    "        if train_label_counter[label] < train_label_count_max :\n",
    "            sent = json_file['content_cleaned']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_train.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_train.append(json_file['country'])\n",
    "            train_count += 1\n",
    "            train_label_counter[label] += 1\n",
    "            if sum(train_label_counter) % 1e3 == 0:\n",
    "                print('Train', train_label_counter)\n",
    "    else:\n",
    "        if val_label_counter[label] < val_label_count_max :\n",
    "            sent = json_file['content_cleaned']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_val.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_val.append(json_file['country'])\n",
    "            val_count += 1\n",
    "            val_label_counter[label] += 1\n",
    "            if sum(val_label_counter) % 1e3 == 0:\n",
    "                print('Val', val_label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "628be312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "from collections import Counter\n",
    "median = statistics.median(list(Counter(y_train).values()))\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7c2f39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare K-Fold for minority country\n",
    "import random\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "australia_idx = np.where(np.array(y_val) == 'Australia')[0]\n",
    "india_idx = np.where(np.array(y_val) == 'India')[0]\n",
    "majority_index = []\n",
    "for i in range(10):\n",
    "    australia_sample = np.array(random.sample(list(australia_idx), median))\n",
    "    india_sample = np.array(random.sample(list(india_idx), median))\n",
    "    majority_index.append(np.hstack([australia_sample, india_sample])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "79b2623d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 216s 817ms/step - loss: 0.2439 - accuracy: 0.8986 - precision: 0.8915 - recall: 0.9075 - TP: 3836.0000 - TN: 3765.0000 - FP: 467.0000 - FN: 391.0000\n",
      "67/67 [==============================] - 17s 257ms/step - loss: 0.1426 - accuracy: 0.9456 - precision: 0.9826 - recall: 0.9075 - TP: 962.0000 - TN: 1038.0000 - FP: 17.0000 - FN: 98.0000\n",
      "[0.14255183935165405, 0.9456264972686768, 0.9826353192329407, 0.9075471758842468, 962.0, 1038.0, 17.0, 98.0, 0.9435997965251304, 0.9137323943661971, 0.9838862559241706, 0.947512551346417]\n",
      "[05:02:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9555555555555556, 0.9530956848030019, 0.9584905660377359, 0.9557855126999059, 0.9580552907530981, 0.95260663507109, 0.9553231939163498]\n",
      "[0.9612293144208038, 0.9561567164179104, 0.9669811320754716, 0.9615384615384615, 0.9664429530201343, 0.9554502369668246, 0.9609151572926597]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 824ms/step - loss: 0.2268 - accuracy: 0.9041 - precision: 0.9077 - recall: 0.8979 - TP: 3765.0000 - TN: 3883.0000 - FP: 383.0000 - FN: 428.0000\n",
      "67/67 [==============================] - 17s 258ms/step - loss: 0.1204 - accuracy: 0.9527 - precision: 0.9414 - recall: 0.9689 - TP: 1060.0000 - TN: 955.0000 - FP: 66.0000 - FN: 34.0000\n",
      "[0.12041790783405304, 0.9527186751365662, 0.9413854479789734, 0.9689213633537292, 1060.0, 955.0, 66.0, 34.0, 0.9549549488972534, 0.9656218402426694, 0.9353574926542605, 0.9502487562189055]\n",
      "[05:08:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9508274231678487, 0.9541284403669725, 0.9506398537477148, 0.9523809523809523, 0.9473170731707317, 0.951028403525955, 0.949169110459433]\n",
      "[0.9602836879432625, 0.9658671586715867, 0.9570383912248629, 0.9614325068870524, 0.9544131910766246, 0.9637610186092067, 0.9590643274853801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 823ms/step - loss: 0.2188 - accuracy: 0.9102 - precision: 0.9021 - recall: 0.9200 - TP: 3888.0000 - TN: 3811.0000 - FP: 422.0000 - FN: 338.0000\n",
      "67/67 [==============================] - 17s 252ms/step - loss: 0.1563 - accuracy: 0.9470 - precision: 0.9233 - recall: 0.9755 - TP: 1035.0000 - TN: 968.0000 - FP: 86.0000 - FN: 26.0000\n",
      "[0.15631914138793945, 0.9470449090003967, 0.92328280210495, 0.9754948019981384, 1035.0, 968.0, 86.0, 26.0, 0.9486709473309715, 0.9738430583501007, 0.9184060721062619, 0.9453125]\n",
      "[05:14:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.956501182033097, 0.9498607242339833, 0.9641847313854854, 0.9569691300280637, 0.9633911368015414, 0.9487666034155597, 0.9560229445506693]\n",
      "[0.9602836879432625, 0.9535747446610956, 0.9679547596606974, 0.9607109448082319, 0.9672447013487476, 0.952561669829222, 0.9598470363288719]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 823ms/step - loss: 0.2228 - accuracy: 0.9076 - precision: 0.8910 - recall: 0.9296 - TP: 3947.0000 - TN: 3730.0000 - FP: 483.0000 - FN: 299.0000\n",
      "67/67 [==============================] - 17s 256ms/step - loss: 0.1158 - accuracy: 0.9565 - precision: 0.9481 - recall: 0.9645 - TP: 1004.0000 - TN: 1019.0000 - FP: 55.0000 - FN: 37.0000\n",
      "[0.11582086980342865, 0.9565011858940125, 0.9480642080307007, 0.9644572734832764, 1004.0, 1019.0, 55.0, 37.0, 0.9561904846585525, 0.9649621212121212, 0.9487895716945997, 0.9568075117370892]\n",
      "[05:20:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.956501182033097, 0.9566891241578441, 0.9548511047070125, 0.9557692307692308, 0.9563197026022305, 0.9581005586592178, 0.9572093023255814]\n",
      "[0.9640661938534278, 0.9599618684461392, 0.9673390970220941, 0.9636363636363637, 0.9681050656660413, 0.9608938547486033, 0.9644859813084112]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 824ms/step - loss: 0.2096 - accuracy: 0.9143 - precision: 0.9142 - recall: 0.9144 - TP: 3866.0000 - TN: 3868.0000 - FP: 363.0000 - FN: 362.0000\n",
      "67/67 [==============================] - 17s 256ms/step - loss: 0.1410 - accuracy: 0.9461 - precision: 0.9574 - recall: 0.9339 - TP: 989.0000 - TN: 1012.0000 - FP: 44.0000 - FN: 70.0000\n",
      "[0.14098863303661346, 0.9460992813110352, 0.9574056267738342, 0.9338998794555664, 989.0, 1012.0, 44.0, 70.0, 0.9455066846569153, 0.9353049907578558, 0.9583333333333334, 0.9466791393826006]\n",
      "[05:26:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9574468085106383, 0.9540768509840675, 0.9612842304060434, 0.9576669802445906, 0.9608778625954199, 0.9535984848484849, 0.9572243346007604]\n",
      "[0.9560283687943263, 0.9539473684210527, 0.9584513692162417, 0.9561940650023552, 0.9581351094196003, 0.9535984848484849, 0.955861414333175]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 823ms/step - loss: 0.2221 - accuracy: 0.9104 - precision: 0.9022 - recall: 0.9201 - TP: 3883.0000 - TN: 3818.0000 - FP: 421.0000 - FN: 337.0000\n",
      "67/67 [==============================] - 18s 264ms/step - loss: 0.1541 - accuracy: 0.9442 - precision: 0.9151 - recall: 0.9803 - TP: 1046.0000 - TN: 951.0000 - FP: 97.0000 - FN: 21.0000\n",
      "[0.15412886440753937, 0.944208025932312, 0.9151356220245361, 0.9803186655044556, 1046.0, 951.0, 97.0, 21.0, 0.946606349349864, 0.9783950617283951, 0.9074427480916031, 0.9415841584158415]\n",
      "[05:32:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9569739952718677, 0.9501845018450185, 0.9653233364573571, 0.9576940957694096, 0.9641125121241513, 0.9484732824427481, 0.9562289562289562]\n",
      "[0.9588652482269504, 0.9553903345724907, 0.9634489222118088, 0.9594027064862343, 0.9624639076034649, 0.9541984732824428, 0.9583133684714902]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 824ms/step - loss: 0.2295 - accuracy: 0.9054 - precision: 0.8884 - recall: 0.9288 - TP: 3955.0000 - TN: 3704.0000 - FP: 497.0000 - FN: 303.0000\n",
      "67/67 [==============================] - 18s 262ms/step - loss: 0.1498 - accuracy: 0.9499 - precision: 0.9620 - recall: 0.9339 - TP: 961.0000 - TN: 1048.0000 - FP: 38.0000 - FN: 68.0000\n",
      "[0.1498207449913025, 0.9498817920684814, 0.9619619846343994, 0.933916449546814, 961.0, 1048.0, 38.0, 68.0, 0.9477317797296008, 0.9390681003584229, 0.9650092081031307, 0.9518619436875567]\n",
      "[05:38:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9583924349881797, 0.9528392685274302, 0.9620991253644315, 0.9574468085106383, 0.9637546468401487, 0.9548802946593001, 0.9592969472710453]\n",
      "[0.9607565011820332, 0.9548076923076924, 0.9650145772594753, 0.9598840019333013, 0.9665116279069768, 0.9567219152854513, 0.961591855622397]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 823ms/step - loss: 0.2083 - accuracy: 0.9154 - precision: 0.9170 - recall: 0.9136 - TP: 3869.0000 - TN: 3874.0000 - FP: 350.0000 - FN: 366.0000\n",
      "67/67 [==============================] - 18s 262ms/step - loss: 0.1137 - accuracy: 0.9570 - precision: 0.9537 - recall: 0.9601 - TP: 1010.0000 - TN: 1014.0000 - FP: 49.0000 - FN: 42.0000\n",
      "[0.11374502629041672, 0.9569739699363708, 0.9537299275398254, 0.9600760340690613, 1010.0, 1014.0, 49.0, 42.0, 0.9568924590825739, 0.9602272727272727, 0.9539040451552211, 0.9570552147239264]\n",
      "[05:44:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9583924349881797, 0.9564393939393939, 0.9600760456273765, 0.9582542694497154, 0.9603399433427762, 0.9567262464722484, 0.9585296889726672]\n",
      "[0.9640661938534278, 0.9612476370510397, 0.9667300380228137, 0.9639810426540284, 0.9668874172185431, 0.9614299153339605, 0.9641509433962264]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 823ms/step - loss: 0.2405 - accuracy: 0.8979 - precision: 0.8808 - recall: 0.9196 - TP: 3879.0000 - TN: 3716.0000 - FP: 525.0000 - FN: 339.0000\n",
      "67/67 [==============================] - 17s 254ms/step - loss: 0.1443 - accuracy: 0.9409 - precision: 0.9777 - recall: 0.9036 - TP: 966.0000 - TN: 1024.0000 - FP: 22.0000 - FN: 103.0000\n",
      "[0.1442503035068512, 0.9408983588218689, 0.97773277759552, 0.9036482572555542, 966.0, 1024.0, 22.0, 103.0, 0.9392318771893668, 0.9086069210292813, 0.9789674952198852, 0.9424758398527382]\n",
      "[05:50:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.956501182033097, 0.9710703953712633, 0.9420018709073901, 0.9563152896486229, 0.9424860853432282, 0.97131931166348, 0.9566854990583804]\n",
      "[0.9593380614657211, 0.9667616334283001, 0.9522918615528532, 0.9594721960414704, 0.9519774011299436, 0.9665391969407265, 0.9592030360531308]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 822ms/step - loss: 0.2191 - accuracy: 0.9099 - precision: 0.9154 - recall: 0.9033 - TP: 3821.0000 - TN: 3876.0000 - FP: 353.0000 - FN: 409.0000\n",
      "67/67 [==============================] - 17s 253ms/step - loss: 0.1091 - accuracy: 0.9551 - precision: 0.9555 - recall: 0.9546 - TP: 1009.0000 - TN: 1011.0000 - FP: 47.0000 - FN: 48.0000\n",
      "[0.10908699780702591, 0.9550827145576477, 0.9554924368858337, 0.9545884728431702, 1009.0, 1011.0, 47.0, 48.0, 0.9550402409596913, 0.9546742209631728, 0.9555765595463138, 0.9551251771374586]\n",
      "[05:56:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9621749408983451, 0.9692603266090298, 0.9545884578997161, 0.9618684461391802, 0.9553072625698324, 0.9697542533081286, 0.9624765478424014]\n",
      "[0.9631205673758865, 0.9622285174693107, 0.9640491958372753, 0.9631379962192818, 0.9640151515151515, 0.9621928166351607, 0.9631031220435194]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(               loss   accuracy  precision     recall           TP  \\\n",
       " count     10.000000  10.000000  10.000000  10.000000    10.000000   \n",
       " mean       0.134713   0.949504   0.951683   0.948287  1004.200000   \n",
       " std        0.018017   0.005589   0.021227   0.027281    35.282353   \n",
       " min        0.109087   0.940898   0.915136   0.903648   961.000000   \n",
       " 25%        0.116970   0.945745   0.943055   0.933904   971.750000   \n",
       " 50%        0.141770   0.948463   0.954611   0.957332  1006.500000   \n",
       " 75%        0.148428   0.954492   0.960823   0.967805  1028.750000   \n",
       " max        0.156319   0.956974   0.982635   0.980319  1060.000000   \n",
       " ci_lower   0.122018   0.945565   0.936726   0.929065   979.340076   \n",
       " ci_upper   0.147408   0.953442   0.966639   0.967509  1029.059924   \n",
       " \n",
       "                    TN         FP          FN         f1  precision_neg  \\\n",
       " count       10.000000  10.000000   10.000000  10.000000      10.000000   \n",
       " mean      1004.000000  52.100000   54.700000   0.949443       0.949444   \n",
       " std         34.052247  25.396631   28.917699   0.006039       0.024336   \n",
       " min        951.000000  17.000000   21.000000   0.939232       0.908607   \n",
       " 25%        978.750000  39.500000   34.750000   0.945782       0.936246   \n",
       " 50%       1013.000000  48.000000   45.000000   0.948201       0.957451   \n",
       " 75%       1022.750000  63.250000   69.500000   0.955019       0.965457   \n",
       " max       1048.000000  97.000000  103.000000   0.956892       0.978395   \n",
       " ci_lower   980.006808  34.205549   34.324607   0.945188       0.932296   \n",
       " ci_upper  1027.993192  69.994451   75.075393   0.953698       0.966591   \n",
       " \n",
       "           recall_neg     f1_neg  \n",
       " count      10.000000  10.000000  \n",
       " mean        0.950567   0.949466  \n",
       " std         0.024382   0.005673  \n",
       " min         0.907443   0.941584  \n",
       " 25%         0.938716   0.945654  \n",
       " 50%         0.954740   0.948881  \n",
       " 75%         0.963340   0.954309  \n",
       " max         0.983886   0.957055  \n",
       " ci_lower    0.933388   0.945469  \n",
       " ci_upper    0.967747   0.953464  ,\n",
       "            accuracy  precision     recall         f1  precision_neg  \\\n",
       " count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
       " mean       0.956927   0.956764   0.957354   0.957015       0.957196   \n",
       " std        0.002823   0.007416   0.007083   0.002385       0.007240   \n",
       " min        0.950827   0.949861   0.942002   0.952381       0.942486   \n",
       " 25%        0.956501   0.952903   0.954654   0.955918       0.955560   \n",
       " 50%        0.956738   0.954103   0.959283   0.957208       0.959198   \n",
       " 75%        0.958156   0.956627   0.961895   0.957687       0.962763   \n",
       " max        0.962175   0.971070   0.965323   0.961868       0.964113   \n",
       " ci_lower   0.954937   0.951539   0.952363   0.955335       0.952095   \n",
       " ci_upper   0.958916   0.961989   0.962345   0.958696       0.962297   \n",
       " \n",
       "           recall_neg     f1_neg  \n",
       " count      10.000000  10.000000  \n",
       " mean        0.956525   0.956817  \n",
       " std         0.008016   0.003385  \n",
       " min         0.948473   0.949169  \n",
       " 25%         0.951423   0.956074  \n",
       " 50%         0.954239   0.956947  \n",
       " 75%         0.957757   0.958203  \n",
       " max         0.971319   0.962477  \n",
       " ci_lower    0.950878   0.954431  \n",
       " ci_upper    0.962173   0.959202  ,\n",
       "            accuracy  precision     recall         f1  precision_neg  \\\n",
       " count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
       " mean       0.960804   0.958994   0.962930   0.960939       0.962620   \n",
       " std        0.002496   0.004915   0.005264   0.002361       0.005794   \n",
       " min        0.956028   0.953575   0.952292   0.956194       0.951977   \n",
       " 25%        0.959574   0.954953   0.959701   0.959575       0.959217   \n",
       " 50%        0.960520   0.958059   0.964532   0.961072       0.965229   \n",
       " 75%        0.962648   0.961983   0.966918   0.962738       0.966793   \n",
       " max        0.964066   0.966762   0.967955   0.963981       0.968105   \n",
       " ci_lower   0.959045   0.955531   0.959221   0.959276       0.958537   \n",
       " ci_upper   0.962563   0.962458   0.966639   0.962602       0.966702   \n",
       " \n",
       "           recall_neg     f1_neg  \n",
       " count      10.000000  10.000000  \n",
       " mean        0.958735   0.960654  \n",
       " std         0.004830   0.002740  \n",
       " min         0.952562   0.955861  \n",
       " 25%         0.954511   0.959099  \n",
       " 50%         0.958808   0.960381  \n",
       " 75%         0.962002   0.962725  \n",
       " max         0.966539   0.964486  \n",
       " ci_lower    0.955332   0.958723  \n",
       " ci_upper    0.962138   0.962585  )"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = []\n",
    "eval_results_xgb = []\n",
    "eval_results_svm = []\n",
    "metrics_names = None\n",
    "    \n",
    "for maj_index in majority_index:\n",
    "    tf.keras.backend.clear_session()\n",
    "    X_all_sample = np.hstack([X_train, X_val[maj_index]])\n",
    "    y_all_sample = np.hstack([y_train, y_val[maj_index]])\n",
    "    y_all_sample = np.array(list(map(lambda x: int(x in PEACE_COUNTRY), y_all_sample)))\n",
    "    \n",
    "    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(X_all_sample, y_all_sample, test_size=0.2)\n",
    "    \n",
    "    model = get_model()\n",
    "        \n",
    "    train_input1 = np.vstack([x['input_ids'] for x in X_train_fold])\n",
    "    train_input2 = np.vstack([x['attention_mask'] for x in X_train_fold])\n",
    "    model.fit(x=[train_input1, train_input2], \n",
    "              y=np.asarray(y_train_fold),\n",
    "              epochs = 1, \n",
    "              batch_size = 32,\n",
    "              class_weight={0: 1., 1: 1.})\n",
    "\n",
    "    eval_input1 = np.vstack([x['input_ids'] for x in X_val_fold])\n",
    "    eval_input2 = np.vstack([x['attention_mask'] for x in X_val_fold])\n",
    "    er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                        y=np.asarray(y_val_fold), return_dict=True)\n",
    "    f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "\n",
    "    precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "    recall_neg = er['TN'] / (er['TN']+er['FP'])\n",
    "    f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "\n",
    "    er = list(er.values())\n",
    "    er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "    eval_results.append(er)\n",
    "    metrics_names = model.metrics_names\n",
    "    print(er)\n",
    "\n",
    "    # Get Embeddings \n",
    "    input_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='input_ids', dtype='int32')\n",
    "    input_masks_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='attention_mask', dtype='int32')\n",
    "    output_layer = model.layers[2].layers[0](input_ids_in, input_masks_ids_in)[0]\n",
    "    embedding_model = tf.keras.Model(inputs=[input_ids_in, input_masks_ids_in], outputs = output_layer)\n",
    "\n",
    "    train_embedding = embedding_model.predict(x=[train_input1, train_input2])[:, 0]\n",
    "    val_embedding = embedding_model.predict(x=[eval_input1, eval_input2])[:, 0]\n",
    "\n",
    "    # Get XGB results\n",
    "    dtrain = xgb.DMatrix(train_embedding, label=y_train_fold)\n",
    "    dval = xgb.DMatrix(val_embedding, label=y_val_fold)\n",
    "    param = {'max_depth': 2, 'eta': 1,'objective': 'binary:logistic'}\n",
    "    num_round = 10\n",
    "    evallist = [(dval, 'eval'), (dtrain, 'train')]\n",
    "    bst = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)\n",
    "    y_pred = (bst.predict(dval) > 0.5).astype(int)\n",
    "    xgb_acc = np.mean(y_val_fold == y_pred)\n",
    "    xgb_score = precision_recall_fscore_support(y_val_fold, y_pred)\n",
    "    # Accuracy, Precision+, Recall+, F1+, Precision-, Recall-, F1-\n",
    "    er = [xgb_acc, xgb_score[0][1], xgb_score[1][1], xgb_score[2][1], \n",
    "                             xgb_score[0][0], xgb_score[1][0], xgb_score[2][0]]\n",
    "    print(er)\n",
    "    eval_results_xgb.append(er)\n",
    "\n",
    "\n",
    "    # Get SVM results\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('pca', PCA()),\n",
    "                     ('svc', SVC())])\n",
    "    pipe.fit(train_embedding, y_train_fold)\n",
    "    y_pred = (pipe.predict(val_embedding) > 0.5).astype(int)\n",
    "    svm_acc = np.mean(y_val_fold == y_pred)\n",
    "    svm_score = precision_recall_fscore_support(y_val_fold, y_pred)\n",
    "    # Accuracy, Precision+, Recall+, F1+, Precision-, Recall-, F1-\n",
    "    er = [svm_acc, svm_score[0][1], svm_score[1][1], svm_score[2][1], \n",
    "                             svm_score[0][0], svm_score[1][0], svm_score[2][0]]\n",
    "    print(er)\n",
    "    eval_results_svm.append(er)\n",
    "    \n",
    "eval_results = np.array(eval_results)\n",
    "metrics_names += ['f1', 'precision_neg', 'recall_neg', 'f1_neg']\n",
    "eval_results = pd.DataFrame(eval_results, columns=metrics_names)\n",
    "ci = eval_results.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "ci.index = ['ci_lower', 'ci_upper']\n",
    "eval_results = pd.concat([eval_results.describe(), ci])\n",
    "\n",
    "eval_results_xgb = np.array(eval_results_xgb)\n",
    "eval_results_xgb = pd.DataFrame(eval_results_xgb, columns=['accuracy', 'precision', 'recall', 'f1', 'precision_neg', 'recall_neg', 'f1_neg'])\n",
    "ci = eval_results_xgb.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "ci.index = ['ci_lower', 'ci_upper']\n",
    "eval_results_xgb = pd.concat([eval_results_xgb.describe(), ci])\n",
    "\n",
    "eval_results_svm = np.array(eval_results_svm)\n",
    "eval_results_svm = pd.DataFrame(eval_results_svm, columns=['accuracy', 'precision', 'recall', 'f1', 'precision_neg', 'recall_neg', 'f1_neg'])\n",
    "ci = eval_results_svm.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "ci.index = ['ci_lower', 'ci_upper']\n",
    "eval_results_svm = pd.concat([eval_results_svm.describe(), ci])\n",
    "\n",
    "eval_results, eval_results_xgb, eval_results_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494d91c",
   "metadata": {},
   "source": [
    "## Shuffled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a637d350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val [532, 468]\n",
      "Val [1065, 935]\n",
      "Val [1607, 1393]\n",
      "Val [2144, 1856]\n",
      "Val [2690, 2310]\n",
      "Train [240, 760]\n",
      "Val [3255, 2745]\n",
      "Val [3805, 3195]\n",
      "Val [4348, 3652]\n",
      "Val [4906, 4094]\n",
      "Val [5457, 4543]\n",
      "Train [481, 1519]\n",
      "Val [5984, 5016]\n",
      "Val [6519, 5481]\n",
      "Val [7059, 5941]\n",
      "Val [7638, 6362]\n",
      "Val [8191, 6809]\n",
      "Train [749, 2251]\n",
      "Val [8747, 7253]\n",
      "Val [9315, 7685]\n",
      "Val [9837, 8163]\n",
      "Val [10000, 9000]\n",
      "Train [1028, 2972]\n",
      "Val [10000, 10000]\n",
      "Train [1268, 3732]\n",
      "Train [1522, 4478]\n",
      "Train [1770, 5230]\n",
      "Train [2027, 5973]\n",
      "Train [2301, 6699]\n",
      "Train [2562, 7438]\n",
      "Train [2796, 8204]\n",
      "Train [3071, 8929]\n",
      "Train [3327, 9673]\n",
      "Train [3601, 10399]\n",
      "Train [3863, 11137]\n",
      "Train [4128, 11872]\n",
      "Train [4384, 12616]\n",
      "Train [4619, 13381]\n",
      "Train [4861, 14139]\n",
      "Train [5101, 14899]\n",
      "Train [5352, 15648]\n",
      "Train [5588, 16412]\n",
      "Train [5843, 17157]\n",
      "Train [6113, 17887]\n",
      "Train [6353, 18647]\n",
      "Train [6634, 19366]\n",
      "Train [6926, 20074]\n",
      "Train [7193, 20807]\n",
      "Train [7450, 21550]\n",
      "Train [7714, 22286]\n",
      "Train [7969, 23031]\n",
      "Train [8237, 23763]\n",
      "Train [8478, 24522]\n",
      "Train [8739, 25261]\n",
      "Train [8998, 26002]\n",
      "Train [9246, 26754]\n",
      "Train [9510, 27490]\n",
      "Train [9770, 28230]\n",
      "Train [10036, 28964]\n",
      "Train [10282, 29718]\n",
      "Train [10535, 30465]\n",
      "Train [10780, 31220]\n",
      "Train [11045, 31955]\n",
      "Train [11295, 32705]\n",
      "Train [11532, 33468]\n",
      "Train [11790, 34210]\n",
      "Train [12040, 34960]\n",
      "Train [12289, 35711]\n",
      "Train [12528, 36472]\n",
      "Train [12795, 37205]\n",
      "Train [13051, 37949]\n",
      "Train [13300, 38700]\n",
      "Train [13554, 39446]\n",
      "Train [14000, 40000]\n",
      "Train [15000, 40000]\n",
      "Train [16000, 40000]\n",
      "Train [17000, 40000]\n",
      "Train [18000, 40000]\n",
      "Train [19000, 40000]\n",
      "Train [20000, 40000]\n",
      "Train [21000, 40000]\n",
      "Train [22000, 40000]\n",
      "Train [23000, 40000]\n",
      "Train [24000, 40000]\n",
      "Train [25000, 40000]\n",
      "Train [26000, 40000]\n",
      "Train [27000, 40000]\n",
      "Train [28000, 40000]\n",
      "Train [29000, 40000]\n",
      "Train [30000, 40000]\n",
      "Train [31000, 40000]\n",
      "Train [32000, 40000]\n",
      "Train [33000, 40000]\n",
      "Train [34000, 40000]\n",
      "Train [35000, 40000]\n",
      "Train [36000, 40000]\n",
      "Train [37000, 40000]\n",
      "Train [38000, 40000]\n",
      "Train [39000, 40000]\n",
      "Train [40000, 40000]\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "train_count = 0  \n",
    "val_count = 0  \n",
    "train_label_counter = [0, 0]\n",
    "val_label_counter = [0, 0]\n",
    "\n",
    "train_label_count_max = 4e4\n",
    "val_label_count_max = 1e4\n",
    "total_train = 2 * train_label_count_max\n",
    "total_val = 2 * val_label_count_max\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for line in fs.open('s3://compressed-data-sample/shuffled_train.json'):\n",
    "    if train_count >= total_train and val_count >= total_val:\n",
    "        break\n",
    "    json_file = json.loads(line)\n",
    "    country = json_file['country']\n",
    "    label =  int(json_file['country'] in PEACE_COUNTRY)\n",
    "    \n",
    "    if not country in MAJOR_COUNTRY:\n",
    "        if train_label_counter[label] < train_label_count_max :\n",
    "            sent = json_file['content_cleaned_shuffled']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_train.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_train.append(label)\n",
    "            train_count += 1\n",
    "            train_label_counter[label] += 1\n",
    "            if sum(train_label_counter) % 1e3 == 0:\n",
    "                print('Train', train_label_counter)\n",
    "    else:\n",
    "        if val_label_counter[label] < val_label_count_max :\n",
    "            sent = json_file['content_cleaned_shuffled']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_val.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_val.append(label)\n",
    "            val_count += 1\n",
    "            val_label_counter[label] += 1\n",
    "            if sum(val_label_counter) % 1e3 == 0:\n",
    "                print('Val', val_label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "387e425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_all = np.hstack([X_train, X_val])\n",
    "y_all = np.hstack([y_train, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37da668",
   "metadata": {},
   "source": [
    "### Random Split - Shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0bcbe80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 198s 793ms/step - loss: 0.3291 - accuracy: 0.8531 - precision: 0.8278 - recall: 0.8917 - TP: 3567.0000 - TN: 3258.0000 - FP: 742.0000 - FN: 433.0000\n",
      "63/63 [==============================] - 16s 259ms/step - loss: 0.2040 - accuracy: 0.9165 - precision: 0.9016 - recall: 0.9350 - TP: 935.0000 - TN: 898.0000 - FP: 102.0000 - FN: 65.0000\n",
      "[0.20400559902191162, 0.9164999723434448, 0.9016393423080444, 0.9350000023841858, 935.0, 898.0, 102.0, 65.0, 0.9180166913488028, 0.9325025960539979, 0.898, 0.9149261334691798]\n",
      "[23:44:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9095, 0.9107321965897693, 0.908, 0.9093640460691037, 0.9082751744765702, 0.911, 0.90963554667998]\n",
      "[0.925, 0.9267068273092369, 0.923, 0.9248496993987976, 0.9233067729083665, 0.927, 0.9251497005988024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 185s 740ms/step - loss: 0.3532 - accuracy: 0.8405 - precision: 0.8061 - recall: 0.8967 - TP: 3587.0000 - TN: 3137.0000 - FP: 863.0000 - FN: 413.0000\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 0.2893 - accuracy: 0.8910 - precision: 0.9278 - recall: 0.8480 - TP: 848.0000 - TN: 934.0000 - FP: 66.0000 - FN: 152.0000\n",
      "[0.28933852910995483, 0.890999972820282, 0.9277899265289307, 0.8479999899864197, 848.0, 934.0, 66.0, 152.0, 0.8861023943078947, 0.860036832412523, 0.934, 0.8954937679769894]\n",
      "[23:49:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.906, 0.9084507042253521, 0.903, 0.9057171514543632, 0.9035785288270378, 0.909, 0.9062811565304087]\n",
      "[0.912, 0.9071146245059288, 0.918, 0.9125248508946322, 0.917004048582996, 0.906, 0.9114688128772637]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 825ms/step - loss: 0.3604 - accuracy: 0.8363 - precision: 0.8088 - recall: 0.8808 - TP: 3523.0000 - TN: 3167.0000 - FP: 833.0000 - FN: 477.0000\n",
      "63/63 [==============================] - 16s 257ms/step - loss: 0.2506 - accuracy: 0.9020 - precision: 0.8675 - recall: 0.9490 - TP: 949.0000 - TN: 855.0000 - FP: 145.0000 - FN: 51.0000\n",
      "[0.2506489157676697, 0.9020000100135803, 0.8674588799476624, 0.9490000009536743, 949.0, 855.0, 145.0, 51.0, 0.9063992436637149, 0.9437086092715232, 0.855, 0.8971668415529905]\n",
      "[23:55:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.895, 0.8887795275590551, 0.903, 0.8958333333333334, 0.9014227642276422, 0.887, 0.8941532258064515]\n",
      "[0.8965, 0.8937437934458788, 0.9, 0.8968609865470851, 0.8992950654582075, 0.893, 0.8961364776718515]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 826ms/step - loss: 0.3470 - accuracy: 0.8494 - precision: 0.8161 - recall: 0.9020 - TP: 3608.0000 - TN: 3187.0000 - FP: 813.0000 - FN: 392.0000\n",
      "63/63 [==============================] - 16s 255ms/step - loss: 0.2589 - accuracy: 0.8970 - precision: 0.9170 - recall: 0.8730 - TP: 873.0000 - TN: 921.0000 - FP: 79.0000 - FN: 127.0000\n",
      "[0.2589396834373474, 0.8970000147819519, 0.9170168042182922, 0.8730000257492065, 873.0, 921.0, 79.0, 127.0, 0.8944672254389473, 0.8788167938931297, 0.921, 0.8994140625]\n",
      "[00:00:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9095, 0.9066534260178749, 0.913, 0.9098156452416543, 0.9123867069486404, 0.906, 0.9091821374811841]\n",
      "[0.911, 0.9005847953216374, 0.924, 0.912142152023692, 0.9219712525667351, 0.898, 0.9098277608915907]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 188s 752ms/step - loss: 0.3486 - accuracy: 0.8424 - precision: 0.8164 - recall: 0.8835 - TP: 3534.0000 - TN: 3205.0000 - FP: 795.0000 - FN: 466.0000\n",
      "63/63 [==============================] - 16s 252ms/step - loss: 0.2252 - accuracy: 0.9100 - precision: 0.8818 - recall: 0.9470 - TP: 947.0000 - TN: 873.0000 - FP: 127.0000 - FN: 53.0000\n",
      "[0.2252473682165146, 0.9100000262260437, 0.8817504644393921, 0.9470000267028809, 947.0, 873.0, 127.0, 53.0, 0.9132111979341848, 0.9427645788336934, 0.873, 0.9065420560747663]\n",
      "[00:06:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9135, 0.913086913086913, 0.914, 0.9135432283858069, 0.913913913913914, 0.913, 0.913456728364182]\n",
      "[0.914, 0.9207317073170732, 0.906, 0.9133064516129032, 0.90748031496063, 0.922, 0.9146825396825398]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 825ms/step - loss: 0.3400 - accuracy: 0.8499 - precision: 0.8344 - recall: 0.8730 - TP: 3492.0000 - TN: 3307.0000 - FP: 693.0000 - FN: 508.0000\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.2220 - accuracy: 0.9140 - precision: 0.9140 - recall: 0.9140 - TP: 914.0000 - TN: 914.0000 - FP: 86.0000 - FN: 86.0000\n",
      "[0.22195102274417877, 0.9139999747276306, 0.9139999747276306, 0.9139999747276306, 914.0, 914.0, 86.0, 86.0, 0.9139999747276306, 0.914, 0.914, 0.914]\n",
      "[00:11:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9045, 0.9016881827209533, 0.908, 0.9048330842052815, 0.9073514602215509, 0.901, 0.9041645760160562]\n",
      "[0.9145, 0.9116186693147964, 0.918, 0.9147982062780269, 0.9174219536757301, 0.911, 0.9141996989463121]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 205s 822ms/step - loss: 0.3508 - accuracy: 0.8470 - precision: 0.8260 - recall: 0.8792 - TP: 3517.0000 - TN: 3259.0000 - FP: 741.0000 - FN: 483.0000\n",
      "63/63 [==============================] - 17s 262ms/step - loss: 0.2819 - accuracy: 0.8940 - precision: 0.9147 - recall: 0.8690 - TP: 869.0000 - TN: 919.0000 - FP: 81.0000 - FN: 131.0000\n",
      "[0.281893789768219, 0.8939999938011169, 0.9147368669509888, 0.8690000176429749, 869.0, 919.0, 81.0, 131.0, 0.8912820723556829, 0.8752380952380953, 0.919, 0.8965853658536587]\n",
      "[00:17:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.895, 0.8903162055335968, 0.901, 0.8956262425447316, 0.8997975708502024, 0.889, 0.8943661971830986]\n",
      "[0.905, 0.905, 0.905, 0.905, 0.905, 0.905, 0.905]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 824ms/step - loss: 0.3598 - accuracy: 0.8335 - precision: 0.8046 - recall: 0.8810 - TP: 3524.0000 - TN: 3144.0000 - FP: 856.0000 - FN: 476.0000\n",
      "63/63 [==============================] - 16s 259ms/step - loss: 0.2330 - accuracy: 0.8960 - precision: 0.8468 - recall: 0.9670 - TP: 967.0000 - TN: 825.0000 - FP: 175.0000 - FN: 33.0000\n",
      "[0.23296888172626495, 0.8960000276565552, 0.846760094165802, 0.9670000076293945, 967.0, 825.0, 175.0, 33.0, 0.9028945081636329, 0.9615384615384616, 0.825, 0.8880516684607105]\n",
      "[00:23:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9125, 0.9112662013958126, 0.914, 0.9126310534198703, 0.9137412236710131, 0.911, 0.9123685528292439]\n",
      "[0.915, 0.9076620825147348, 0.924, 0.9157581764122895, 0.9226069246435845, 0.906, 0.9142280524722503]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 825ms/step - loss: 0.3438 - accuracy: 0.8525 - precision: 0.8458 - recall: 0.8622 - TP: 3449.0000 - TN: 3371.0000 - FP: 629.0000 - FN: 551.0000\n",
      "63/63 [==============================] - 16s 260ms/step - loss: 0.2147 - accuracy: 0.9115 - precision: 0.8677 - recall: 0.9710 - TP: 971.0000 - TN: 852.0000 - FP: 148.0000 - FN: 29.0000\n",
      "[0.21465188264846802, 0.9114999771118164, 0.8677390813827515, 0.9710000157356262, 971.0, 852.0, 148.0, 29.0, 0.9164700560264692, 0.9670828603859251, 0.852, 0.9059011164274322]\n",
      "[00:28:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9215, 0.9120234604105572, 0.933, 0.9223924864063273, 0.9314227226202662, 0.91, 0.9205867475973697]\n",
      "[0.9275, 0.9138431752178122, 0.944, 0.9286768322675848, 0.9420889348500517, 0.911, 0.9262836807320795]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 826ms/step - loss: 0.3366 - accuracy: 0.8456 - precision: 0.8361 - recall: 0.8597 - TP: 3439.0000 - TN: 3326.0000 - FP: 674.0000 - FN: 561.0000\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.2851 - accuracy: 0.8835 - precision: 0.9295 - recall: 0.8300 - TP: 830.0000 - TN: 937.0000 - FP: 63.0000 - FN: 170.0000\n",
      "[0.28510403633117676, 0.8834999799728394, 0.9294512867927551, 0.8299999833106995, 830.0, 937.0, 63.0, 170.0, 0.8769149400548438, 0.8464317976513098, 0.937, 0.8894162316089226]\n",
      "[00:34:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.903, 0.899009900990099, 0.908, 0.9034825870646767, 0.907070707070707, 0.898, 0.9025125628140703]\n",
      "[0.9095, 0.9026548672566371, 0.918, 0.9102627664848786, 0.9165818921668362, 0.901, 0.908724155320222]\n",
      "               loss   accuracy  precision     recall          TP          TN  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000   10.000000   10.000000   \n",
      "mean       0.246475   0.901550   0.896834   0.910300  910.300000  892.800000   \n",
      "std        0.031283   0.011029   0.028891   0.051418   51.417788   39.003704   \n",
      "min        0.204006   0.883500   0.846760   0.830000  830.000000  825.000000   \n",
      "25%        0.222775   0.894500   0.871242   0.870000  870.000000  859.500000   \n",
      "50%        0.241809   0.899500   0.907820   0.924500  924.500000  906.000000   \n",
      "75%        0.276155   0.911125   0.916447   0.948500  948.500000  920.500000   \n",
      "max        0.289339   0.916500   0.929451   0.971000  971.000000  937.000000   \n",
      "ci_lower   0.224433   0.893779   0.876478   0.874071  874.071059  865.318015   \n",
      "ci_upper   0.268517   0.909321   0.917191   0.946529  946.528941  920.281985   \n",
      "\n",
      "                  FP          FN         f1  precision_neg  recall_neg  \\\n",
      "count      10.000000   10.000000  10.000000      10.000000   10.000000   \n",
      "mean      107.200000   89.700000   0.901976       0.912212    0.892800   \n",
      "std        39.003704   51.417788   0.014179       0.043863    0.039004   \n",
      "min        63.000000   29.000000   0.876915       0.846432    0.825000   \n",
      "25%        79.500000   51.500000   0.892078       0.876133    0.859500   \n",
      "50%        94.000000   75.500000   0.904647       0.923251    0.906000   \n",
      "75%       140.500000  130.000000   0.913803       0.943473    0.920500   \n",
      "max       175.000000  170.000000   0.918017       0.967083    0.937000   \n",
      "ci_lower   79.718015   53.471059   0.891985       0.881306    0.865318   \n",
      "ci_upper  134.681985  125.928941   0.911966       0.943118    0.920282   \n",
      "\n",
      "             f1_neg  \n",
      "count     10.000000  \n",
      "mean       0.900750  \n",
      "std        0.009345  \n",
      "min        0.888052  \n",
      "25%        0.895767  \n",
      "50%        0.898290  \n",
      "75%        0.906382  \n",
      "max        0.914926  \n",
      "ci_lower   0.894165  \n",
      "ci_upper   0.907334  \n",
      "           accuracy  precision     recall         f1  precision_neg  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
      "mean       0.907000   0.904201   0.910500   0.907324       0.909896   \n",
      "std        0.008209   0.008948   0.009180   0.008142       0.009014   \n",
      "min        0.895000   0.888780   0.901000   0.895626       0.899798   \n",
      "25%        0.903375   0.899679   0.904250   0.903820       0.904452   \n",
      "50%        0.907750   0.907552   0.908000   0.907541       0.907813   \n",
      "75%        0.911750   0.911133   0.913750   0.911927       0.913403   \n",
      "max        0.921500   0.913087   0.933000   0.922392       0.931423   \n",
      "ci_lower   0.901216   0.897896   0.904032   0.901587       0.903545   \n",
      "ci_upper   0.912784   0.910506   0.916968   0.913061       0.916247   \n",
      "\n",
      "          recall_neg     f1_neg  \n",
      "count      10.000000  10.000000  \n",
      "mean        0.903500   0.906671  \n",
      "std         0.009431   0.008294  \n",
      "min         0.887000   0.894153  \n",
      "25%         0.898750   0.902926  \n",
      "50%         0.907500   0.907732  \n",
      "75%         0.910750   0.911685  \n",
      "max         0.913000   0.920587  \n",
      "ci_lower    0.896855   0.900827  \n",
      "ci_upper    0.910145   0.912515  \n",
      "           accuracy  precision     recall         f1  precision_neg  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
      "mean       0.913000   0.908966   0.918000   0.913418       0.917276   \n",
      "std        0.008913   0.009699   0.012517   0.008996       0.011893   \n",
      "min        0.896500   0.893744   0.900000   0.896861       0.899295   \n",
      "25%        0.909875   0.903241   0.909000   0.910733       0.909756   \n",
      "50%        0.913000   0.907388   0.918000   0.912916       0.917213   \n",
      "75%        0.914875   0.913287   0.923750   0.915518       0.922448   \n",
      "max        0.927500   0.926707   0.944000   0.928677       0.942089   \n",
      "ci_lower   0.906720   0.902132   0.909181   0.907080       0.908896   \n",
      "ci_upper   0.919280   0.915800   0.926819   0.919756       0.925656   \n",
      "\n",
      "          recall_neg     f1_neg  \n",
      "count       10.00000  10.000000  \n",
      "mean         0.90800   0.912570  \n",
      "std          0.01036   0.008864  \n",
      "min          0.89300   0.896136  \n",
      "25%          0.90200   0.909000  \n",
      "50%          0.90600   0.912834  \n",
      "75%          0.91100   0.914569  \n",
      "max          0.92700   0.926284  \n",
      "ci_lower     0.90070   0.906325  \n",
      "ci_upper     0.91530   0.918815  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Throw out some data to make sample # same as split by country\n",
    "_, train_index = train_test_split(np.array(range(len(X_all))),test_size=0.1, stratify=y_all)\n",
    "\n",
    "train_index, val_index = list(zip(*[train_test_split(np.array(range(len(X_all))), \n",
    "                            test_size=0.2, stratify=y_all) for i in range(10)]))\n",
    "\n",
    "test_result, test_result_xgb, test_result_svm = run_test(X_all, y_all, X_all, y_all, \n",
    "                                                         train_index,val_index,class_weight = {0: 1., 1: 1.}, epochs=1)\n",
    "print(test_result)\n",
    "print(test_result_xgb)\n",
    "print(test_result_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec8dbc",
   "metadata": {},
   "source": [
    "### Split by Country - Shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1c00593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 202s 809ms/step - loss: 0.3102 - accuracy: 0.8671 - precision: 0.8555 - recall: 0.8835 - TP: 3534.0000 - TN: 3403.0000 - FP: 597.0000 - FN: 466.0000\n",
      "63/63 [==============================] - 17s 265ms/step - loss: 0.8276 - accuracy: 0.7080 - precision: 0.6525 - recall: 0.8900 - TP: 890.0000 - TN: 526.0000 - FP: 474.0000 - FN: 110.0000\n",
      "[0.8275672793388367, 0.7080000042915344, 0.6524926424026489, 0.8899999856948853, 890.0, 526.0, 474.0, 110.0, 0.7529610603333862, 0.8270440251572327, 0.526, 0.6430317848410758]\n",
      "[02:51:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.688, 0.6525974025974026, 0.804, 0.7204301075268817, 0.7447916666666666, 0.572, 0.6470588235294118]\n",
      "[0.6935, 0.6455981941309256, 0.858, 0.7367969085444397, 0.7883755588673621, 0.529, 0.6331538001196888]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 825ms/step - loss: 0.3389 - accuracy: 0.8577 - precision: 0.8396 - recall: 0.8845 - TP: 3538.0000 - TN: 3324.0000 - FP: 676.0000 - FN: 462.0000\n",
      "63/63 [==============================] - 16s 253ms/step - loss: 0.6795 - accuracy: 0.7185 - precision: 0.6504 - recall: 0.9450 - TP: 945.0000 - TN: 492.0000 - FP: 508.0000 - FN: 55.0000\n",
      "[0.6795116662979126, 0.718500018119812, 0.6503785252571106, 0.9449999928474426, 945.0, 492.0, 508.0, 55.0, 0.7704851165305981, 0.8994515539305301, 0.492, 0.6360698125404007]\n",
      "[02:57:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.715, 0.6771004942339374, 0.822, 0.7425474254742548, 0.7735368956743003, 0.608, 0.6808510638297873]\n",
      "[0.716, 0.6653905053598775, 0.869, 0.753686036426713, 0.8112391930835735, 0.563, 0.6646989374262101]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 826ms/step - loss: 0.3026 - accuracy: 0.8712 - precision: 0.8556 - recall: 0.8932 - TP: 3573.0000 - TN: 3397.0000 - FP: 603.0000 - FN: 427.0000\n",
      "63/63 [==============================] - 16s 254ms/step - loss: 0.7116 - accuracy: 0.6785 - precision: 0.6554 - recall: 0.7530 - TP: 753.0000 - TN: 604.0000 - FP: 396.0000 - FN: 247.0000\n",
      "[0.7115849852561951, 0.6784999966621399, 0.6553524732589722, 0.753000020980835, 753.0, 604.0, 396.0, 247.0, 0.7007910706051134, 0.7097532314923619, 0.604, 0.6526202052944354]\n",
      "[03:02:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.6845, 0.6493927125506073, 0.802, 0.7176733780760627, 0.7411764705882353, 0.567, 0.6424929178470256]\n",
      "[0.693, 0.6486902927580893, 0.842, 0.7328111401218451, 0.7749287749287749, 0.544, 0.6392479435957696]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 824ms/step - loss: 0.3104 - accuracy: 0.8705 - precision: 0.8459 - recall: 0.9060 - TP: 3624.0000 - TN: 3340.0000 - FP: 660.0000 - FN: 376.0000\n",
      "63/63 [==============================] - 16s 254ms/step - loss: 0.6419 - accuracy: 0.7350 - precision: 0.6962 - recall: 0.8340 - TP: 834.0000 - TN: 636.0000 - FP: 364.0000 - FN: 166.0000\n",
      "[0.6419467329978943, 0.7350000143051147, 0.6961602568626404, 0.8339999914169312, 834.0, 636.0, 364.0, 166.0, 0.758871691904221, 0.7930174563591023, 0.636, 0.7058823529411765]\n",
      "[03:08:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.6925, 0.6586974443528442, 0.799, 0.7220967013104383, 0.7445997458703939, 0.586, 0.6558477895914941]\n",
      "[0.7275, 0.6863226863226863, 0.838, 0.754615038271049, 0.79204107830552, 0.617, 0.6936481169196177]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 826ms/step - loss: 0.2984 - accuracy: 0.8742 - precision: 0.8658 - recall: 0.8857 - TP: 3543.0000 - TN: 3451.0000 - FP: 549.0000 - FN: 457.0000\n",
      "63/63 [==============================] - 16s 260ms/step - loss: 0.8789 - accuracy: 0.6940 - precision: 0.6502 - recall: 0.8400 - TP: 840.0000 - TN: 548.0000 - FP: 452.0000 - FN: 160.0000\n",
      "[0.8789104223251343, 0.6940000057220459, 0.6501547694206238, 0.8399999737739563, 840.0, 548.0, 452.0, 160.0, 0.7329842645624146, 0.7740112994350282, 0.548, 0.6416861826697893]\n",
      "[03:13:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.6815, 0.6437054631828979, 0.813, 0.7185152452496686, 0.746268656716418, 0.55, 0.6332757628094415]\n",
      "[0.693, 0.6480061349693251, 0.845, 0.7335069444444444, 0.7772988505747126, 0.541, 0.6379716981132075]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 826ms/step - loss: 0.3023 - accuracy: 0.8605 - precision: 0.8307 - recall: 0.9055 - TP: 3622.0000 - TN: 3262.0000 - FP: 738.0000 - FN: 378.0000\n",
      "63/63 [==============================] - 16s 259ms/step - loss: 0.7339 - accuracy: 0.7325 - precision: 0.6757 - recall: 0.8940 - TP: 894.0000 - TN: 571.0000 - FP: 429.0000 - FN: 106.0000\n",
      "[0.733902633190155, 0.7325000166893005, 0.6757369637489319, 0.8939999938011169, 894.0, 571.0, 429.0, 106.0, 0.7696943599335108, 0.843426883308715, 0.571, 0.6809779367918903]\n",
      "[03:19:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.709, 0.6622670807453416, 0.853, 0.7456293706293706, 0.7935393258426966, 0.565, 0.6600467289719626]\n",
      "[0.737, 0.6848673946957878, 0.878, 0.7695004382120947, 0.83008356545961, 0.596, 0.6938300349243306]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 826ms/step - loss: 0.2944 - accuracy: 0.8664 - precision: 0.8487 - recall: 0.8917 - TP: 3567.0000 - TN: 3364.0000 - FP: 636.0000 - FN: 433.0000\n",
      "63/63 [==============================] - 16s 254ms/step - loss: 0.6803 - accuracy: 0.6875 - precision: 0.6676 - recall: 0.7470 - TP: 747.0000 - TN: 628.0000 - FP: 372.0000 - FN: 253.0000\n",
      "[0.6803498864173889, 0.6875, 0.667560338973999, 0.746999979019165, 747.0, 628.0, 372.0, 253.0, 0.7050495519555693, 0.7128263337116912, 0.628, 0.6677299308878256]\n",
      "[03:25:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.7, 0.6584786053882726, 0.831, 0.7347480106100797, 0.7710027100271003, 0.569, 0.6547756041426926]\n",
      "[0.71, 0.661042944785276, 0.862, 0.7482638888888888, 0.8017241379310345, 0.558, 0.6580188679245284]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 823ms/step - loss: 0.3104 - accuracy: 0.8677 - precision: 0.8372 - recall: 0.9130 - TP: 3652.0000 - TN: 3290.0000 - FP: 710.0000 - FN: 348.0000\n",
      "63/63 [==============================] - 16s 260ms/step - loss: 0.7860 - accuracy: 0.7455 - precision: 0.6905 - recall: 0.8900 - TP: 890.0000 - TN: 601.0000 - FP: 399.0000 - FN: 110.0000\n",
      "[0.7860105633735657, 0.7455000281333923, 0.6904577016830444, 0.8899999856948853, 890.0, 601.0, 399.0, 110.0, 0.7776321372327734, 0.8452883263009845, 0.601, 0.7025131502045587]\n",
      "[03:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.726, 0.6810897435897436, 0.85, 0.7562277580071174, 0.800531914893617, 0.602, 0.687214611872146]\n",
      "[0.7415, 0.6906077348066298, 0.875, 0.771945302161447, 0.8294679399727148, 0.608, 0.701673398730525]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 206s 825ms/step - loss: 0.2907 - accuracy: 0.8740 - precision: 0.8601 - recall: 0.8932 - TP: 3573.0000 - TN: 3419.0000 - FP: 581.0000 - FN: 427.0000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 1.0115 - accuracy: 0.6715 - precision: 0.6197 - recall: 0.8880 - TP: 888.0000 - TN: 455.0000 - FP: 545.0000 - FN: 112.0000\n",
      "[1.0114799737930298, 0.671500027179718, 0.6196789741516113, 0.8880000114440918, 888.0, 455.0, 545.0, 112.0, 0.7299629979532722, 0.8024691358024691, 0.455, 0.5807275047862157]\n",
      "[03:36:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.667, 0.6235207100591716, 0.843, 0.7168367346938775, 0.7577160493827161, 0.491, 0.595873786407767]\n",
      "[0.6755, 0.6306775874906925, 0.847, 0.7230046948356808, 0.7671232876712328, 0.504, 0.6083283041641521]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 189s 757ms/step - loss: 0.3412 - accuracy: 0.8504 - precision: 0.8136 - recall: 0.9090 - TP: 3636.0000 - TN: 3167.0000 - FP: 833.0000 - FN: 364.0000\n",
      "63/63 [==============================] - 16s 257ms/step - loss: 0.8217 - accuracy: 0.7215 - precision: 0.6627 - recall: 0.9020 - TP: 902.0000 - TN: 541.0000 - FP: 459.0000 - FN: 98.0000\n",
      "[0.8217496275901794, 0.7214999794960022, 0.6627479791641235, 0.9020000100135803, 902.0, 541.0, 459.0, 98.0, 0.7640830190894456, 0.8466353677621283, 0.541, 0.6601586333129958]\n",
      "[03:41:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.6975, 0.6488319517709118, 0.861, 0.740008594757198, 0.7934621099554234, 0.534, 0.638374178123132]\n",
      "[0.707, 0.6556390977443609, 0.872, 0.7484978540772533, 0.808955223880597, 0.542, 0.6491017964071857]\n",
      "               loss   accuracy  precision     recall          TP          TN  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000   10.000000   10.000000   \n",
      "mean       0.777301   0.709250   0.662072   0.858300  858.300000  560.200000   \n",
      "std        0.112164   0.025454   0.022106   0.064974   64.973584   59.111571   \n",
      "min        0.641947   0.671500   0.619679   0.747000  747.000000  455.000000   \n",
      "25%        0.688159   0.689125   0.650907   0.835500  835.500000  529.750000   \n",
      "50%        0.759957   0.713250   0.659050   0.889000  889.000000  559.500000   \n",
      "75%        0.826113   0.729750   0.673693   0.893000  893.000000  603.250000   \n",
      "max        1.011480   0.745500   0.696160   0.945000  945.000000  636.000000   \n",
      "ci_lower   0.698270   0.691315   0.646496   0.812520  812.519653  518.550025   \n",
      "ci_upper   0.856332   0.727185   0.677648   0.904080  904.080347  601.849975   \n",
      "\n",
      "                  FP          FN         f1  precision_neg  recall_neg  \\\n",
      "count      10.000000   10.000000  10.000000      10.000000   10.000000   \n",
      "mean      439.800000  141.700000   0.746252       0.805392    0.560200   \n",
      "std        59.111571   64.973584   0.027587       0.060477    0.059112   \n",
      "min       364.000000   55.000000   0.700791       0.709753    0.455000   \n",
      "25%       396.750000  107.000000   0.730718       0.778763    0.529750   \n",
      "50%       440.500000  111.000000   0.755916       0.814757    0.559500   \n",
      "75%       470.250000  164.500000   0.768292       0.844823    0.603250   \n",
      "max       545.000000  253.000000   0.777632       0.899452    0.636000   \n",
      "ci_lower  398.150025   95.919653   0.726814       0.762780    0.518550   \n",
      "ci_upper  481.449975  187.480347   0.765689       0.848004    0.601850   \n",
      "\n",
      "             f1_neg  \n",
      "count     10.000000  \n",
      "mean       0.657140  \n",
      "std        0.036309  \n",
      "min        0.580728  \n",
      "25%        0.642023  \n",
      "50%        0.656389  \n",
      "75%        0.677666  \n",
      "max        0.705882  \n",
      "ci_lower   0.631556  \n",
      "ci_upper   0.682723  \n",
      "           accuracy  precision     recall         f1  precision_neg  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
      "mean       0.696100   0.655568   0.827800   0.731471       0.766663   \n",
      "std        0.017346   0.016459   0.023059   0.014149       0.023008   \n",
      "min        0.667000   0.623521   0.799000   0.716837       0.741176   \n",
      "25%        0.685375   0.648972   0.806250   0.718994       0.745161   \n",
      "50%        0.695000   0.655538   0.826500   0.728422       0.764359   \n",
      "75%        0.706750   0.661375   0.848250   0.741913       0.788481   \n",
      "max        0.726000   0.681090   0.861000   0.756228       0.800532   \n",
      "ci_lower   0.683878   0.643971   0.811552   0.721502       0.750451   \n",
      "ci_upper   0.708322   0.667166   0.844048   0.741441       0.782874   \n",
      "\n",
      "          recall_neg     f1_neg  \n",
      "count      10.000000  10.000000  \n",
      "mean        0.564400   0.649581  \n",
      "std         0.033971   0.025609  \n",
      "min         0.491000   0.595874  \n",
      "25%         0.553750   0.639404  \n",
      "50%         0.568000   0.650917  \n",
      "75%         0.582500   0.658997  \n",
      "max         0.608000   0.687215  \n",
      "ci_lower    0.540464   0.631537  \n",
      "ci_upper    0.588336   0.667625  \n",
      "           accuracy  precision     recall         f1  precision_neg  \\\n",
      "count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
      "mean       0.709400   0.661684   0.858600   0.747263       0.798124   \n",
      "std        0.021368   0.020022   0.014774   0.015978       0.022038   \n",
      "min        0.675500   0.630678   0.838000   0.723005       0.767123   \n",
      "25%        0.693125   0.648177   0.845500   0.734329       0.780068   \n",
      "50%        0.708500   0.658341   0.860000   0.748381       0.796883   \n",
      "75%        0.724625   0.679998   0.871250   0.754383       0.810668   \n",
      "max        0.741500   0.690608   0.878000   0.771945       0.830084   \n",
      "ci_lower   0.694344   0.647577   0.848190   0.736005       0.782596   \n",
      "ci_upper   0.724456   0.675792   0.869010   0.758521       0.813651   \n",
      "\n",
      "          recall_neg     f1_neg  \n",
      "count      10.000000  10.000000  \n",
      "mean        0.560200   0.657967  \n",
      "std         0.036362   0.030593  \n",
      "min         0.504000   0.608328  \n",
      "25%         0.541250   0.638291  \n",
      "50%         0.551000   0.653560  \n",
      "75%         0.587750   0.686411  \n",
      "max         0.617000   0.701673  \n",
      "ci_lower    0.534580   0.636411  \n",
      "ci_upper    0.585820   0.679523  \n"
     ]
    }
   ],
   "source": [
    "train_index = [val_split for _, val_split in skf_train.split(X_train,y_train)] \n",
    "val_index = [val_split for _, val_split in skf_val.split(X_val,y_val)] \n",
    "\n",
    "test_result, test_result_xgb, test_result_svm = run_test(X_train, y_train, X_val, y_val,\n",
    "                       train_index, val_index, class_weight = {0: 1., 1: 1.}, epochs=1)\n",
    "print(test_result)\n",
    "print(test_result_xgb)\n",
    "print(test_result_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860fb0f",
   "metadata": {},
   "source": [
    "### Country Balanced Split - Shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a9ed1f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val [532, 468]\n",
      "Val [1000, 1000]\n",
      "Train [240, 760]\n",
      "Train [481, 1519]\n",
      "Train [749, 2251]\n",
      "Train [1028, 2972]\n",
      "Train [1268, 3732]\n",
      "Train [1522, 4478]\n",
      "Train [2000, 5000]\n",
      "Train [3000, 5000]\n",
      "Train [4000, 5000]\n",
      "Train [5000, 5000]\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "train_count = 0  \n",
    "val_count = 0  \n",
    "train_label_counter = [0, 0]\n",
    "val_label_counter = [0, 0]\n",
    "\n",
    "train_label_count_max = 5e3 \n",
    "val_label_count_max = 1e3\n",
    "total_train = 2 * train_label_count_max\n",
    "total_val = 2 * val_label_count_max\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for line in fs.open('s3://compressed-data-sample/shuffled_train.json'):\n",
    "    if train_count >= total_train and val_count >= total_val:\n",
    "        break\n",
    "    json_file = json.loads(line)\n",
    "    country = json_file['country']\n",
    "    label =  int(json_file['country'] in PEACE_COUNTRY)\n",
    "    \n",
    "    if not country in MAJOR_COUNTRY:\n",
    "        if train_label_counter[label] < train_label_count_max :\n",
    "            sent = json_file['content_cleaned_shuffled']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_train.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_train.append(json_file['country'])\n",
    "            train_count += 1\n",
    "            train_label_counter[label] += 1\n",
    "            if sum(train_label_counter) % 1e3 == 0:\n",
    "                print('Train', train_label_counter)\n",
    "    else:\n",
    "        if val_label_counter[label] < val_label_count_max :\n",
    "            sent = json_file['content_cleaned_shuffled']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_val.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_val.append(json_file['country'])\n",
    "            val_count += 1\n",
    "            val_label_counter[label] += 1\n",
    "            if sum(val_label_counter) % 1e3 == 0:\n",
    "                print('Val', val_label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9294ceb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median = statistics.median(list(Counter(y_train).values()))\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d07132e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare K-Fold for minority country\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "australia_idx = np.where(np.array(y_val) == 'Australia')[0]\n",
    "india_idx = np.where(np.array(y_val) == 'India')[0]\n",
    "majority_index = []\n",
    "for i in range(10):\n",
    "    australia_sample = np.array(random.sample(list(australia_idx), median))\n",
    "    india_sample = np.array(random.sample(list(india_idx), median))\n",
    "    majority_index.append(np.hstack([australia_sample, india_sample]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6d08d998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 215s 812ms/step - loss: 0.3168 - accuracy: 0.8547 - precision: 0.8328 - recall: 0.8870 - TP: 3745.0000 - TN: 3485.0000 - FP: 752.0000 - FN: 477.0000\n",
      "67/67 [==============================] - 18s 265ms/step - loss: 0.2325 - accuracy: 0.9106 - precision: 0.9311 - recall: 0.8883 - TP: 946.0000 - TN: 980.0000 - FP: 70.0000 - FN: 119.0000\n",
      "[0.2325342297554016, 0.9106382727622986, 0.9311023354530334, 0.888262927532196, 946.0, 980.0, 70.0, 119.0, 0.90917827568558, 0.89171974522293, 0.9333333333333333, 0.9120521172638437]\n",
      "[03:58:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9125295508274232, 0.9104477611940298, 0.9164319248826291, 0.9134300421151147, 0.9146692233940557, 0.9085714285714286, 0.9116101290014333]\n",
      "[0.915839243498818, 0.9140989729225023, 0.9192488262910798, 0.9166666666666666, 0.9176245210727969, 0.9123809523809524, 0.9149952244508118]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 219s 828ms/step - loss: 0.3133 - accuracy: 0.8661 - precision: 0.8452 - recall: 0.8968 - TP: 3799.0000 - TN: 3527.0000 - FP: 696.0000 - FN: 437.0000\n",
      "67/67 [==============================] - 17s 258ms/step - loss: 0.2159 - accuracy: 0.9158 - precision: 0.9091 - recall: 0.9229 - TP: 970.0000 - TN: 967.0000 - FP: 97.0000 - FN: 81.0000\n",
      "[0.2158750742673874, 0.9158392548561096, 0.9090909361839294, 0.9229305386543274, 970.0, 967.0, 97.0, 81.0, 0.9159584633057599, 0.9227099236641222, 0.9088345864661654, 0.915719696969697]\n",
      "[04:04:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9106382978723404, 0.9058380414312618, 0.9153187440532826, 0.9105537150970185, 0.9154795821462488, 0.9060150375939849, 0.9107227208313652]\n",
      "[0.91725768321513, 0.9040590405904059, 0.9324452901998097, 0.9180327868852459, 0.9311348205625606, 0.9022556390977443, 0.9164677804295942]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 198s 747ms/step - loss: 0.3155 - accuracy: 0.8618 - precision: 0.8423 - recall: 0.8914 - TP: 3782.0000 - TN: 3508.0000 - FP: 708.0000 - FN: 461.0000\n",
      "67/67 [==============================] - 16s 241ms/step - loss: 0.1923 - accuracy: 0.9248 - precision: 0.9210 - recall: 0.9272 - TP: 968.0000 - TN: 988.0000 - FP: 83.0000 - FN: 76.0000\n",
      "[0.19227299094200134, 0.9248226881027222, 0.9210276007652283, 0.9272030591964722, 968.0, 988.0, 83.0, 76.0, 0.924105013009148, 0.9285714285714286, 0.9225023342670402, 0.9255269320843093]\n",
      "[04:10:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9153664302600473, 0.9130850047755492, 0.9157088122605364, 0.9143950263032042, 0.9176029962546817, 0.9150326797385621, 0.9163160355306218]\n",
      "[0.9271867612293144, 0.9278846153846154, 0.9243295019157088, 0.9261036468330135, 0.9265116279069767, 0.9299719887955182, 0.9282385834109972]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 823ms/step - loss: 0.3001 - accuracy: 0.8698 - precision: 0.8647 - recall: 0.8787 - TP: 3739.0000 - TN: 3619.0000 - FP: 585.0000 - FN: 516.0000\n",
      "67/67 [==============================] - 16s 242ms/step - loss: 0.2127 - accuracy: 0.9087 - precision: 0.9158 - recall: 0.8953 - TP: 924.0000 - TN: 998.0000 - FP: 85.0000 - FN: 108.0000\n",
      "[0.21267084777355194, 0.9087470173835754, 0.9157581925392151, 0.895348846912384, 924.0, 998.0, 85.0, 108.0, 0.9054385233783055, 0.9023508137432188, 0.9215143120960295, 0.9118318867062585]\n",
      "[04:16:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9111111111111111, 0.9065510597302505, 0.9118217054263565, 0.9091787439613527, 0.9155060352831941, 0.9104339796860572, 0.9129629629629629]\n",
      "[0.91725768321513, 0.9172346640701071, 0.9127906976744186, 0.9150072850898495, 0.9172794117647058, 0.9215143120960295, 0.9193919852602488]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 203s 764ms/step - loss: 0.3227 - accuracy: 0.8545 - precision: 0.8449 - recall: 0.8664 - TP: 3645.0000 - TN: 3583.0000 - FP: 669.0000 - FN: 562.0000\n",
      "67/67 [==============================] - 17s 258ms/step - loss: 0.2050 - accuracy: 0.9210 - precision: 0.9072 - recall: 0.9417 - TP: 1017.0000 - TN: 931.0000 - FP: 104.0000 - FN: 63.0000\n",
      "[0.20497092604637146, 0.9210401773452759, 0.9072256684303284, 0.9416666626930237, 1017.0, 931.0, 104.0, 63.0, 0.9241253837438721, 0.9366197183098591, 0.8995169082125604, 0.917693445046821]\n",
      "[04:22:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9134751773049645, 0.9126034958601656, 0.9185185185185185, 0.9155514536225196, 0.914396887159533, 0.9082125603864735, 0.9112942317014058]\n",
      "[0.924822695035461, 0.9122649955237243, 0.9435185185185185, 0.9276285844333183, 0.938877755511022, 0.9053140096618357, 0.9217904574520412]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 824ms/step - loss: 0.3224 - accuracy: 0.8629 - precision: 0.8492 - recall: 0.8847 - TP: 3767.0000 - TN: 3532.0000 - FP: 669.0000 - FN: 491.0000\n",
      "67/67 [==============================] - 17s 255ms/step - loss: 0.2132 - accuracy: 0.9069 - precision: 0.8962 - recall: 0.9145 - TP: 941.0000 - TN: 977.0000 - FP: 109.0000 - FN: 88.0000\n",
      "[0.21315714716911316, 0.9068558216094971, 0.8961904644966125, 0.9144800901412964, 941.0, 977.0, 109.0, 88.0, 0.9052429053506342, 0.9173708920187793, 0.8996316758747698, 0.9084146908414691]\n",
      "[04:28:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9063829787234042, 0.8938388625592417, 0.9164237123420796, 0.9049904030710172, 0.9188679245283019, 0.8968692449355433, 0.9077353215284251]\n",
      "[0.9125295508274232, 0.8996212121212122, 0.923226433430515, 0.9112709832134293, 0.9254013220018886, 0.9023941068139963, 0.9137529137529138]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 218s 821ms/step - loss: 0.3171 - accuracy: 0.8588 - precision: 0.8384 - recall: 0.8899 - TP: 3773.0000 - TN: 3492.0000 - FP: 727.0000 - FN: 467.0000\n",
      "67/67 [==============================] - 17s 256ms/step - loss: 0.2134 - accuracy: 0.9106 - precision: 0.9307 - recall: 0.8854 - TP: 927.0000 - TN: 999.0000 - FP: 69.0000 - FN: 120.0000\n",
      "[0.21344351768493652, 0.9106382727622986, 0.9307228922843933, 0.8853868246078491, 927.0, 999.0, 69.0, 120.0, 0.9074889898168048, 0.8927613941018767, 0.9353932584269663, 0.9135802469135803]\n",
      "[04:34:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9205673758865248, 0.9157994323557237, 0.9245463228271251, 0.9201520912547528, 0.9253308128544423, 0.9166666666666666, 0.9209783631232361]\n",
      "[0.9243498817966903, 0.9195837275307474, 0.9283667621776505, 0.9239543726235742, 0.9291115311909263, 0.9204119850187266, 0.9247412982126058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 219s 828ms/step - loss: 0.3027 - accuracy: 0.8667 - precision: 0.8556 - recall: 0.8800 - TP: 3696.0000 - TN: 3635.0000 - FP: 624.0000 - FN: 504.0000\n",
      "67/67 [==============================] - 17s 257ms/step - loss: 0.2059 - accuracy: 0.9177 - precision: 0.9533 - recall: 0.8832 - TP: 960.0000 - TN: 981.0000 - FP: 47.0000 - FN: 127.0000\n",
      "[0.20593100786209106, 0.9177305102348328, 0.9533267021179199, 0.883164644241333, 960.0, 981.0, 47.0, 127.0, 0.9169054233670595, 0.8853790613718412, 0.9542801556420234, 0.9185393258426966]\n",
      "[04:40:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.926241134751773, 0.9306197964847364, 0.9254829806807727, 0.9280442804428044, 0.9216634429400387, 0.9270428015564203, 0.9243452958292919]\n",
      "[0.9347517730496454, 0.9357208448117539, 0.937442502299908, 0.936580882352941, 0.9337231968810916, 0.9319066147859922, 0.9328140214216163]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 219s 827ms/step - loss: 0.3166 - accuracy: 0.8675 - precision: 0.8377 - recall: 0.9141 - TP: 3896.0000 - TN: 3442.0000 - FP: 755.0000 - FN: 366.0000\n",
      "67/67 [==============================] - 17s 256ms/step - loss: 0.1943 - accuracy: 0.9187 - precision: 0.9161 - recall: 0.9161 - TP: 939.0000 - TN: 1004.0000 - FP: 86.0000 - FN: 86.0000\n",
      "[0.1943126618862152, 0.9186761379241943, 0.9160975813865662, 0.9160975813865662, 939.0, 1004.0, 86.0, 86.0, 0.9160975813865662, 0.9211009174311927, 0.9211009174311927, 0.9211009174311927]\n",
      "[04:46:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9139479905437352, 0.9096209912536443, 0.9131707317073171, 0.9113924050632911, 0.91804788213628, 0.9146788990825688, 0.9163602941176471]\n",
      "[0.9295508274231679, 0.9227799227799228, 0.9326829268292683, 0.9277049975739933, 0.9360518999073216, 0.926605504587156, 0.9313047487321346]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 219s 826ms/step - loss: 0.3059 - accuracy: 0.8622 - precision: 0.8436 - recall: 0.8880 - TP: 3742.0000 - TN: 3551.0000 - FP: 694.0000 - FN: 472.0000\n",
      "67/67 [==============================] - 17s 255ms/step - loss: 0.1845 - accuracy: 0.9225 - precision: 0.9260 - recall: 0.9208 - TP: 988.0000 - TN: 963.0000 - FP: 79.0000 - FN: 85.0000\n",
      "[0.1844778060913086, 0.9224586486816406, 0.9259606599807739, 0.9207828640937805, 988.0, 963.0, 79.0, 85.0, 0.9233645034304653, 0.9188931297709924, 0.9241842610364683, 0.9215311004784689]\n",
      "[04:52:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9224586288416076, 0.9275634995296331, 0.918918918918919, 0.9232209737827715, 0.9173003802281369, 0.9261036468330134, 0.9216809933142311]\n",
      "[0.9271867612293144, 0.9282385834109972, 0.9282385834109972, 0.9282385834109972, 0.9261036468330134, 0.9261036468330134, 0.9261036468330134]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(               loss   accuracy  precision     recall           TP  \\\n",
       " count     10.000000  10.000000  10.000000  10.000000    10.000000   \n",
       " mean       0.206965   0.915745   0.920650   0.909532   958.000000   \n",
       " std        0.013875   0.006220   0.015859   0.020139    28.944391   \n",
       " min        0.184478   0.906856   0.896190   0.883165   924.000000   \n",
       " 25%        0.196977   0.910638   0.910758   0.890034   939.500000   \n",
       " 50%        0.209301   0.916785   0.918563   0.915289   953.000000   \n",
       " 75%        0.213372   0.920449   0.929532   0.922394   969.500000   \n",
       " max        0.232534   0.924823   0.953327   0.941667  1017.000000   \n",
       " ci_lower   0.197188   0.911362   0.909476   0.895342   937.605800   \n",
       " ci_upper   0.216741   0.920127   0.931825   0.923723   978.394200   \n",
       " \n",
       "                    TN          FP          FN         f1  precision_neg  \\\n",
       " count       10.000000   10.000000   10.000000  10.000000      10.000000   \n",
       " mean       978.800000   82.900000   95.300000   0.914791       0.911748   \n",
       " std         21.549942   18.266241   21.613010   0.007589       0.017439   \n",
       " min        931.000000   47.000000   63.000000   0.905243       0.885379   \n",
       " 25%        969.500000   72.250000   82.000000   0.907911       0.895159   \n",
       " 50%        980.500000   84.000000   87.000000   0.916028       0.918132   \n",
       " 75%        995.500000   94.250000  116.250000   0.921750       0.922308   \n",
       " max       1004.000000  109.000000  127.000000   0.924125       0.936620   \n",
       " ci_lower   963.615924   70.029618   80.071486   0.909443       0.899460   \n",
       " ci_upper   993.984076   95.770382  110.528514   0.920138       0.924035   \n",
       " \n",
       "           recall_neg     f1_neg  \n",
       " count      10.000000  10.000000  \n",
       " mean        0.922029   0.916599  \n",
       " std         0.016783   0.005266  \n",
       " min         0.899517   0.908415  \n",
       " 25%         0.911901   0.912434  \n",
       " 50%         0.922008   0.916707  \n",
       " 75%         0.931046   0.920461  \n",
       " max         0.954280   0.925527  \n",
       " ci_lower    0.910204   0.912889  \n",
       " ci_upper    0.933855   0.920309  ,\n",
       "            accuracy  precision     recall         f1  precision_neg  \\\n",
       " count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
       " mean       0.915272   0.912597   0.917634   0.915091       0.917887   \n",
       " std        0.006058   0.010577   0.004441   0.006946       0.003417   \n",
       " min        0.906383   0.893839   0.911822   0.904990       0.914397   \n",
       " 25%        0.911466   0.907319   0.915416   0.910763       0.915486   \n",
       " 50%        0.913712   0.911526   0.916428   0.913913       0.917452   \n",
       " 75%        0.919267   0.915121   0.918819   0.919002       0.918663   \n",
       " max        0.926241   0.930620   0.925483   0.928044       0.925331   \n",
       " ci_lower   0.911003   0.905144   0.914505   0.910197       0.915479   \n",
       " ci_upper   0.919541   0.920049   0.920763   0.919985       0.920294   \n",
       " \n",
       "           recall_neg     f1_neg  \n",
       " count      10.000000  10.000000  \n",
       " mean        0.912963   0.915401  \n",
       " std         0.009113   0.005479  \n",
       " min         0.896869   0.907735  \n",
       " 25%         0.908302   0.911373  \n",
       " 50%         0.912556   0.914639  \n",
       " 75%         0.916258   0.919824  \n",
       " max         0.927043   0.924345  \n",
       " ci_lower    0.906542   0.911540  \n",
       " ci_upper    0.919384   0.919261  ,\n",
       "            accuracy  precision     recall         f1  precision_neg  \\\n",
       " count     10.000000  10.000000  10.000000  10.000000      10.000000   \n",
       " mean       0.923073   0.918149   0.928229   0.923119       0.928182   \n",
       " std        0.007056   0.011168   0.008916   0.007689       0.007163   \n",
       " min        0.912530   0.899621   0.912791   0.911271       0.917279   \n",
       " 25%        0.917258   0.912723   0.923502   0.917008       0.925577   \n",
       " 50%        0.924586   0.918409   0.928303   0.925029       0.927812   \n",
       " 75%        0.927187   0.926608   0.932624   0.927686       0.933076   \n",
       " max        0.934752   0.935721   0.943519   0.936581       0.938878   \n",
       " ci_lower   0.918102   0.910280   0.921947   0.917701       0.923135   \n",
       " ci_upper   0.928045   0.926018   0.934511   0.928537       0.933229   \n",
       " \n",
       "           recall_neg     f1_neg  \n",
       " count      10.000000  10.000000  \n",
       " mean        0.917886   0.922960  \n",
       " std         0.011443   0.006767  \n",
       " min         0.902256   0.913753  \n",
       " 25%         0.907081   0.917199  \n",
       " 50%         0.920963   0.923266  \n",
       " 75%         0.926480   0.927705  \n",
       " max         0.931907   0.932814  \n",
       " ci_lower    0.909823   0.918192  \n",
       " ci_upper    0.925948   0.927728  )"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = []\n",
    "eval_results_xgb = []\n",
    "eval_results_svm = []\n",
    "metrics_names = None\n",
    "    \n",
    "for maj_index in majority_index:\n",
    "    tf.keras.backend.clear_session()\n",
    "    X_all_sample = np.hstack([X_train, X_val[maj_index]])\n",
    "    y_all_sample = np.hstack([y_train, y_val[maj_index]])\n",
    "    y_all_sample = np.array(list(map(lambda x: int(x in PEACE_COUNTRY), y_all_sample)))\n",
    "    \n",
    "    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(X_all_sample, y_all_sample, test_size=0.2)\n",
    "    \n",
    "    model = get_model()\n",
    "        \n",
    "    train_input1 = np.vstack([x['input_ids'] for x in X_train_fold])\n",
    "    train_input2 = np.vstack([x['attention_mask'] for x in X_train_fold])\n",
    "    model.fit(x=[train_input1, train_input2], \n",
    "              y=np.asarray(y_train_fold),\n",
    "              epochs = 1, \n",
    "              batch_size = 32,\n",
    "              class_weight={0: 1., 1: 1.})\n",
    "\n",
    "    eval_input1 = np.vstack([x['input_ids'] for x in X_val_fold])\n",
    "    eval_input2 = np.vstack([x['attention_mask'] for x in X_val_fold])\n",
    "    er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                        y=np.asarray(y_val_fold), return_dict=True)\n",
    "    f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "\n",
    "    precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "    recall_neg = er['TN'] / (er['TN']+er['FP'])\n",
    "    f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "\n",
    "    er = list(er.values())\n",
    "    er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "    eval_results.append(er)\n",
    "    metrics_names = model.metrics_names\n",
    "    print(er)\n",
    "\n",
    "    # Get Embeddings \n",
    "    input_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='input_ids', dtype='int32')\n",
    "    input_masks_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='attention_mask', dtype='int32')\n",
    "    output_layer = model.layers[2].layers[0](input_ids_in, input_masks_ids_in)[0]\n",
    "    embedding_model = tf.keras.Model(inputs=[input_ids_in, input_masks_ids_in], outputs = output_layer)\n",
    "\n",
    "    train_embedding = embedding_model.predict(x=[train_input1, train_input2])[:, 0]\n",
    "    val_embedding = embedding_model.predict(x=[eval_input1, eval_input2])[:, 0]\n",
    "\n",
    "    # Get XGB results\n",
    "    dtrain = xgb.DMatrix(train_embedding, label=y_train_fold)\n",
    "    dval = xgb.DMatrix(val_embedding, label=y_val_fold)\n",
    "    param = {'max_depth': 2, 'eta': 1,'objective': 'binary:logistic'}\n",
    "    num_round = 10\n",
    "    evallist = [(dval, 'eval'), (dtrain, 'train')]\n",
    "    bst = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)\n",
    "    y_pred = (bst.predict(dval) > 0.5).astype(int)\n",
    "    xgb_acc = np.mean(y_val_fold == y_pred)\n",
    "    xgb_score = precision_recall_fscore_support(y_val_fold, y_pred)\n",
    "    # Accuracy, Precision+, Recall+, F1+, Precision-, Recall-, F1-\n",
    "    er = [xgb_acc, xgb_score[0][1], xgb_score[1][1], xgb_score[2][1], \n",
    "                             xgb_score[0][0], xgb_score[1][0], xgb_score[2][0]]\n",
    "    print(er)\n",
    "    eval_results_xgb.append(er)\n",
    "\n",
    "\n",
    "    # Get SVM results\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('pca', PCA()),\n",
    "                     ('svc', SVC())])\n",
    "    pipe.fit(train_embedding, y_train_fold)\n",
    "    y_pred = (pipe.predict(val_embedding) > 0.5).astype(int)\n",
    "    svm_acc = np.mean(y_val_fold == y_pred)\n",
    "    svm_score = precision_recall_fscore_support(y_val_fold, y_pred)\n",
    "    # Accuracy, Precision+, Recall+, F1+, Precision-, Recall-, F1-\n",
    "    er = [svm_acc, svm_score[0][1], svm_score[1][1], svm_score[2][1], \n",
    "                             svm_score[0][0], svm_score[1][0], svm_score[2][0]]\n",
    "    print(er)\n",
    "    eval_results_svm.append(er)\n",
    "    \n",
    "eval_results = np.array(eval_results)\n",
    "metrics_names += ['f1', 'precision_neg', 'recall_neg', 'f1_neg']\n",
    "eval_results = pd.DataFrame(eval_results, columns=metrics_names)\n",
    "ci = eval_results.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "ci.index = ['ci_lower', 'ci_upper']\n",
    "eval_results = pd.concat([eval_results.describe(), ci])\n",
    "\n",
    "eval_results_xgb = np.array(eval_results_xgb)\n",
    "eval_results_xgb = pd.DataFrame(eval_results_xgb, columns=['accuracy', 'precision', 'recall', 'f1', 'precision_neg', 'recall_neg', 'f1_neg'])\n",
    "ci = eval_results_xgb.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "ci.index = ['ci_lower', 'ci_upper']\n",
    "eval_results_xgb = pd.concat([eval_results_xgb.describe(), ci])\n",
    "\n",
    "eval_results_svm = np.array(eval_results_svm)\n",
    "eval_results_svm = pd.DataFrame(eval_results_svm, columns=['accuracy', 'precision', 'recall', 'f1', 'precision_neg', 'recall_neg', 'f1_neg'])\n",
    "ci = eval_results_svm.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "ci.index = ['ci_lower', 'ci_upper']\n",
    "eval_results_svm = pd.concat([eval_results_svm.describe(), ci])\n",
    "\n",
    "eval_results, eval_results_xgb, eval_results_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e85282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
