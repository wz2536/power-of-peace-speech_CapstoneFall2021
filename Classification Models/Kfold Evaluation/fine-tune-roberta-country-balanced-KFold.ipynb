{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95eec105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 2.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (2.26.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 44.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (21.2)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 50.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.1.2 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow==2.3.0\n",
      "  Downloading tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4 MB)\n",
      "\u001b[K     |███████████████████████████▊    | 277.7 MB 139.1 MB/s eta 0:00:01  |█▍                              | 14.2 MB 4.3 MB/s eta 0:01:12     |████████████████                | 160.3 MB 99.9 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 320.4 MB 43 kB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2.3.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (2.6.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.41.0)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 116.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.36.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (3.19.1)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 114.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gast==0.3.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.14.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (58.5.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.8.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.6.0)\n",
      "Installing collected packages: numpy, scipy, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.3\n",
      "    Uninstalling scipy-1.5.3:\n",
      "      Successfully uninstalled scipy-1.5.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.3.4\n",
      "    Uninstalling tensorflow-2.3.4:\n",
      "      Successfully uninstalled tensorflow-2.3.4\n",
      "Successfully installed numpy-1.18.5 scipy-1.4.1 tensorflow-2.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install tensorflow==2.3.0\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaConfig, AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification\n",
    "# import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05033615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import s3fs\n",
    "import json\n",
    "import re\n",
    "import gc\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "MAX_LEN = 128\n",
    "PEACE_COUNTRY = set(['Australia', 'New Zealand', \n",
    "                 'Belgium', 'Sweden', 'Denmark', \n",
    "                 'Norway', 'Finland', 'Czech Republic', \n",
    "                 'Netherlands', 'Austria'])\n",
    "MAJOR_COUNTRY = set(['Australia', 'India'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa9685",
   "metadata": {},
   "source": [
    "# Initialize tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9956dca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307201bd07354287b10570c21d1dead2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6590a4969741efb835275ece0fcdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7e85113471460d87cb7e1bcbf4ef58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67900d951e247beaa5a1d5c01c1d000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=1, #Binary Classification\n",
    "    dropout=0.1,\n",
    "    attention_dropout=0.1,\n",
    "    output_hidden_states=False,\n",
    "    output_attentions=False\n",
    ")\n",
    "\n",
    "def regular_encode(texts, tokenizer, maxlen=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Function to encode the word\n",
    "    \"\"\"\n",
    "    # encode the word to vector of integer\n",
    "    enc_di = tokenizer.encode_plus(\n",
    "        texts, \n",
    "        return_attention_mask=True, \n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=maxlen)\n",
    "    \n",
    "    return np.array(enc_di['input_ids']), np.array(enc_di['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f021c",
   "metadata": {},
   "source": [
    "## Initialize Model getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341d0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(lr = 3e-5):\n",
    "    bert_model = TFAutoModelForSequenceClassification.from_pretrained('roberta-base', trainable=True, config=config)\n",
    "    input_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='input_ids', dtype='int32')\n",
    "    input_masks_ids_in = tf.keras.layers.Input(shape=(MAX_LEN,), name='attention_mask', dtype='int32')\n",
    "    output_layer = bert_model(input_ids_in, input_masks_ids_in)[0]\n",
    "    output_layer = tf.keras.layers.Activation(activation='sigmoid')(output_layer)\n",
    "    model = tf.keras.Model(inputs=[input_ids_in, input_masks_ids_in], outputs = output_layer)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.5),\n",
    "               tf.keras.metrics.Precision(name='precision', thresholds=0.5),\n",
    "               tf.keras.metrics.Recall(name='recall', thresholds=0.5),\n",
    "               tf.keras.metrics.TruePositives(name='TP', thresholds=0.5),\n",
    "               tf.keras.metrics.TrueNegatives(name='TN', thresholds=0.5),\n",
    "               tf.keras.metrics.FalsePositives(name='FP', thresholds=0.5),\n",
    "               tf.keras.metrics.FalseNegatives(name='FN', thresholds=0.5)]\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36d1dd",
   "metadata": {},
   "source": [
    "## Load Unshuffled, Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30278282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val [532, 468]\n",
      "Val [1000, 1000]\n",
      "Train [240, 760]\n",
      "Train [481, 1519]\n",
      "Train [749, 2251]\n",
      "Train [1028, 2972]\n",
      "Train [1268, 3732]\n",
      "Train [1522, 4478]\n",
      "Train [2000, 5000]\n",
      "Train [3000, 5000]\n",
      "Train [4000, 5000]\n",
      "Train [5000, 5000]\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "train_count = 0  \n",
    "val_count = 0  \n",
    "train_label_counter = [0, 0]\n",
    "val_label_counter = [0, 0]\n",
    "\n",
    "train_label_count_max = 5e3 \n",
    "val_label_count_max = 1e3\n",
    "total_train = 2 * train_label_count_max\n",
    "total_val = 2 * val_label_count_max\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for line in fs.open('s3://compressed-data-sample/processed_train.json'):\n",
    "    if train_count >= total_train and val_count >= total_val:\n",
    "        break\n",
    "    json_file = json.loads(line)\n",
    "    country = json_file['country']\n",
    "    label =  int(json_file['country'] in PEACE_COUNTRY)\n",
    "    \n",
    "    if not country in MAJOR_COUNTRY:\n",
    "        if train_label_counter[label] < train_label_count_max :\n",
    "            sent = json_file['content_cleaned']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_train.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_train.append(json_file['country'])\n",
    "            train_count += 1\n",
    "            train_label_counter[label] += 1\n",
    "            if sum(train_label_counter) % 1e3 == 0:\n",
    "                print('Train', train_label_counter)\n",
    "    else:\n",
    "        if val_label_counter[label] < val_label_count_max :\n",
    "            sent = json_file['content_cleaned']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_val.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_val.append(json_file['country'])\n",
    "            val_count += 1\n",
    "            val_label_counter[label] += 1\n",
    "            if sum(val_label_counter) % 1e3 == 0:\n",
    "                print('Val', val_label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965e83ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import statistics\n",
    "\n",
    "median = statistics.median(list(Counter(y_train).values()))\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7719c6",
   "metadata": {},
   "source": [
    "## Prepare K-Fold for minority country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0016dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bfdfaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "## Split minority samples to 10 folds\n",
    "# skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "# minority_index = [val_split for _, val_split in skf.split(X_train,y_train)]\n",
    "\n",
    "## draw meidan * 10 number of samples from each majoriy\n",
    "australia_idx = np.where(np.array(y_val) == 'Australia')[0]\n",
    "india_idx = np.where(np.array(y_val) == 'India')[0]\n",
    "majority_index = []\n",
    "for i in range(10):\n",
    "    australia_sample = np.array(random.sample(list(australia_idx), median))\n",
    "    india_sample = np.array(random.sample(list(india_idx), median))\n",
    "    majority_index.append(np.hstack([australia_sample, india_sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fdb4707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0e423ec25a44b1bc34d321f5e90cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/627M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 208s 785ms/step - loss: 0.2252 - accuracy: 0.9074 - precision: 0.8889 - recall: 0.9318 - TP: 3951.0000 - TN: 3725.0000 - FP: 494.0000 - FN: 289.0000\n",
      "67/67 [==============================] - 17s 260ms/step - loss: 0.1000 - accuracy: 0.9641 - precision: 0.9533 - recall: 0.9752 - TP: 1021.0000 - TN: 1018.0000 - FP: 50.0000 - FN: 26.0000\n",
      "[0.10004357993602753, 0.964066207408905, 0.9533146619796753, 0.9751671552658081, 1021.0, 1018.0, 50.0, 26.0, 0.9641170984165603, 0.975095785440613, 0.9531835205992509, 0.9640151515151515]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 225s 850ms/step - loss: 0.2332 - accuracy: 0.9025 - precision: 0.9021 - recall: 0.9036 - TP: 3833.0000 - TN: 3801.0000 - FP: 416.0000 - FN: 409.0000\n",
      "67/67 [==============================] - 18s 269ms/step - loss: 0.1254 - accuracy: 0.9504 - precision: 0.9738 - recall: 0.9244 - TP: 966.0000 - TN: 1044.0000 - FP: 26.0000 - FN: 79.0000\n",
      "[0.1253553181886673, 0.9503546357154846, 0.9737903475761414, 0.9244019389152527, 966.0, 1044.0, 26.0, 79.0, 0.9484536332830832, 0.9296527159394479, 0.9757009345794393, 0.9521203830369358]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 229s 865ms/step - loss: 0.2263 - accuracy: 0.9006 - precision: 0.9074 - recall: 0.8889 - TP: 3704.0000 - TN: 3914.0000 - FP: 378.0000 - FN: 463.0000\n",
      "67/67 [==============================] - 18s 273ms/step - loss: 0.1508 - accuracy: 0.9461 - precision: 0.9800 - recall: 0.9170 - TP: 1027.0000 - TN: 974.0000 - FP: 21.0000 - FN: 93.0000\n",
      "[0.1507667601108551, 0.9460992813110352, 0.9799618124961853, 0.9169642925262451, 1027.0, 974.0, 21.0, 93.0, 0.9474169686622305, 0.9128397375820057, 0.978894472361809, 0.9447138700290979]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 229s 865ms/step - loss: 0.2038 - accuracy: 0.9119 - precision: 0.9091 - recall: 0.9158 - TP: 3882.0000 - TN: 3832.0000 - FP: 388.0000 - FN: 357.0000\n",
      "67/67 [==============================] - 18s 273ms/step - loss: 0.1254 - accuracy: 0.9494 - precision: 0.9672 - recall: 0.9294 - TP: 974.0000 - TN: 1034.0000 - FP: 33.0000 - FN: 74.0000\n",
      "[0.12536859512329102, 0.949409008026123, 0.9672293663024902, 0.9293892979621887, 974.0, 1034.0, 33.0, 74.0, 0.9479318522522303, 0.9332129963898917, 0.9690721649484536, 0.9508045977011494]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 229s 863ms/step - loss: 0.2083 - accuracy: 0.9149 - precision: 0.9011 - recall: 0.9326 - TP: 3956.0000 - TN: 3783.0000 - FP: 434.0000 - FN: 286.0000\n",
      "67/67 [==============================] - 18s 269ms/step - loss: 0.1364 - accuracy: 0.9461 - precision: 0.9463 - recall: 0.9445 - TP: 987.0000 - TN: 1014.0000 - FP: 56.0000 - FN: 58.0000\n",
      "[0.1363515704870224, 0.9460992813110352, 0.9463087320327759, 0.9444975852966309, 987.0, 1014.0, 56.0, 58.0, 0.9454022912430987, 0.9458955223880597, 0.9476635514018692, 0.9467787114845938]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 231s 872ms/step - loss: 0.2180 - accuracy: 0.9102 - precision: 0.8917 - recall: 0.9356 - TP: 3992.0000 - TN: 3707.0000 - FP: 485.0000 - FN: 275.0000\n",
      "67/67 [==============================] - 18s 273ms/step - loss: 0.1224 - accuracy: 0.9560 - precision: 0.9715 - recall: 0.9363 - TP: 955.0000 - TN: 1067.0000 - FP: 28.0000 - FN: 65.0000\n",
      "[0.12244981527328491, 0.9560283422470093, 0.9715157747268677, 0.936274528503418, 955.0, 1067.0, 28.0, 65.0, 0.9535696584429428, 0.9425795053003534, 0.9744292237442922, 0.9582397844634036]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 231s 872ms/step - loss: 0.2247 - accuracy: 0.9091 - precision: 0.9034 - recall: 0.9172 - TP: 3901.0000 - TN: 3789.0000 - FP: 417.0000 - FN: 352.0000\n",
      "67/67 [==============================] - 18s 272ms/step - loss: 0.1282 - accuracy: 0.9546 - precision: 0.9580 - recall: 0.9487 - TP: 981.0000 - TN: 1038.0000 - FP: 43.0000 - FN: 53.0000\n",
      "[0.12823329865932465, 0.9546099305152893, 0.9580078125, 0.9487427473068237, 981.0, 1038.0, 43.0, 53.0, 0.953352770028538, 0.9514207149404217, 0.9602220166512488, 0.9558011049723758]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 227s 858ms/step - loss: 0.2132 - accuracy: 0.9094 - precision: 0.9079 - recall: 0.9117 - TP: 3862.0000 - TN: 3831.0000 - FP: 392.0000 - FN: 374.0000\n",
      "67/67 [==============================] - 18s 270ms/step - loss: 0.1039 - accuracy: 0.9593 - precision: 0.9414 - recall: 0.9791 - TP: 1029.0000 - TN: 1000.0000 - FP: 64.0000 - FN: 22.0000\n",
      "[0.10388487577438354, 0.9593380689620972, 0.9414455890655518, 0.9790675640106201, 1029.0, 1000.0, 64.0, 22.0, 0.959888077890602, 0.9784735812133072, 0.9398496240601504, 0.9587727708533077]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 227s 858ms/step - loss: 0.2083 - accuracy: 0.9161 - precision: 0.9152 - recall: 0.9177 - TP: 3894.0000 - TN: 3855.0000 - FP: 361.0000 - FN: 349.0000\n",
      "67/67 [==============================] - 18s 272ms/step - loss: 0.1140 - accuracy: 0.9579 - precision: 0.9622 - recall: 0.9521 - TP: 994.0000 - TN: 1032.0000 - FP: 39.0000 - FN: 50.0000\n",
      "[0.11397368460893631, 0.9579195976257324, 0.9622458815574646, 0.9521072506904602, 994.0, 1032.0, 39.0, 50.0, 0.9571497184556493, 0.9537892791127541, 0.9635854341736695, 0.9586623316302834]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 227s 856ms/step - loss: 0.2200 - accuracy: 0.9076 - precision: 0.8885 - recall: 0.9325 - TP: 3952.0000 - TN: 3725.0000 - FP: 496.0000 - FN: 286.0000\n",
      "67/67 [==============================] - 18s 268ms/step - loss: 0.1328 - accuracy: 0.9513 - precision: 0.9665 - recall: 0.9342 - TP: 980.0000 - TN: 1032.0000 - FP: 34.0000 - FN: 69.0000\n",
      "[0.1328258514404297, 0.9513002634048462, 0.966469407081604, 0.9342230558395386, 980.0, 1032.0, 34.0, 69.0, 0.95007269242448, 0.9373297002724795, 0.9681050656660413, 0.9524688509460083]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision_neg</th>\n",
       "      <th>recall_neg</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.123925</td>\n",
       "      <td>0.953522</td>\n",
       "      <td>0.962029</td>\n",
       "      <td>0.944084</td>\n",
       "      <td>991.400000</td>\n",
       "      <td>1025.300000</td>\n",
       "      <td>39.400000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>0.952735</td>\n",
       "      <td>0.946029</td>\n",
       "      <td>0.963071</td>\n",
       "      <td>0.954238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.015092</td>\n",
       "      <td>0.005906</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>26.022213</td>\n",
       "      <td>25.534508</td>\n",
       "      <td>13.841965</td>\n",
       "      <td>22.343033</td>\n",
       "      <td>0.006072</td>\n",
       "      <td>0.020023</td>\n",
       "      <td>0.012837</td>\n",
       "      <td>0.005962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.100044</td>\n",
       "      <td>0.946099</td>\n",
       "      <td>0.941446</td>\n",
       "      <td>0.916964</td>\n",
       "      <td>955.000000</td>\n",
       "      <td>974.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.945402</td>\n",
       "      <td>0.912840</td>\n",
       "      <td>0.939850</td>\n",
       "      <td>0.944714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.116093</td>\n",
       "      <td>0.949645</td>\n",
       "      <td>0.954488</td>\n",
       "      <td>0.930598</td>\n",
       "      <td>975.500000</td>\n",
       "      <td>1015.000000</td>\n",
       "      <td>29.250000</td>\n",
       "      <td>50.750000</td>\n",
       "      <td>0.948062</td>\n",
       "      <td>0.934242</td>\n",
       "      <td>0.954943</td>\n",
       "      <td>0.951134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.125362</td>\n",
       "      <td>0.952955</td>\n",
       "      <td>0.964358</td>\n",
       "      <td>0.940386</td>\n",
       "      <td>984.000000</td>\n",
       "      <td>1032.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>0.951713</td>\n",
       "      <td>0.944238</td>\n",
       "      <td>0.965845</td>\n",
       "      <td>0.954135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.131678</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.970444</td>\n",
       "      <td>0.951266</td>\n",
       "      <td>1014.250000</td>\n",
       "      <td>1037.000000</td>\n",
       "      <td>48.250000</td>\n",
       "      <td>72.750000</td>\n",
       "      <td>0.956255</td>\n",
       "      <td>0.953197</td>\n",
       "      <td>0.973090</td>\n",
       "      <td>0.958557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.150767</td>\n",
       "      <td>0.964066</td>\n",
       "      <td>0.979962</td>\n",
       "      <td>0.979068</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1067.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.964117</td>\n",
       "      <td>0.978474</td>\n",
       "      <td>0.978894</td>\n",
       "      <td>0.964015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_lower</th>\n",
       "      <td>0.113291</td>\n",
       "      <td>0.949361</td>\n",
       "      <td>0.953369</td>\n",
       "      <td>0.929656</td>\n",
       "      <td>973.064765</td>\n",
       "      <td>1007.308402</td>\n",
       "      <td>29.646960</td>\n",
       "      <td>43.157113</td>\n",
       "      <td>0.948457</td>\n",
       "      <td>0.931921</td>\n",
       "      <td>0.954026</td>\n",
       "      <td>0.950037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_upper</th>\n",
       "      <td>0.134559</td>\n",
       "      <td>0.957684</td>\n",
       "      <td>0.970689</td>\n",
       "      <td>0.958511</td>\n",
       "      <td>1009.735235</td>\n",
       "      <td>1043.291598</td>\n",
       "      <td>49.153040</td>\n",
       "      <td>74.642887</td>\n",
       "      <td>0.957014</td>\n",
       "      <td>0.960137</td>\n",
       "      <td>0.972115</td>\n",
       "      <td>0.958439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss   accuracy  precision     recall           TP  \\\n",
       "count     10.000000  10.000000  10.000000  10.000000    10.000000   \n",
       "mean       0.123925   0.953522   0.962029   0.944084   991.400000   \n",
       "std        0.015092   0.005906   0.012291   0.020476    26.022213   \n",
       "min        0.100044   0.946099   0.941446   0.916964   955.000000   \n",
       "25%        0.116093   0.949645   0.954488   0.930598   975.500000   \n",
       "50%        0.125362   0.952955   0.964358   0.940386   984.000000   \n",
       "75%        0.131678   0.957447   0.970444   0.951266  1014.250000   \n",
       "max        0.150767   0.964066   0.979962   0.979068  1029.000000   \n",
       "ci_lower   0.113291   0.949361   0.953369   0.929656   973.064765   \n",
       "ci_upper   0.134559   0.957684   0.970689   0.958511  1009.735235   \n",
       "\n",
       "                   TN         FP         FN         f1  precision_neg  \\\n",
       "count       10.000000  10.000000  10.000000  10.000000      10.000000   \n",
       "mean      1025.300000  39.400000  58.900000   0.952735       0.946029   \n",
       "std         25.534508  13.841965  22.343033   0.006072       0.020023   \n",
       "min        974.000000  21.000000  22.000000   0.945402       0.912840   \n",
       "25%       1015.000000  29.250000  50.750000   0.948062       0.934242   \n",
       "50%       1032.000000  36.500000  61.500000   0.951713       0.944238   \n",
       "75%       1037.000000  48.250000  72.750000   0.956255       0.953197   \n",
       "max       1067.000000  64.000000  93.000000   0.964117       0.978474   \n",
       "ci_lower  1007.308402  29.646960  43.157113   0.948457       0.931921   \n",
       "ci_upper  1043.291598  49.153040  74.642887   0.957014       0.960137   \n",
       "\n",
       "          recall_neg     f1_neg  \n",
       "count      10.000000  10.000000  \n",
       "mean        0.963071   0.954238  \n",
       "std         0.012837   0.005962  \n",
       "min         0.939850   0.944714  \n",
       "25%         0.954943   0.951134  \n",
       "50%         0.965845   0.954135  \n",
       "75%         0.973090   0.958557  \n",
       "max         0.978894   0.964015  \n",
       "ci_lower    0.954026   0.950037  \n",
       "ci_upper    0.972115   0.958439  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = []\n",
    "metrics_names = None\n",
    "    \n",
    "for maj_index in majority_index:\n",
    "    tf.keras.backend.clear_session()\n",
    "    X_all_sample = np.hstack([X_train, X_val[maj_index]])\n",
    "    y_all_sample = np.hstack([y_train, y_val[maj_index]])\n",
    "    y_all_sample = np.array(list(map(lambda x: int(x in PEACE_COUNTRY), y_all_sample)))\n",
    "    \n",
    "    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(X_all_sample, y_all_sample, test_size=0.2)\n",
    "    \n",
    "    \n",
    "    model = get_model()\n",
    "        \n",
    "    train_input1 = np.vstack([x['input_ids'] for x in X_train_fold])\n",
    "    train_input2 = np.vstack([x['attention_mask'] for x in X_train_fold])\n",
    "    model.fit(x=[train_input1, train_input2], \n",
    "              y=np.asarray(y_train_fold),\n",
    "              epochs = 1, \n",
    "              batch_size = 32,\n",
    "              class_weight={0: 1., 1: 1.})\n",
    "\n",
    "    eval_input1 = np.vstack([x['input_ids'] for x in X_val_fold])\n",
    "    eval_input2 = np.vstack([x['attention_mask'] for x in X_val_fold])\n",
    "    er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                        y=np.asarray(y_val_fold), return_dict=True)\n",
    "    f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "\n",
    "    precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "    recall_neg = er['TN'] / (er['TN']+er['FP'])\n",
    "    f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "\n",
    "    er = list(er.values())\n",
    "    er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "    eval_results.append(er)\n",
    "    metrics_names = model.metrics_names\n",
    "    print(er)\n",
    "\n",
    "    \n",
    "eval_results = np.array(eval_results)\n",
    "metrics_names += ['f1', 'precision_neg', 'recall_neg', 'f1_neg']\n",
    "eval_results = pd.DataFrame(eval_results, columns=metrics_names)\n",
    "ci = eval_results.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "ci.index = ['ci_lower', 'ci_upper']\n",
    "eval_results = pd.concat([eval_results.describe(), ci])\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7ff5e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>F1</th>\n",
       "      <th>precision_neg</th>\n",
       "      <th>recall_neg</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.205580</td>\n",
       "      <td>0.914640</td>\n",
       "      <td>0.907970</td>\n",
       "      <td>0.923080</td>\n",
       "      <td>3599.700000</td>\n",
       "      <td>3525.500000</td>\n",
       "      <td>381.700000</td>\n",
       "      <td>317.700000</td>\n",
       "      <td>0.915363</td>\n",
       "      <td>0.921933</td>\n",
       "      <td>0.906080</td>\n",
       "      <td>0.913830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.038214</td>\n",
       "      <td>0.018037</td>\n",
       "      <td>0.017811</td>\n",
       "      <td>0.023205</td>\n",
       "      <td>909.569746</td>\n",
       "      <td>883.095094</td>\n",
       "      <td>124.501718</td>\n",
       "      <td>117.964260</td>\n",
       "      <td>0.018063</td>\n",
       "      <td>0.022080</td>\n",
       "      <td>0.019687</td>\n",
       "      <td>0.018152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.900600</td>\n",
       "      <td>0.888500</td>\n",
       "      <td>0.888900</td>\n",
       "      <td>1021.000000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.898055</td>\n",
       "      <td>0.894220</td>\n",
       "      <td>0.882492</td>\n",
       "      <td>0.902100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.208300</td>\n",
       "      <td>0.907975</td>\n",
       "      <td>0.901350</td>\n",
       "      <td>0.912725</td>\n",
       "      <td>3840.250000</td>\n",
       "      <td>3739.500000</td>\n",
       "      <td>380.500000</td>\n",
       "      <td>286.000000</td>\n",
       "      <td>0.909839</td>\n",
       "      <td>0.911988</td>\n",
       "      <td>0.898026</td>\n",
       "      <td>0.905509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.909800</td>\n",
       "      <td>0.905400</td>\n",
       "      <td>0.917450</td>\n",
       "      <td>3888.000000</td>\n",
       "      <td>3795.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>350.500000</td>\n",
       "      <td>0.911343</td>\n",
       "      <td>0.915990</td>\n",
       "      <td>0.904263</td>\n",
       "      <td>0.908492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.223525</td>\n",
       "      <td>0.914150</td>\n",
       "      <td>0.908800</td>\n",
       "      <td>0.932575</td>\n",
       "      <td>3939.250000</td>\n",
       "      <td>3831.750000</td>\n",
       "      <td>429.750000</td>\n",
       "      <td>369.750000</td>\n",
       "      <td>0.915617</td>\n",
       "      <td>0.929458</td>\n",
       "      <td>0.910961</td>\n",
       "      <td>0.912681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.964100</td>\n",
       "      <td>0.953300</td>\n",
       "      <td>0.975200</td>\n",
       "      <td>3992.000000</td>\n",
       "      <td>3914.000000</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>0.964126</td>\n",
       "      <td>0.975096</td>\n",
       "      <td>0.953184</td>\n",
       "      <td>0.964015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_lower</th>\n",
       "      <td>0.178654</td>\n",
       "      <td>0.901931</td>\n",
       "      <td>0.895421</td>\n",
       "      <td>0.906730</td>\n",
       "      <td>2958.817729</td>\n",
       "      <td>2903.271752</td>\n",
       "      <td>293.976168</td>\n",
       "      <td>234.582456</td>\n",
       "      <td>0.902636</td>\n",
       "      <td>0.906375</td>\n",
       "      <td>0.892209</td>\n",
       "      <td>0.901040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_upper</th>\n",
       "      <td>0.232506</td>\n",
       "      <td>0.927349</td>\n",
       "      <td>0.920519</td>\n",
       "      <td>0.939430</td>\n",
       "      <td>4240.582271</td>\n",
       "      <td>4147.728248</td>\n",
       "      <td>469.423832</td>\n",
       "      <td>400.817544</td>\n",
       "      <td>0.928090</td>\n",
       "      <td>0.937491</td>\n",
       "      <td>0.919952</td>\n",
       "      <td>0.926620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss   accuracy  precision     recall           TP  \\\n",
       "count     10.000000  10.000000  10.000000  10.000000    10.000000   \n",
       "mean       0.205580   0.914640   0.907970   0.923080  3599.700000   \n",
       "std        0.038214   0.018037   0.017811   0.023205   909.569746   \n",
       "min        0.100000   0.900600   0.888500   0.888900  1021.000000   \n",
       "25%        0.208300   0.907975   0.901350   0.912725  3840.250000   \n",
       "50%        0.215600   0.909800   0.905400   0.917450  3888.000000   \n",
       "75%        0.223525   0.914150   0.908800   0.932575  3939.250000   \n",
       "max        0.233200   0.964100   0.953300   0.975200  3992.000000   \n",
       "ci_lower   0.178654   0.901931   0.895421   0.906730  2958.817729   \n",
       "ci_upper   0.232506   0.927349   0.920519   0.939430  4240.582271   \n",
       "\n",
       "                   TN          FP          FN         F1  precision_neg  \\\n",
       "count       10.000000   10.000000   10.000000  10.000000      10.000000   \n",
       "mean      3525.500000  381.700000  317.700000   0.915363       0.921933   \n",
       "std        883.095094  124.501718  117.964260   0.018063       0.022080   \n",
       "min       1018.000000   50.000000   26.000000   0.898055       0.894220   \n",
       "25%       3739.500000  380.500000  286.000000   0.909839       0.911988   \n",
       "50%       3795.000000  404.000000  350.500000   0.911343       0.915990   \n",
       "75%       3831.750000  429.750000  369.750000   0.915617       0.929458   \n",
       "max       3914.000000  496.000000  463.000000   0.964126       0.975096   \n",
       "ci_lower  2903.271752  293.976168  234.582456   0.902636       0.906375   \n",
       "ci_upper  4147.728248  469.423832  400.817544   0.928090       0.937491   \n",
       "\n",
       "          recall_neg     f1_neg  \n",
       "count      10.000000  10.000000  \n",
       "mean        0.906080   0.913830  \n",
       "std         0.019687   0.018152  \n",
       "min         0.882492   0.902100  \n",
       "25%         0.898026   0.905509  \n",
       "50%         0.904263   0.908492  \n",
       "75%         0.910961   0.912681  \n",
       "max         0.953184   0.964015  \n",
       "ci_lower    0.892209   0.901040  \n",
       "ci_upper    0.919952   0.926620  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View results on training data\n",
    "eval_results = np.array([\n",
    "    [0.1000, 0.9641, 0.9533, 0.9752, 1021.0000, 1018.0000, 50.0000, 26.0000],\n",
    "    [0.2332, 0.9025, 0.9021, 0.9036, 3833.0000, 3801.0000, 416.0000, 409.0000],\n",
    "    [0.2263, 0.9006, 0.9074, 0.8889, 3704.0000, 3914.0000, 378.0000, 463.0000],\n",
    "    [0.2038, 0.9119, 0.9091, 0.9158, 3882.0000, 3832.0000, 388.0000, 357.0000],\n",
    "    [0.2083, 0.9149, 0.9011, 0.9326, 3956.0000, 3783.0000, 434.0000, 286.0000],\n",
    "    [0.2180, 0.9102, 0.8917, 0.9356, 3992.0000, 3707.0000, 485.0000, 275.0000],\n",
    "    [0.2247, 0.9091, 0.9034, 0.9172, 3901.0000, 3789.0000, 417.0000, 352.0000],\n",
    "    [0.2132, 0.9094, 0.9079, 0.9117, 3862.0000, 3831.0000, 392.0000, 374.0000],\n",
    "    [0.2083, 0.9161, 0.9152, 0.9177, 3894.0000, 3855.0000, 361.0000, 349.0000],\n",
    "    [0.2200, 0.9076, 0.8885, 0.9325, 3952.0000, 3725.0000, 496.0000, 286.0000]\n",
    "])\n",
    "eval_results = pd.DataFrame(eval_results, columns=metrics_names[:8])\n",
    "eval_results['F1'] =  2*eval_results['precision']*eval_results['recall'] / (eval_results['precision']+eval_results['recall'])\n",
    "eval_results['precision_neg'] = eval_results['TN'] / (eval_results['TN'] + eval_results['FN'])\n",
    "eval_results['recall_neg'] = eval_results['TN'] / (eval_results['TN']+eval_results['FP'])\n",
    "eval_results['f1_neg'] = 2 * eval_results['precision_neg'] * eval_results['recall_neg'] / (eval_results['precision_neg'] + eval_results['recall_neg'])\n",
    "\n",
    "ci = eval_results.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "ci.index = ['ci_lower', 'ci_upper']\n",
    "eval_results = pd.concat([eval_results.describe(), ci])\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392c29c",
   "metadata": {},
   "source": [
    "## Load Shuffled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7556fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val [532, 468]\n",
      "Val [1000, 1000]\n",
      "Train [240, 760]\n",
      "Train [481, 1519]\n",
      "Train [749, 2251]\n",
      "Train [1028, 2972]\n",
      "Train [1268, 3732]\n",
      "Train [1522, 4478]\n",
      "Train [2000, 5000]\n",
      "Train [3000, 5000]\n",
      "Train [4000, 5000]\n",
      "Train [5000, 5000]\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "train_count = 0  \n",
    "val_count = 0  \n",
    "train_label_counter = [0, 0]\n",
    "val_label_counter = [0, 0]\n",
    "\n",
    "train_label_count_max = 5e3 \n",
    "val_label_count_max = 1e3\n",
    "total_train = 2 * train_label_count_max\n",
    "total_val = 2 * val_label_count_max\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for line in fs.open('s3://compressed-data-sample/shuffled_train.json'):\n",
    "    if train_count >= total_train and val_count >= total_val:\n",
    "        break\n",
    "    json_file = json.loads(line)\n",
    "    country = json_file['country']\n",
    "    label =  int(json_file['country'] in PEACE_COUNTRY)\n",
    "    \n",
    "    if not country in MAJOR_COUNTRY:\n",
    "        if train_label_counter[label] < train_label_count_max :\n",
    "            sent = json_file['content_cleaned_shuffled']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_train.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_train.append(json_file['country'])\n",
    "            train_count += 1\n",
    "            train_label_counter[label] += 1\n",
    "            if sum(train_label_counter) % 1e3 == 0:\n",
    "                print('Train', train_label_counter)\n",
    "    else:\n",
    "        if val_label_counter[label] < val_label_count_max :\n",
    "            sent = json_file['content_cleaned_shuffled']\n",
    "            ids, msk = regular_encode(sent, tokenizer) # tokenize content_cleaned\n",
    "            \n",
    "            X_val.append({'input_ids': ids,'attention_mask':msk})\n",
    "            y_val.append(json_file['country'])\n",
    "            val_count += 1\n",
    "            val_label_counter[label] += 1\n",
    "            if sum(val_label_counter) % 1e3 == 0:\n",
    "                print('Val', val_label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffb73d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median = statistics.median(list(Counter(y_train).values()))\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef44f1",
   "metadata": {},
   "source": [
    "## Prepare K-Fold for minority country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b99b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010c9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "australia_idx = np.where(np.array(y_val) == 'Australia')[0]\n",
    "india_idx = np.where(np.array(y_val) == 'India')[0]\n",
    "majority_index = []\n",
    "for i in range(10):\n",
    "    australia_sample = np.array(random.sample(list(australia_idx), median))\n",
    "    india_sample = np.array(random.sample(list(india_idx), median))\n",
    "    majority_index.append(np.hstack([australia_sample, india_sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "890ec82d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 227s 856ms/step - loss: 0.3107 - accuracy: 0.8656 - precision: 0.8514 - recall: 0.8873 - TP: 3770.0000 - TN: 3552.0000 - FP: 658.0000 - FN: 479.0000\n",
      "67/67 [==============================] - 18s 275ms/step - loss: 0.1897 - accuracy: 0.9239 - precision: 0.9064 - recall: 0.9422 - TP: 978.0000 - TN: 976.0000 - FP: 101.0000 - FN: 60.0000\n",
      "[0.18971237540245056, 0.9238770604133606, 0.9063948392868042, 0.9421965479850769, 978.0, 976.0, 101.0, 60.0, 0.9239490074091894, 0.9420849420849421, 0.9062209842154132, 0.9238050165641268]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 230s 870ms/step - loss: 0.3373 - accuracy: 0.8497 - precision: 0.8198 - recall: 0.8974 - TP: 3804.0000 - TN: 3384.0000 - FP: 836.0000 - FN: 435.0000\n",
      "67/67 [==============================] - 18s 270ms/step - loss: 0.1969 - accuracy: 0.9196 - precision: 0.9181 - recall: 0.9198 - TP: 964.0000 - TN: 981.0000 - FP: 86.0000 - FN: 84.0000\n",
      "[0.19686239957809448, 0.9196217656135559, 0.9180952310562134, 0.919847309589386, 964.0, 981.0, 86.0, 84.0, 0.9189704352098627, 0.9211267605633803, 0.9194001874414246, 0.9202626641651032]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 231s 871ms/step - loss: 0.3271 - accuracy: 0.8557 - precision: 0.8429 - recall: 0.8764 - TP: 3728.0000 - TN: 3510.0000 - FP: 695.0000 - FN: 526.0000\n",
      "67/67 [==============================] - 18s 269ms/step - loss: 0.2425 - accuracy: 0.9073 - precision: 0.9505 - recall: 0.8548 - TP: 883.0000 - TN: 1036.0000 - FP: 46.0000 - FN: 150.0000\n",
      "[0.24253001809120178, 0.9073286056518555, 0.9504843950271606, 0.8547918796539307, 883.0, 1036.0, 46.0, 150.0, 0.9001019445076582, 0.8735244519392917, 0.9574861367837338, 0.9135802469135804]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 233s 878ms/step - loss: 0.3015 - accuracy: 0.8670 - precision: 0.8547 - recall: 0.8881 - TP: 3801.0000 - TN: 3533.0000 - FP: 646.0000 - FN: 479.0000\n",
      "67/67 [==============================] - 18s 272ms/step - loss: 0.2087 - accuracy: 0.9125 - precision: 0.9127 - recall: 0.9027 - TP: 909.0000 - TN: 1021.0000 - FP: 87.0000 - FN: 98.0000\n",
      "[0.20869390666484833, 0.9125295281410217, 0.9126505851745605, 0.9026812314987183, 909.0, 1021.0, 87.0, 98.0, 0.9076385337234141, 0.9124218051831993, 0.9214801444043321, 0.9169286035024697]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 232s 874ms/step - loss: 0.3183 - accuracy: 0.8635 - precision: 0.8488 - recall: 0.8844 - TP: 3740.0000 - TN: 3564.0000 - FP: 666.0000 - FN: 489.0000\n",
      "67/67 [==============================] - 18s 275ms/step - loss: 0.1959 - accuracy: 0.9243 - precision: 0.9127 - recall: 0.9386 - TP: 993.0000 - TN: 962.0000 - FP: 95.0000 - FN: 65.0000\n",
      "[0.1959303617477417, 0.9243499040603638, 0.9126838445663452, 0.938563346862793, 993.0, 962.0, 95.0, 65.0, 0.9254427045180253, 0.9367088607594937, 0.9101229895931883, 0.9232245681381958]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 232s 874ms/step - loss: 0.3758 - accuracy: 0.8365 - precision: 0.8006 - recall: 0.8940 - TP: 3761.0000 - TN: 3315.0000 - FP: 937.0000 - FN: 446.0000\n",
      "67/67 [==============================] - 18s 274ms/step - loss: 0.2507 - accuracy: 0.8960 - precision: 0.8446 - recall: 0.9759 - TP: 1054.0000 - TN: 841.0000 - FP: 194.0000 - FN: 26.0000\n",
      "[0.25072550773620605, 0.8959810733795166, 0.8445512652397156, 0.9759259223937988, 1054.0, 841.0, 194.0, 26.0, 0.9054982706038119, 0.9700115340253749, 0.81256038647343, 0.8843322818086226]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 231s 871ms/step - loss: 0.3054 - accuracy: 0.8690 - precision: 0.8494 - recall: 0.8972 - TP: 3797.0000 - TN: 3554.0000 - FP: 673.0000 - FN: 435.0000\n",
      "67/67 [==============================] - 18s 270ms/step - loss: 0.1947 - accuracy: 0.9168 - precision: 0.9127 - recall: 0.9213 - TP: 972.0000 - TN: 967.0000 - FP: 93.0000 - FN: 83.0000\n",
      "[0.1947273313999176, 0.9167848825454712, 0.9126760363578796, 0.9213269948959351, 972.0, 967.0, 93.0, 83.0, 0.9169811124208188, 0.920952380952381, 0.9122641509433962, 0.9165876777251185]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 232s 876ms/step - loss: 0.3014 - accuracy: 0.8665 - precision: 0.8553 - recall: 0.8837 - TP: 3754.0000 - TN: 3576.0000 - FP: 635.0000 - FN: 494.0000\n",
      "67/67 [==============================] - 18s 275ms/step - loss: 0.2044 - accuracy: 0.9210 - precision: 0.9495 - recall: 0.8864 - TP: 921.0000 - TN: 1027.0000 - FP: 49.0000 - FN: 118.0000\n",
      "[0.20444567501544952, 0.9210401773452759, 0.9494845271110535, 0.8864292502403259, 921.0, 1027.0, 49.0, 118.0, 0.9168740578831184, 0.896943231441048, 0.9544609665427509, 0.9248086447546151]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 230s 868ms/step - loss: 0.3171 - accuracy: 0.8612 - precision: 0.8426 - recall: 0.8877 - TP: 3746.0000 - TN: 3539.0000 - FP: 700.0000 - FN: 474.0000\n",
      "67/67 [==============================] - 18s 269ms/step - loss: 0.2075 - accuracy: 0.9083 - precision: 0.9405 - recall: 0.8735 - TP: 932.0000 - TN: 989.0000 - FP: 59.0000 - FN: 135.0000\n",
      "[0.20745012164115906, 0.908274233341217, 0.940464198589325, 0.873477041721344, 932.0, 989.0, 59.0, 135.0, 0.905733733566746, 0.8798932384341637, 0.9437022900763359, 0.9106813996316759]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 230s 867ms/step - loss: 0.3019 - accuracy: 0.8645 - precision: 0.8369 - recall: 0.9051 - TP: 3824.0000 - TN: 3489.0000 - FP: 745.0000 - FN: 401.0000\n",
      "67/67 [==============================] - 18s 270ms/step - loss: 0.2294 - accuracy: 0.9149 - precision: 0.8882 - recall: 0.9501 - TP: 1009.0000 - TN: 926.0000 - FP: 127.0000 - FN: 53.0000\n",
      "[0.2293669879436493, 0.914893627166748, 0.8882042169570923, 0.9500941634178162, 1009.0, 926.0, 127.0, 53.0, 0.9181073665330891, 0.9458631256384066, 0.879392212725546, 0.9114173228346456]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision_neg</th>\n",
       "      <th>recall_neg</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.212044</td>\n",
       "      <td>0.914468</td>\n",
       "      <td>0.913569</td>\n",
       "      <td>0.916533</td>\n",
       "      <td>961.500000</td>\n",
       "      <td>972.600000</td>\n",
       "      <td>93.700000</td>\n",
       "      <td>87.200000</td>\n",
       "      <td>0.913930</td>\n",
       "      <td>0.919953</td>\n",
       "      <td>0.911709</td>\n",
       "      <td>0.914563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.021355</td>\n",
       "      <td>0.008796</td>\n",
       "      <td>0.031371</td>\n",
       "      <td>0.037410</td>\n",
       "      <td>51.232043</td>\n",
       "      <td>57.063707</td>\n",
       "      <td>43.212781</td>\n",
       "      <td>38.686202</td>\n",
       "      <td>0.008582</td>\n",
       "      <td>0.030362</td>\n",
       "      <td>0.042176</td>\n",
       "      <td>0.011773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.189712</td>\n",
       "      <td>0.895981</td>\n",
       "      <td>0.844551</td>\n",
       "      <td>0.854792</td>\n",
       "      <td>883.000000</td>\n",
       "      <td>841.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.900102</td>\n",
       "      <td>0.873524</td>\n",
       "      <td>0.812560</td>\n",
       "      <td>0.884332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.196163</td>\n",
       "      <td>0.909338</td>\n",
       "      <td>0.907959</td>\n",
       "      <td>0.890492</td>\n",
       "      <td>923.750000</td>\n",
       "      <td>963.250000</td>\n",
       "      <td>65.750000</td>\n",
       "      <td>61.250000</td>\n",
       "      <td>0.906210</td>\n",
       "      <td>0.900813</td>\n",
       "      <td>0.907196</td>\n",
       "      <td>0.911958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.205948</td>\n",
       "      <td>0.915839</td>\n",
       "      <td>0.912680</td>\n",
       "      <td>0.920587</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>978.500000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>83.500000</td>\n",
       "      <td>0.916928</td>\n",
       "      <td>0.921040</td>\n",
       "      <td>0.915832</td>\n",
       "      <td>0.916758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.224199</td>\n",
       "      <td>0.920686</td>\n",
       "      <td>0.934872</td>\n",
       "      <td>0.941288</td>\n",
       "      <td>989.250000</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>99.500000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>0.918755</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.938147</td>\n",
       "      <td>0.922484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.250726</td>\n",
       "      <td>0.924350</td>\n",
       "      <td>0.950484</td>\n",
       "      <td>0.975926</td>\n",
       "      <td>1054.000000</td>\n",
       "      <td>1036.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.925443</td>\n",
       "      <td>0.970012</td>\n",
       "      <td>0.957486</td>\n",
       "      <td>0.924809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_lower</th>\n",
       "      <td>0.196997</td>\n",
       "      <td>0.908270</td>\n",
       "      <td>0.891465</td>\n",
       "      <td>0.890174</td>\n",
       "      <td>925.401935</td>\n",
       "      <td>932.392948</td>\n",
       "      <td>63.252302</td>\n",
       "      <td>59.941726</td>\n",
       "      <td>0.907883</td>\n",
       "      <td>0.898560</td>\n",
       "      <td>0.881992</td>\n",
       "      <td>0.906268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_upper</th>\n",
       "      <td>0.227092</td>\n",
       "      <td>0.920666</td>\n",
       "      <td>0.935673</td>\n",
       "      <td>0.942893</td>\n",
       "      <td>997.598065</td>\n",
       "      <td>1012.807052</td>\n",
       "      <td>124.147698</td>\n",
       "      <td>114.458274</td>\n",
       "      <td>0.919976</td>\n",
       "      <td>0.941346</td>\n",
       "      <td>0.941426</td>\n",
       "      <td>0.922858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss   accuracy  precision     recall           TP  \\\n",
       "count     10.000000  10.000000  10.000000  10.000000    10.000000   \n",
       "mean       0.212044   0.914468   0.913569   0.916533   961.500000   \n",
       "std        0.021355   0.008796   0.031371   0.037410    51.232043   \n",
       "min        0.189712   0.895981   0.844551   0.854792   883.000000   \n",
       "25%        0.196163   0.909338   0.907959   0.890492   923.750000   \n",
       "50%        0.205948   0.915839   0.912680   0.920587   968.000000   \n",
       "75%        0.224199   0.920686   0.934872   0.941288   989.250000   \n",
       "max        0.250726   0.924350   0.950484   0.975926  1054.000000   \n",
       "ci_lower   0.196997   0.908270   0.891465   0.890174   925.401935   \n",
       "ci_upper   0.227092   0.920666   0.935673   0.942893   997.598065   \n",
       "\n",
       "                   TN          FP          FN         f1  precision_neg  \\\n",
       "count       10.000000   10.000000   10.000000  10.000000      10.000000   \n",
       "mean       972.600000   93.700000   87.200000   0.913930       0.919953   \n",
       "std         57.063707   43.212781   38.686202   0.008582       0.030362   \n",
       "min        841.000000   46.000000   26.000000   0.900102       0.873524   \n",
       "25%        963.250000   65.750000   61.250000   0.906210       0.900813   \n",
       "50%        978.500000   90.000000   83.500000   0.916928       0.921040   \n",
       "75%       1013.000000   99.500000  113.000000   0.918755       0.940741   \n",
       "max       1036.000000  194.000000  150.000000   0.925443       0.970012   \n",
       "ci_lower   932.392948   63.252302   59.941726   0.907883       0.898560   \n",
       "ci_upper  1012.807052  124.147698  114.458274   0.919976       0.941346   \n",
       "\n",
       "          recall_neg     f1_neg  \n",
       "count      10.000000  10.000000  \n",
       "mean        0.911709   0.914563  \n",
       "std         0.042176   0.011773  \n",
       "min         0.812560   0.884332  \n",
       "25%         0.907196   0.911958  \n",
       "50%         0.915832   0.916758  \n",
       "75%         0.938147   0.922484  \n",
       "max         0.957486   0.924809  \n",
       "ci_lower    0.881992   0.906268  \n",
       "ci_upper    0.941426   0.922858  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = []\n",
    "metrics_names = None\n",
    "    \n",
    "for maj_index in majority_index:\n",
    "    tf.keras.backend.clear_session()\n",
    "    X_all_sample = np.hstack([X_train, X_val[maj_index]])\n",
    "    y_all_sample = np.hstack([y_train, y_val[maj_index]])\n",
    "    y_all_sample = np.array(list(map(lambda x: int(x in PEACE_COUNTRY), y_all_sample)))\n",
    "    \n",
    "    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(X_all_sample, y_all_sample, test_size=0.2)\n",
    "    \n",
    "    \n",
    "    model = get_model()\n",
    "        \n",
    "    train_input1 = np.vstack([x['input_ids'] for x in X_train_fold])\n",
    "    train_input2 = np.vstack([x['attention_mask'] for x in X_train_fold])\n",
    "    model.fit(x=[train_input1, train_input2], \n",
    "              y=np.asarray(y_train_fold),\n",
    "              epochs = 1, \n",
    "              batch_size = 32,\n",
    "              class_weight={0: 1., 1: 1.})\n",
    "\n",
    "    eval_input1 = np.vstack([x['input_ids'] for x in X_val_fold])\n",
    "    eval_input2 = np.vstack([x['attention_mask'] for x in X_val_fold])\n",
    "    er = model.evaluate(x=[eval_input1, eval_input2], \n",
    "                        y=np.asarray(y_val_fold), return_dict=True)\n",
    "    f1 = 2*er['precision']*er['recall'] / (er['precision']+er['recall'])\n",
    "\n",
    "    precision_neg = er['TN'] / (er['TN'] + er['FN'])\n",
    "    recall_neg = er['TN'] / (er['TN']+er['FP'])\n",
    "    f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg)\n",
    "\n",
    "    er = list(er.values())\n",
    "    er += [f1, precision_neg, recall_neg, f1_neg]\n",
    "    eval_results.append(er)\n",
    "    metrics_names = model.metrics_names\n",
    "    print(er)\n",
    "\n",
    "    \n",
    "eval_results = np.array(eval_results)\n",
    "metrics_names += ['f1', 'precision_neg', 'recall_neg', 'f1_neg']\n",
    "eval_results = pd.DataFrame(eval_results, columns=metrics_names)\n",
    "ci = eval_results.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "ci.index = ['ci_lower', 'ci_upper']\n",
    "eval_results = pd.concat([eval_results.describe(), ci])\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3bf1b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>F1</th>\n",
       "      <th>precision_neg</th>\n",
       "      <th>recall_neg</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.319650</td>\n",
       "      <td>0.859920</td>\n",
       "      <td>0.840240</td>\n",
       "      <td>0.890130</td>\n",
       "      <td>3772.500000</td>\n",
       "      <td>3501.600000</td>\n",
       "      <td>719.100000</td>\n",
       "      <td>465.800000</td>\n",
       "      <td>0.864333</td>\n",
       "      <td>0.882668</td>\n",
       "      <td>0.829683</td>\n",
       "      <td>0.855186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.023067</td>\n",
       "      <td>0.010079</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>32.104863</td>\n",
       "      <td>85.620870</td>\n",
       "      <td>96.526853</td>\n",
       "      <td>36.303658</td>\n",
       "      <td>0.008552</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.022299</td>\n",
       "      <td>0.011938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.301400</td>\n",
       "      <td>0.836500</td>\n",
       "      <td>0.800600</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>3728.000000</td>\n",
       "      <td>3315.000000</td>\n",
       "      <td>635.000000</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>0.844726</td>\n",
       "      <td>0.869673</td>\n",
       "      <td>0.779633</td>\n",
       "      <td>0.827405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.302775</td>\n",
       "      <td>0.857075</td>\n",
       "      <td>0.838325</td>\n",
       "      <td>0.885125</td>\n",
       "      <td>3748.000000</td>\n",
       "      <td>3494.250000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>437.750000</td>\n",
       "      <td>0.860633</td>\n",
       "      <td>0.879664</td>\n",
       "      <td>0.826713</td>\n",
       "      <td>0.853312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.313900</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.845850</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>3765.500000</td>\n",
       "      <td>3536.000000</td>\n",
       "      <td>684.000000</td>\n",
       "      <td>476.500000</td>\n",
       "      <td>0.867607</td>\n",
       "      <td>0.881293</td>\n",
       "      <td>0.837826</td>\n",
       "      <td>0.859747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.866275</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>0.896400</td>\n",
       "      <td>3800.000000</td>\n",
       "      <td>3553.500000</td>\n",
       "      <td>733.750000</td>\n",
       "      <td>486.500000</td>\n",
       "      <td>0.869566</td>\n",
       "      <td>0.885043</td>\n",
       "      <td>0.843417</td>\n",
       "      <td>0.862498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.375800</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.855300</td>\n",
       "      <td>0.905100</td>\n",
       "      <td>3824.000000</td>\n",
       "      <td>3576.000000</td>\n",
       "      <td>937.000000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>0.872646</td>\n",
       "      <td>0.896915</td>\n",
       "      <td>0.849204</td>\n",
       "      <td>0.865141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_lower</th>\n",
       "      <td>0.303397</td>\n",
       "      <td>0.852818</td>\n",
       "      <td>0.827967</td>\n",
       "      <td>0.884269</td>\n",
       "      <td>3749.878934</td>\n",
       "      <td>3441.271589</td>\n",
       "      <td>651.087240</td>\n",
       "      <td>440.220466</td>\n",
       "      <td>0.858307</td>\n",
       "      <td>0.877479</td>\n",
       "      <td>0.813970</td>\n",
       "      <td>0.846774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci_upper</th>\n",
       "      <td>0.335903</td>\n",
       "      <td>0.867022</td>\n",
       "      <td>0.852513</td>\n",
       "      <td>0.895991</td>\n",
       "      <td>3795.121066</td>\n",
       "      <td>3561.928411</td>\n",
       "      <td>787.112760</td>\n",
       "      <td>491.379534</td>\n",
       "      <td>0.870359</td>\n",
       "      <td>0.887858</td>\n",
       "      <td>0.845395</td>\n",
       "      <td>0.863597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss   accuracy  precision     recall           TP  \\\n",
       "count     10.000000  10.000000  10.000000  10.000000    10.000000   \n",
       "mean       0.319650   0.859920   0.840240   0.890130  3772.500000   \n",
       "std        0.023067   0.010079   0.017418   0.008318    32.104863   \n",
       "min        0.301400   0.836500   0.800600   0.876400  3728.000000   \n",
       "25%        0.302775   0.857075   0.838325   0.885125  3748.000000   \n",
       "50%        0.313900   0.864000   0.845850   0.887900  3765.500000   \n",
       "75%        0.324900   0.866275   0.850900   0.896400  3800.000000   \n",
       "max        0.375800   0.869000   0.855300   0.905100  3824.000000   \n",
       "ci_lower   0.303397   0.852818   0.827967   0.884269  3749.878934   \n",
       "ci_upper   0.335903   0.867022   0.852513   0.895991  3795.121066   \n",
       "\n",
       "                   TN          FP          FN         F1  precision_neg  \\\n",
       "count       10.000000   10.000000   10.000000  10.000000      10.000000   \n",
       "mean      3501.600000  719.100000  465.800000   0.864333       0.882668   \n",
       "std         85.620870   96.526853   36.303658   0.008552       0.007365   \n",
       "min       3315.000000  635.000000  401.000000   0.844726       0.869673   \n",
       "25%       3494.250000  660.000000  437.750000   0.860633       0.879664   \n",
       "50%       3536.000000  684.000000  476.500000   0.867607       0.881293   \n",
       "75%       3553.500000  733.750000  486.500000   0.869566       0.885043   \n",
       "max       3576.000000  937.000000  526.000000   0.872646       0.896915   \n",
       "ci_lower  3441.271589  651.087240  440.220466   0.858307       0.877479   \n",
       "ci_upper  3561.928411  787.112760  491.379534   0.870359       0.887858   \n",
       "\n",
       "          recall_neg     f1_neg  \n",
       "count      10.000000  10.000000  \n",
       "mean        0.829683   0.855186  \n",
       "std         0.022299   0.011938  \n",
       "min         0.779633   0.827405  \n",
       "25%         0.826713   0.853312  \n",
       "50%         0.837826   0.859747  \n",
       "75%         0.843417   0.862498  \n",
       "max         0.849204   0.865141  \n",
       "ci_lower    0.813970   0.846774  \n",
       "ci_upper    0.845395   0.863597  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View results on training data\n",
    "eval_results = np.array([\n",
    "    [0.3107, 0.8656, 0.8514, 0.8873, 3770.0000, 3552.0000, 658.0000, 479.0000],\n",
    "    [0.3373, 0.8497, 0.8198, 0.8974, 3804.0000, 3384.0000, 836.0000, 435.0000],\n",
    "    [0.3271, 0.8557, 0.8429, 0.8764, 3728.0000, 3510.0000, 695.0000, 526.0000],\n",
    "    [0.3015, 0.8670, 0.8547, 0.8881, 3801.0000, 3533.0000, 646.0000, 479.0000],\n",
    "    [0.3183, 0.8635, 0.8488, 0.8844, 3740.0000, 3564.0000, 666.0000, 489.0000],\n",
    "    [0.3758, 0.8365, 0.8006, 0.8940, 3761.0000, 3315.0000, 937.0000, 446.0000],\n",
    "    [0.3054, 0.8690, 0.8494, 0.8972, 3797.0000, 3554.0000, 673.0000, 435.0000],\n",
    "    [0.3014, 0.8665, 0.8553, 0.8837, 3754.0000, 3576.0000, 635.0000, 494.0000],\n",
    "    [0.3171, 0.8612, 0.8426, 0.8877, 3746.0000, 3539.0000, 700.0000, 474.0000],\n",
    "    [0.3019, 0.8645, 0.8369, 0.9051, 3824.0000, 3489.0000, 745.0000, 401.0000]\n",
    "])\n",
    "eval_results = pd.DataFrame(eval_results, columns=metrics_names[:8])\n",
    "eval_results['F1'] =  2*eval_results['precision']*eval_results['recall'] / (eval_results['precision']+eval_results['recall'])\n",
    "eval_results['precision_neg'] = eval_results['TN'] / (eval_results['TN'] + eval_results['FN'])\n",
    "eval_results['recall_neg'] = eval_results['TN'] / (eval_results['TN']+eval_results['FP'])\n",
    "eval_results['f1_neg'] = 2 * eval_results['precision_neg'] * eval_results['recall_neg'] / (eval_results['precision_neg'] + eval_results['recall_neg'])\n",
    "\n",
    "ci = eval_results.apply(lambda x: st.t.interval(0.95, len(x), loc=np.mean(x), scale=st.sem(x)))\n",
    "ci.index = ['ci_lower', 'ci_upper']\n",
    "eval_results = pd.concat([eval_results.describe(), ci])\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a631bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
