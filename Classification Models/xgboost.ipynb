{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a45137a",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b56b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import s3fs\n",
    "import boto3\n",
    "import io\n",
    "import gc\n",
    "import tarfile\n",
    "import time\n",
    "import pickle as pkl\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import sagemaker \n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.xgboost.model import XGBoostModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc88ff5",
   "metadata": {},
   "source": [
    "## Setup Model Parameter and Train the model\n",
    "### Skip those steps to deploy model step if you already have a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adcc44e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpkl\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mxgboost\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mxgb\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36ms3fs\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Hyperparameters are described here\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_round\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_depth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m5\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--eta\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.2\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--gamma\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--min_child_weight\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m6\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--subsample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.7\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--objective\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mbinary:logistic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--eval_metric\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mauc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--verbosity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# Sagemaker specific arguments. Defaults are set in the environment variables.\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33ms3://compressed-data-sample/train_embedding_yfirst.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33ms3://compressed-data-sample/test_embedding_yfirst.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    args = parser.parse_args()\r\n",
      "    \r\n",
      "    train_hp = {\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mmax_depth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.max_depth,\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33meta\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.eta,\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mgamma\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.gamma,\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mmin_child_weight\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.min_child_weight,\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33msubsample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.subsample,\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mverbosity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.verbosity,\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mobjective\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.objective,\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33meval_metric\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.eval_metric\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[37m# Transform to matrix, specify data columns and label columns\u001b[39;49;00m\r\n",
      "    s3 = s3fs.S3FileSystem(anon=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "    train = pd.read_csv(s3.open(args.train, mode=\u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), header=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "    dtrain = xgb.DMatrix(train.iloc[:, \u001b[34m1\u001b[39;49;00m:], label=train.iloc[:, \u001b[34m0\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    \u001b[34mif\u001b[39;49;00m args.validation \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        val =  pd.read_csv(s3.open(args.validation, mode=\u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), header=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "        dval = xgb.DMatrix(val.iloc[:, \u001b[34m1\u001b[39;49;00m:], label=val.iloc[:, \u001b[34m0\u001b[39;49;00m])\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        dval = \u001b[34mNone\u001b[39;49;00m\r\n",
      "    \r\n",
      "    watchlist = [(dtrain, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), (dval, \u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)] \u001b[34mif\u001b[39;49;00m dval \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m [(dtrain, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)]\r\n",
      "    \r\n",
      "    bst = xgb.train(\r\n",
      "        params=train_hp,\r\n",
      "        dtrain=dtrain,\r\n",
      "        evals=watchlist,\r\n",
      "        num_boost_round=args.num_round,\r\n",
      "        \u001b[37m# callbacks=[xgb.callback.LearningRateScheduler(custom_rates)]\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    \r\n",
      "    model_location = args.model_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/xgboost-model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    pkl.dump(bst, \u001b[36mopen\u001b[39;49;00m(model_location, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mStored trained model at \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_location))\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./scripts/xgboost_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f45fe247",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'num_round':'50',\n",
    "                   'max_depth':'5',\n",
    "                   'eta':'0.2',\n",
    "                   'gamma':'4',\n",
    "                   'min_child_weight':'6',\n",
    "                   'subsample':'0.7',\n",
    "                   'objective':'binary:logistic',\n",
    "                   'eval_metric': 'auc',\n",
    "                   'verbosity':'1'}\n",
    "\n",
    "\n",
    "xgb_estimator = XGBoost(entry_point='xgboost_train.py',\n",
    "                        framework_version = '1.2-2',\n",
    "                        source_dir='./scripts',\n",
    "                        hyperparameters=hyperparameters,\n",
    "                        role=sagemaker.get_execution_role(),\n",
    "                        instance_count=1,\n",
    "                        instance_type='ml.m5.2xlarge',\n",
    "                        requirements_file='requirements.txt',\n",
    "                        use_spot_instances=True,\n",
    "                        max_run=300,\n",
    "                        max_wait=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b79c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-08 16:14:43 Starting - Starting the training job...\n",
      "2021-11-08 16:15:12 Starting - Launching requested ML instancesProfilerReport-1636388083: InProgress\n",
      "......\n",
      "2021-11-08 16:16:13 Starting - Preparing the instances for training......\n",
      "2021-11-08 16:17:13 Downloading - Downloading input data...\n",
      "2021-11-08 16:17:33 Training - Downloading the training image...\n",
      "2021-11-08 16:18:13 Training - Training image download completed. Training in progress.\u001b[34m[2021-11-08 16:17:59.616 ip-10-0-136-231.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-11-08:16:17:59:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2021-11-08:16:17:59:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-11-08:16:17:59:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[34m[2021-11-08:16:18:00:INFO] Module xgboost_train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m[2021-11-08:16:18:00:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[34m[2021-11-08:16:18:00:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m[2021-11-08:16:18:00:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.11.0-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp>=3.7.1\n",
      "  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=1.4.1\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec==2021.11.0\n",
      "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.20.107,>=1.20.106\n",
      "  Downloading botocore-1.20.106-py2.py3-none-any.whl (7.7 MB)\u001b[0m\n",
      "\u001b[34mCollecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.13.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (79 kB)\u001b[0m\n",
      "\u001b[34mCollecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.8.0-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.0-py3-none-any.whl (6.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\u001b[0m\n",
      "\u001b[34mCollecting attrs>=17.3.0\n",
      "  Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4 in /miniconda3/lib/python3.7/site-packages (from aiohttp>=3.7.1->s3fs->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mCollecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /miniconda3/lib/python3.7/site-packages (from botocore<1.20.107,>=1.20.106->aiobotocore~=1.4.1->s3fs->-r requirements.txt (line 1)) (1.25.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /miniconda3/lib/python3.7/site-packages (from botocore<1.20.107,>=1.20.106->aiobotocore~=1.4.1->s3fs->-r requirements.txt (line 1)) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /miniconda3/lib/python3.7/site-packages (from botocore<1.20.107,>=1.20.106->aiobotocore~=1.4.1->s3fs->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /miniconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.107,>=1.20.106->aiobotocore~=1.4.1->s3fs->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna>=2.0 in /miniconda3/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.7.1->s3fs->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: xgboost-train, aiobotocore\n",
      "  Building wheel for xgboost-train (setup.py): started\n",
      "  Building wheel for xgboost-train (setup.py): finished with status 'done'\n",
      "  Created wheel for xgboost-train: filename=xgboost_train-1.0.0-py2.py3-none-any.whl size=9285 sha256=e1b3d2eb0abfa8247da721b095da87249dbc2700470ecf5298b64703ac572189\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-t5jtvjwc/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "  Building wheel for aiobotocore (setup.py): started\n",
      "  Building wheel for aiobotocore (setup.py): finished with status 'done'\n",
      "  Created wheel for aiobotocore: filename=aiobotocore-1.4.2-py3-none-any.whl size=49910 sha256=08b1656e1b0dcde3c0c9d213088948548cc8dd9412a9e8eddc5c480c14f11230\n",
      "  Stored in directory: /root/.cache/pip/wheels/33/e7/d9/b297a9aa9c43d56bc2463e6e2771655ff638f30b30f0b61fcb\u001b[0m\n",
      "\u001b[34mSuccessfully built xgboost-train aiobotocore\u001b[0m\n",
      "\u001b[34mInstalling collected packages: multidict, frozenlist, yarl, charset-normalizer, attrs, asynctest, async-timeout, aiosignal, wrapt, botocore, aioitertools, aiohttp, fsspec, aiobotocore, xgboost-train, s3fs\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.17.62\n",
      "    Uninstalling botocore-1.17.62:\n",
      "      Successfully uninstalled botocore-1.17.62\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires botocore==1.17.62, but you have botocore 1.20.106 which is incompatible.\u001b[0m\n",
      "\u001b[34mboto3 1.14.62 requires botocore<1.18.0,>=1.17.62, but you have botocore 1.20.106 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiobotocore-1.4.2 aiohttp-3.8.0 aioitertools-0.8.0 aiosignal-1.2.0 async-timeout-4.0.0 asynctest-0.13.0 attrs-21.2.0 botocore-1.20.106 charset-normalizer-2.0.7 frozenlist-1.2.0 fsspec-2021.11.0 multidict-5.2.0 s3fs-2021.11.0 wrapt-1.13.3 xgboost-train-1.0.0 yarl-1.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m[2021-11-08:16:18:07:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-11-08:16:18:07:INFO] Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eval_metric\": \"auc\",\n",
      "        \"max_depth\": \"5\",\n",
      "        \"objective\": \"binary:logistic\",\n",
      "        \"eta\": \"0.2\",\n",
      "        \"num_round\": \"50\",\n",
      "        \"subsample\": \"0.7\",\n",
      "        \"gamma\": \"4\",\n",
      "        \"min_child_weight\": \"6\",\n",
      "        \"verbosity\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-xgboost-2021-11-08-16-14-43-294\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-943601785668/sagemaker-xgboost-2021-11-08-16-14-43-294/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"xgboost_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"xgboost_train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"eta\":\"0.2\",\"eval_metric\":\"auc\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"binary:logistic\",\"subsample\":\"0.7\",\"verbosity\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=xgboost_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=xgboost_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-943601785668/sagemaker-xgboost-2021-11-08-16-14-43-294/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"eta\":\"0.2\",\"eval_metric\":\"auc\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"binary:logistic\",\"subsample\":\"0.7\",\"verbosity\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2021-11-08-16-14-43-294\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-943601785668/sagemaker-xgboost-2021-11-08-16-14-43-294/source/sourcedir.tar.gz\",\"module_name\":\"xgboost_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"xgboost_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--eval_metric\",\"auc\",\"--gamma\",\"4\",\"--max_depth\",\"5\",\"--min_child_weight\",\"6\",\"--num_round\",\"50\",\"--objective\",\"binary:logistic\",\"--subsample\",\"0.7\",\"--verbosity\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_METRIC=auc\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[34mSM_HP_OBJECTIVE=binary:logistic\u001b[0m\n",
      "\u001b[34mSM_HP_ETA=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ROUND=50\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=0.7\u001b[0m\n",
      "\u001b[34mSM_HP_GAMMA=4\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_CHILD_WEIGHT=6\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSITY=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m xgboost_train --eta 0.2 --eval_metric auc --gamma 4 --max_depth 5 --min_child_weight 6 --num_round 50 --objective binary:logistic --subsample 0.7 --verbosity 1\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[0]#011train-auc:0.99693#011validation-auc:0.99657\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.99756#011validation-auc:0.99841\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.99796#011validation-auc:0.99847\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.99811#011validation-auc:0.99842\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.99816#011validation-auc:0.99847\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.99827#011validation-auc:0.99845\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.99833#011validation-auc:0.99845\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.99839#011validation-auc:0.99844\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.99840#011validation-auc:0.99846\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.99862#011validation-auc:0.99841\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.99864#011validation-auc:0.99838\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.99885#011validation-auc:0.99927\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.99894#011validation-auc:0.99939\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.99903#011validation-auc:0.99937\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.99906#011validation-auc:0.99935\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.99908#011validation-auc:0.99936\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.99910#011validation-auc:0.99935\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.99922#011validation-auc:0.99932\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.99924#011validation-auc:0.99930\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.99924#011validation-auc:0.99936\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.99929#011validation-auc:0.99935\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.99932#011validation-auc:0.99933\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.99933#011validation-auc:0.99930\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.99934#011validation-auc:0.99934\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.99935#011validation-auc:0.99933\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.99937#011validation-auc:0.99934\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.99937#011validation-auc:0.99934\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.99939#011validation-auc:0.99929\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.99940#011validation-auc:0.99933\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.99941#011validation-auc:0.99934\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.99943#011validation-auc:0.99933\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.99943#011validation-auc:0.99932\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.99944#011validation-auc:0.99935\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.99947#011validation-auc:0.99936\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.99947#011validation-auc:0.99936\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.99948#011validation-auc:0.99936\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.99949#011validation-auc:0.99939\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.99949#011validation-auc:0.99938\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.99950#011validation-auc:0.99938\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.99951#011validation-auc:0.99938\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.99953#011validation-auc:0.99939\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.99954#011validation-auc:0.99938\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.99955#011validation-auc:0.99937\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.99957#011validation-auc:0.99937\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.99957#011validation-auc:0.99936\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.99958#011validation-auc:0.99936\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.99959#011validation-auc:0.99937\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.99960#011validation-auc:0.99937\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.99961#011validation-auc:0.99938\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.99962#011validation-auc:0.99940\u001b[0m\n",
      "\n",
      "2021-11-08 16:20:38 Uploading - Uploading generated training model\n",
      "2021-11-08 16:20:38 Completed - Training job completed\n",
      "Training seconds: 206\n",
      "Billable seconds: 93\n",
      "Managed Spot Training savings: 54.9%\n"
     ]
    }
   ],
   "source": [
    "xgb_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91028c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 uri where the trained model is located: \n",
      "s3://sagemaker-us-east-1-943601785668/sagemaker-xgboost-2021-11-08-16-14-43-294/output/model.tar.gz\n",
      "\n",
      "latest training job name for this estimator: \n",
      "sagemaker-xgboost-2021-11-08-16-14-43-294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{xgb_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{xgb_estimator.latest_training_job.name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff91eb",
   "metadata": {},
   "source": [
    "## Load Trained jobs \n",
    "### Directly load in model if you already have one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c022e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-11-08 16:20:54 Starting - Preparing the instances for training\n",
      "2021-11-08 16:20:54 Downloading - Downloading input data\n",
      "2021-11-08 16:20:54 Training - Training image download completed. Training in progress.\n",
      "2021-11-08 16:20:54 Uploading - Uploading generated training model\n",
      "2021-11-08 16:20:54 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "xgb_estimator = sagemaker.estimator.Estimator.attach('sagemaker-xgboost-2021-11-08-16-14-43-294')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ac2da",
   "metadata": {},
   "source": [
    "## Deploy Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56d86f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb_estimator.deploy(initial_instance_count=1,instance_type='ml.m4.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade8e79",
   "metadata": {},
   "source": [
    "## View Evaluation Details on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3477fb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train prediction shape: (80000,)\n",
      "Prediction time for 80000 samples: 291.56421756744385 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load in data (since it's small)\n",
    "train = pd.read_csv('s3://compressed-data-sample/train_embedding_yfirst.csv', header=None)\n",
    "\n",
    "# Predict by batch to prevent request timeout\n",
    "TIMEOUT_LIMIT = 400\n",
    "train_predictions_array = []\n",
    "for i in range(np.ceil(len(train)/TIMEOUT_LIMIT).astype(int)):\n",
    "    # Get batch and get rid of label column\n",
    "    train_byte = train.iloc[TIMEOUT_LIMIT*i:TIMEOUT_LIMIT*(i+1), 1:].to_csv(index=False).encode('utf-8') #load the data into an array\n",
    "    train_predictions = xgb_predictor.predict(train_byte, initial_args={'ContentType': 'text/csv'}).decode('utf-8') # predict!\n",
    "    train_predictions_array.append(np.fromstring(train_predictions, sep=',')[1:]) # and turn the prediction into an array\n",
    "\n",
    "train_predictions_array = np.concatenate(train_predictions_array)\n",
    "end_time = time.time()\n",
    "print(f'Train prediction shape: {train_predictions_array.shape}')\n",
    "print(f'Prediction time for {len(train)} samples: {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d1684a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Train Classification Rate: 99.1%\n",
      "\n",
      "Predicted      NonPeace          Peace\n",
      "Observed\n",
      "NonPeace       99% (39558)     1% (357)\n",
      "Peace           1% (359)     99% (39726) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = pd.crosstab(index=train[0], columns=np.round(train_predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]\n",
    "fn = cm.iloc[1,0]\n",
    "tp = cm.iloc[1,1]\n",
    "fp = cm.iloc[0,1]\n",
    "p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Train Classification Rate: \", p))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"NonPeace\", \"Peace\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"NonPeace\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Peace\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd8f6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean-up Memory\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdaa4bd",
   "metadata": {},
   "source": [
    "## View Evaluation Details on Test (Full Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8fa264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction shape: (10000,)\n",
      "Prediction time for 10000 samples: 34.61984467506409 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test = pd.read_csv('s3://compressed-data-sample/test_embedding.csv', header=None)\n",
    "test_predictions_array = []\n",
    "\n",
    "for i in range(np.ceil(len(test)/TIMEOUT_LIMIT).astype(int)):\n",
    "    test_byte = test.iloc[TIMEOUT_LIMIT*i:TIMEOUT_LIMIT*(i+1), 1:-1].to_csv(index=False).encode('utf-8') #load the data into an array\n",
    "    test_predictions = xgb_predictor.predict(test_byte, initial_args={'ContentType': 'text/csv'}).decode('utf-8') # predict!\n",
    "    test_predictions_array.append(np.fromstring(test_predictions, sep=',')[1:]) # and turn the prediction into an array\n",
    "\n",
    "test_predictions_array = np.concatenate(test_predictions_array)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Test prediction shape: {test_predictions_array.shape}')\n",
    "print(f'Prediction time for {len(test)} samples: {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6da34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Test Classification Rate: 98.2%\n",
      "\n",
      "Predicted      NonPeace          Peace\n",
      "Observed\n",
      "NonPeace       98% (4995)     2% (83)\n",
      "Peace           2% (95)     98% (4827) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = pd.crosstab(index=test[0], columns=np.round(test_predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]\n",
    "fn = cm.iloc[1,0]\n",
    "tp = cm.iloc[1,1]\n",
    "fp = cm.iloc[0,1]\n",
    "p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Test Classification Rate: \", p))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"NonPeace\", \"Peace\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"NonPeace\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Peace\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10dccda",
   "metadata": {},
   "source": [
    "## View Evaluation Details on Test (Remove India & Australia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75c2e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size : 1633\n",
      "Test prediction shape: (1633,)\n",
      "Prediction time for 1633 samples: 4.291927337646484 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test = test[~test.iloc[:, -1].isin(['India', 'Australia'])]\n",
    "print(f'Test Size : {len(test)}')\n",
    "\n",
    "test_predictions_array = []\n",
    "\n",
    "for i in range(np.ceil(len(test)/TIMEOUT_LIMIT).astype(int)):\n",
    "    test_byte = test.iloc[TIMEOUT_LIMIT*i:TIMEOUT_LIMIT*(i+1), 1:-1].to_csv(index=False).encode('utf-8') #load the data into an array\n",
    "    test_predictions = xgb_predictor.predict(test_byte, initial_args={'ContentType': 'text/csv'}).decode('utf-8') # predict!\n",
    "    test_predictions_array.append(np.fromstring(test_predictions, sep=',')[1:]) # and turn the prediction into an array\n",
    "\n",
    "test_predictions_array = np.concatenate(test_predictions_array)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Test prediction shape: {test_predictions_array.shape}')\n",
    "print(f'Prediction time for {len(test)} samples: {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81233a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Test (Minority Group) Classification Rate: 95.9%\n",
      "\n",
      "Predicted      NonPeace          Peace\n",
      "Observed\n",
      "NonPeace       90% (420)     2% (20)\n",
      "Peace           10% (47)     98% (1146) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = pd.crosstab(index=test[0], columns=np.round(test_predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]\n",
    "fn = cm.iloc[1,0]\n",
    "tp = cm.iloc[1,1]\n",
    "fp = cm.iloc[0,1]\n",
    "p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Test (Minority Group) Classification Rate: \", p))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"NonPeace\", \"Peace\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"NonPeace\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Peace\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "777b5029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 98.2847 %\n",
      "Recall: 96.0604 %\n",
      "F1: 97.1598 %\n"
     ]
    }
   ],
   "source": [
    "precision = tp /(tp+fp)\n",
    "recall = tp /(tp+fn)\n",
    "print(f'Precision: {100*precision:.4f} %')\n",
    "print(f'Recall: {100*recall:.4f} %')\n",
    "print(f'F1: {100*2*precision*recall/(precision+recall):.4f} %' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81b73a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451211a",
   "metadata": {},
   "source": [
    "## Delete the deployed Model, Configuration and Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbb5062b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '72549625-13c5-4fa8-b851-332f141445f4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '72549625-13c5-4fa8-b851-332f141445f4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 09 Nov 2021 14:33:49 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment_name = xgb_predictor.endpoint_name\n",
    "client = boto3.client('sagemaker')\n",
    "response = client.describe_endpoint_config(EndpointConfigName=deployment_name)\n",
    "\n",
    "model_name = response['ProductionVariants'][0]['ModelName']\n",
    "client.delete_model(ModelName=model_name)    \n",
    "client.delete_endpoint(EndpointName=deployment_name)\n",
    "client.delete_endpoint_config(EndpointConfigName=deployment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5631128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
